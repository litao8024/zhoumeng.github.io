<!DOCTYPE html>
<!-- saved from url=(0030)/ -->
<html lang="zh" class="js json svg checked target dataset details fetch supports csstransforms3d no-ios" style=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
      
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="./index.html2/">
      
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="ja">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="./index2_files/AI.jpg">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-4.4.0">
    
    
      
        <title>第二章:Transformer架构解析 - Transformer</title>
      
    
    
      <link rel="stylesheet" href="./index2_files/application.0284f74d.css">
      
      
    
    
      <script src="./index2_files/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin="">
        <link rel="stylesheet" href="./index2_files/css">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="./index2_files/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-36723568-3", "mkdocs.org")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async="" src="./index2_files/analytics.js"></script>
      
    
    
  <script type="text/javascript">(function(){var s=document.createElement("script");var port=window.location.port;s.src="//"+window.location.hostname+":"+port+ "/livereload.js?port=" + port;document.head.appendChild(s);})();</script><script src="./index2_files/livereload.js"></script></head>
  
    <body dir="ltr" data-md-state="">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#21-transformer" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header" data-md-state="shadow">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="./index.html" title="Transformer" class="md-header-nav__button md-logo">
          
            <img src="./index2_files/AI.jpg" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic" style="width: 648px;">
              Transformer
            </span>
            <span class="md-header-nav__topic" style="width: 648px;">
              
                第二章:Transformer架构解析
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix="">
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="http://www.itcast.cn/" title="前往 Github 仓库" class="md-source" data-md-source="" data-md-state="done">
  
  <div class="md-source__repository">
    「传智播客: 用爱成就每一位学生」
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation" style="height: 637px;">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="./index.html" title="Transformer" class="md-nav__button md-logo">
      
        <img src="./index2_files/AI.jpg" width="48" height="48">
      
    </a>
    Transformer
  </label>
  
    <div class="md-nav__source">
      


  

<a href="http://www.itcast.cn/" title="前往 Github 仓库" class="md-source" data-md-source="" data-md-state="done">
  
  <div class="md-source__repository">
    「传智播客: 用爱成就每一位学生」
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix="">
    
      
      
      


  <li class="md-nav__item">
    <a href="./index.html" title="第一章:Transformer背景介绍" class="md-nav__link">
      第一章:Transformer背景介绍
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        第二章:Transformer架构解析
      </label>
    
    <a href="/" title="第二章:Transformer架构解析" class="md-nav__link md-nav__link--active">
      第二章:Transformer架构解析
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix="">
      
        <li class="md-nav__item">
  <a href="#21-transformer" title="2.1 认识Transformer架构" class="md-nav__link">
    2.1 认识Transformer架构
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer" title="Transformer模型的作用" class="md-nav__link">
    Transformer模型的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer_1" title="Transformer总体架构图" class="md-nav__link">
    Transformer总体架构图
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22" title="2.2 输入部分实现" class="md-nav__link">
    2.2 输入部分实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" title="文本嵌入层的作用" class="md-nav__link">
    文本嵌入层的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" title="位置编码器的作用" class="md-nav__link">
    位置编码器的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23" title="2.3 编码器部分实现" class="md-nav__link">
    2.3 编码器部分实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#231" title="2.3.1 掩码张量" class="md-nav__link">
    2.3.1 掩码张量
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#232" title="2.3.2 注意力机制" class="md-nav__link">
    2.3.2 注意力机制
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#233" title="2.3.3 多头注意力机制" class="md-nav__link">
    2.3.3 多头注意力机制
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#234" title="2.3.4 前馈全连接层" class="md-nav__link">
    2.3.4 前馈全连接层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#235" title="2.3.5 规范化层" class="md-nav__link">
    2.3.5 规范化层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#236" title="2.3.6 子层连接结构" class="md-nav__link">
    2.3.6 子层连接结构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#237" title="2.3.7 编码器层" class="md-nav__link">
    2.3.7 编码器层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#238" title="2.3.8 编码器" class="md-nav__link">
    2.3.8 编码器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#24" title="2.4 解码器部分实现" class="md-nav__link">
    2.4 解码器部分实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#241" title="2.4.1 解码器层" class="md-nav__link">
    2.4.1 解码器层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#242" title="2.4.2 解码器" class="md-nav__link">
    2.4.2 解码器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25" title="2.5 输出部分实现" class="md-nav__link">
    2.5 输出部分实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" title="线性层的作用" class="md-nav__link">
    线性层的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax" title="softmax层的作用" class="md-nav__link">
    softmax层的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26" title="2.6 模型构建" class="md-nav__link">
    2.6 模型构建
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_12" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-" title="编码器-解码器结构的代码实现" class="md-nav__link">
    编码器-解码器结构的代码实现
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tansformer" title="Tansformer模型构建过程的代码分析" class="md-nav__link">
    Tansformer模型构建过程的代码分析
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc" style="height: 637px;">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix="">
      
        <li class="md-nav__item">
  <a href="#21-transformer" title="2.1 认识Transformer架构" class="md-nav__link">
    2.1 认识Transformer架构
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer" title="Transformer模型的作用" class="md-nav__link">
    Transformer模型的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer_1" title="Transformer总体架构图" class="md-nav__link">
    Transformer总体架构图
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22" title="2.2 输入部分实现" class="md-nav__link">
    2.2 输入部分实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" title="文本嵌入层的作用" class="md-nav__link">
    文本嵌入层的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" title="位置编码器的作用" class="md-nav__link">
    位置编码器的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23" title="2.3 编码器部分实现" class="md-nav__link">
    2.3 编码器部分实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#231" title="2.3.1 掩码张量" class="md-nav__link">
    2.3.1 掩码张量
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#232" title="2.3.2 注意力机制" class="md-nav__link">
    2.3.2 注意力机制
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#233" title="2.3.3 多头注意力机制" class="md-nav__link">
    2.3.3 多头注意力机制
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#234" title="2.3.4 前馈全连接层" class="md-nav__link">
    2.3.4 前馈全连接层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#235" title="2.3.5 规范化层" class="md-nav__link">
    2.3.5 规范化层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#236" title="2.3.6 子层连接结构" class="md-nav__link">
    2.3.6 子层连接结构
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#237" title="2.3.7 编码器层" class="md-nav__link">
    2.3.7 编码器层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#238" title="2.3.8 编码器" class="md-nav__link">
    2.3.8 编码器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#24" title="2.4 解码器部分实现" class="md-nav__link">
    2.4 解码器部分实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#241" title="2.4.1 解码器层" class="md-nav__link">
    2.4.1 解码器层
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#242" title="2.4.2 解码器" class="md-nav__link">
    2.4.2 解码器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25" title="2.5 输出部分实现" class="md-nav__link">
    2.5 输出部分实现
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" title="线性层的作用" class="md-nav__link">
    线性层的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax" title="softmax层的作用" class="md-nav__link">
    softmax层的作用
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26" title="2.6 模型构建" class="md-nav__link">
    2.6 模型构建
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_12" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-" title="编码器-解码器结构的代码实现" class="md-nav__link">
    编码器-解码器结构的代码实现
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tansformer" title="Tansformer模型构建过程的代码分析" class="md-nav__link">
    Tansformer模型构建过程的代码分析
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" title="小节总结" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>第二章:Transformer架构解析</h1>
                
                <h2 id="21-transformer">2.1 认识Transformer架构</h2>
<hr>
<h3 id="_1">学习目标</h3>
<ul>
<li>了解Transformer模型的作用.</li>
<li>了解Transformer总体架构图中各个组成部分的名称.</li>
</ul>
<hr>
<h3 id="transformer">Transformer模型的作用</h3>
<ul>
<li>基于seq2seq架构的transformer模型可以完成NLP领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型，用于不同任务的迁移学习.</li>
</ul>
<hr>
<ul>
<li>声明: <ul>
<li>在接下来的架构分析中, 我们将假设使用Transformer模型架构处理从一种语言文本到另一种语言文本的翻译工作, 因此很多命名方式遵循NLP中的规则. 比如: Embeddding层将称作文本嵌入层, Embedding层产生的张量称为词嵌入张量, 它的最后一维将称作词向量等.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="transformer_1">Transformer总体架构图</h3>
<p></p><center><img alt="avatar" src="./index2_files/4.png"></center><p></p>
<hr>
<ul>
<li>Transformer总体架构可分为四个部分:<ul>
<li>输入部分</li>
<li>输出部分</li>
<li>编码器部分</li>
<li>解码器部分</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>输入部分包含:<ul>
<li>源文本嵌入层及其位置编码器</li>
<li>目标文本嵌入层及其位置编码器</li>
</ul>
</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/5.png"></center><p></p>
<hr>
<ul>
<li>输出部分包含:<ul>
<li>线性层</li>
<li>softmax层</li>
</ul>
</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/6.png"></center><p></p>
<hr>
<ul>
<li>编码器部分:<ul>
<li>由N个编码器层堆叠而成 </li>
<li>每个编码器层由两个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/7.png"></center><p></p>
<hr>
<ul>
<li>解码器部分:<ul>
<li>由N个解码器层堆叠而成</li>
<li>每个解码器层由三个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li>
<li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/8.png"></center><p></p>
<hr>
<h3 id="_2">小节总结</h3>
<ul>
<li>
<p>学习了Transformer模型的作用:</p>
<ul>
<li>基于seq2seq架构的transformer模型可以完成NLP领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型，用于不同任务的迁移学习.</li>
</ul>
<hr>
</li>
<li>
<p>Transformer总体架构可分为四个部分:</p>
<ul>
<li>输入部分</li>
<li>输出部分</li>
<li>编码器部分</li>
<li>解码器部分</li>
</ul>
<hr>
</li>
<li>
<p>输入部分包含:</p>
<ul>
<li>源文本嵌入层及其位置编码器</li>
<li>目标文本嵌入层及其位置编码器</li>
</ul>
<hr>
</li>
<li>
<p>输出部分包含:</p>
<ul>
<li>线性层</li>
<li>softmax处理器</li>
</ul>
<hr>
</li>
<li>
<p>编码器部分:</p>
<ul>
<li>由N个编码器层堆叠而成</li>
<li>每个编码器层由两个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
<hr>
</li>
<li>
<p>解码器部分:</p>
<ul>
<li>由N个解码器层堆叠而成</li>
<li>每个解码器层由三个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li>
<li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
</li>
</ul>
<hr>
<hr>
<hr>
<h2 id="22">2.2 输入部分实现</h2>
<hr>
<h3 id="_3">学习目标</h3>
<ul>
<li>了解文本嵌入层和位置编码的作用.</li>
<li>掌握文本嵌入层和位置编码的实现过程.</li>
</ul>
<hr>
<ul>
<li>输入部分包含:<ul>
<li>源文本嵌入层及其位置编码器</li>
<li>目标文本嵌入层及其位置编码器</li>
</ul>
</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/5.png"></center><p></p>
<hr>
<h3 id="_4">文本嵌入层的作用</h3>
<ul>
<li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</li>
</ul>
<hr>
<ul>
<li>pytorch 0.3.0及其必备工具包的安装:</li>
</ul>
<div class="codehilite" id="__code_0"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_0 pre, #__code_0 code"><span class="md-clipboard__message"></span></button><pre id="__code_1"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_1 pre, #__code_1 code"><span class="md-clipboard__message"></span></button><code><span class="o">#</span> <span class="err">使用</span><span class="n">pip安装的工具包包括pytorch</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">3</span><span class="p">.</span><span class="mi">0</span><span class="p">,</span> <span class="n">numpy</span><span class="p">,</span> <span class="n">matplotlib</span><span class="p">,</span> <span class="n">seaborn</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">download</span><span class="p">.</span><span class="n">pytorch</span><span class="p">.</span><span class="n">org</span><span class="o">/</span><span class="n">whl</span><span class="o">/</span><span class="n">cu80</span><span class="o">/</span><span class="n">torch</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">3</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="n">post4</span><span class="o">-</span><span class="n">cp36</span><span class="o">-</span><span class="n">cp36m</span><span class="o">-</span><span class="n">linux_x86_64</span><span class="p">.</span><span class="n">whl</span> <span class="n">numpy</span> <span class="n">matplotlib</span> <span class="n">seaborn</span>

<span class="o">#</span> <span class="n">MAC系统安装</span><span class="p">,</span> <span class="n">python版本</span><span class="o">&lt;=</span><span class="mi">3</span><span class="p">.</span><span class="mi">6</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span><span class="o">==</span><span class="mi">0</span><span class="p">.</span><span class="mi">3</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="n">post4</span> <span class="n">numpy</span> <span class="n">matplotlib</span> <span class="n">seaborn</span>
</code></pre></div>


<hr>
<ul>
<li>文本嵌入层的代码分析:</li>
</ul>
<div class="codehilite" id="__code_2"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_2 pre, #__code_2 code"><span class="md-clipboard__message"></span></button><pre id="__code_3"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_3 pre, #__code_3 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 导入必备的工具包</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># 预定义的网络层torch.nn, 工具开发者已经帮助我们开发好的一些常用层, </span>
<span class="c1"># 比如，卷积层, lstm层, embedding层等, 不需要我们再重新造轮子.</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="c1"># 数学计算工具包</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># torch中变量封装函数Variable.</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="c1"># 定义Embeddings类来实现文本嵌入层，这里s说明代表两个一模一样的嵌入层, 他们共享参数.</span>
<span class="c1"># 该类继承nn.Module, 这样就有标准层的一些功能, 这里我们也可以理解为一种模式, 我们自己实现的所有层都会这样去写.</span>
<span class="k">class</span> <span class="nc">Embeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span>
        <span class="sd">"""类的初始化函数, 有两个参数, d_model: 指词嵌入的维度, vocab: 指词表的大小."""</span>
        <span class="c1"># 接着就是使用super的方式指明继承nn.Module的初始化函数, 我们自己实现的所有层都会这样去写.</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 之后就是调用nn中的预定义层Embedding, 获得一个词嵌入对象self.lut</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># 最后就是将d_model传入类中</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">"""可以将其理解为该层的前向传播逻辑，所有层中都会有此函数</span>
<span class="sd">           当传给该类的实例化对象参数时, 自动调用该类函数</span>
<span class="sd">           参数x: 因为Embedding层是首层, 所以代表输入给模型的文本通过词汇映射后的张量"""</span>

        <span class="c1"># 将x传给self.lut并与根号下self.d_model相乘作为结果返回</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
</code></pre></div>


<ul>
<li>nn.Embedding演示:</li>
</ul>
<div class="codehilite" id="__code_4"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_4 pre, #__code_4 code"><span class="md-clipboard__message"></span></button><pre id="__code_5"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_5 pre, #__code_5 code"><span class="md-clipboard__message"></span></button><code><span class="o">&gt;&gt;&gt;</span> <span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.0251</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6902</span><span class="p">,</span>  <span class="mf">0.7172</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.6431</span><span class="p">,</span>  <span class="mf">0.0748</span><span class="p">,</span>  <span class="mf">0.6969</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.4970</span><span class="p">,</span>  <span class="mf">1.3448</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9685</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.3677</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7265</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1685</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">1.4970</span><span class="p">,</span>  <span class="mf">1.3448</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9685</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.4362</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4004</span><span class="p">,</span>  <span class="mf">0.9400</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.6431</span><span class="p">,</span>  <span class="mf">0.0748</span><span class="p">,</span>  <span class="mf">0.6969</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.9124</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3616</span><span class="p">,</span>  <span class="mf">1.1151</span><span class="p">]]])</span>


<span class="o">&gt;&gt;&gt;</span> <span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.1535</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0309</span><span class="p">,</span>  <span class="mf">0.9315</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1655</span><span class="p">,</span>  <span class="mf">0.9897</span><span class="p">,</span>  <span class="mf">0.0635</span><span class="p">]]])</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>实例化参数: </li>
</ul>
</blockquote>
<div class="codehilite" id="__code_6"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_6 pre, #__code_6 code"><span class="md-clipboard__message"></span></button><pre id="__code_7"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_7 pre, #__code_7 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 词嵌入维度是512维</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>

<span class="c1"># 词表大小是1000</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="mi">1000</span>
</code></pre></div>


<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_8"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_8 pre, #__code_8 code"><span class="md-clipboard__message"></span></button><pre id="__code_9"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_9 pre, #__code_9 code"><span class="md-clipboard__message"></span></button><code><span class="o">#</span> <span class="err">输入</span><span class="n">x是一个使用Variable封装的长整型张量</span><span class="p">,</span> <span class="err">形状是</span><span class="mi">2</span> <span class="n">x</span> <span class="mi">4</span>
<span class="n">x</span> <span class="o">=</span> <span class="k">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">100</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">421</span><span class="p">,</span><span class="mi">508</span><span class="p">],[</span><span class="mi">491</span><span class="p">,</span><span class="mi">998</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">221</span><span class="p">]]))</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_10"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_10 pre, #__code_10 code"><span class="md-clipboard__message"></span></button><pre id="__code_11"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_11 pre, #__code_11 code"><span class="md-clipboard__message"></span></button><code><span class="n">emb</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
<span class="n">embr</span> <span class="o">=</span> <span class="n">emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"embr:"</span><span class="p">,</span> <span class="n">embr</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_12"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_12 pre, #__code_12 code"><span class="md-clipboard__message"></span></button><pre id="__code_13"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_13 pre, #__code_13 code"><span class="md-clipboard__message"></span></button><code><span class="n">embr</span><span class="o">:</span> <span class="n">Variable</span> <span class="n">containing</span><span class="o">:</span>
<span class="o">(</span> <span class="mi">0</span> <span class="o">,.,.)</span> <span class="o">=</span> 
  <span class="mf">35.9321</span>   <span class="mf">3.2582</span> <span class="o">-</span><span class="mf">17.7301</span>  <span class="o">...</span>    <span class="mf">3.4109</span>  <span class="mf">13.8832</span>  <span class="mf">39.0272</span>
   <span class="mf">8.5410</span>  <span class="o">-</span><span class="mf">3.5790</span> <span class="o">-</span><span class="mf">12.0460</span>  <span class="o">...</span>   <span class="mf">40.1880</span>  <span class="mf">36.6009</span>  <span class="mf">34.7141</span>
 <span class="o">-</span><span class="mf">17.0650</span>  <span class="o">-</span><span class="mf">1.8705</span> <span class="o">-</span><span class="mf">20.1807</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">12.5556</span> <span class="o">-</span><span class="mf">34.0739</span>  <span class="mf">35.6536</span>
  <span class="mf">20.6105</span>   <span class="mf">4.4314</span>  <span class="mf">14.9912</span>  <span class="o">...</span>   <span class="o">-</span><span class="mf">0.1342</span>  <span class="o">-</span><span class="mf">9.9270</span>  <span class="mf">28.6771</span>

<span class="o">(</span> <span class="mi">1</span> <span class="o">,.,.)</span> <span class="o">=</span> 
  <span class="mf">27.7016</span>  <span class="mf">16.7183</span>  <span class="mf">46.6900</span>  <span class="o">...</span>   <span class="mf">17.9840</span>  <span class="mf">17.2525</span>  <span class="o">-</span><span class="mf">3.9709</span>
   <span class="mf">3.0645</span>  <span class="o">-</span><span class="mf">5.5105</span>  <span class="mf">10.8802</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">13.0069</span>  <span class="mf">30.8834</span> <span class="o">-</span><span class="mf">38.3209</span>
  <span class="mf">33.1378</span> <span class="o">-</span><span class="mf">32.1435</span>  <span class="o">-</span><span class="mf">3.9369</span>  <span class="o">...</span>   <span class="mf">15.6094</span> <span class="o">-</span><span class="mf">29.7063</span>  <span class="mf">40.1361</span>
 <span class="o">-</span><span class="mf">31.5056</span>   <span class="mf">3.3648</span>   <span class="mf">1.4726</span>  <span class="o">...</span>    <span class="mf">2.8047</span>  <span class="o">-</span><span class="mf">9.6514</span> <span class="o">-</span><span class="mf">23.4909</span>
<span class="o">[</span><span class="n">torch</span><span class="o">.</span><span class="na">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x4x512</span><span class="o">]</span>
</code></pre></div>


<hr>
<h3 id="_5">位置编码器的作用</h3>
<ul>
<li>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失.</li>
</ul>
<hr>
<ul>
<li>位置编码器的代码分析:</li>
</ul>
<div class="codehilite" id="__code_14"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_14 pre, #__code_14 code"><span class="md-clipboard__message"></span></button><pre id="__code_15"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_15 pre, #__code_15 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 定义位置编码器类, 我们同样把它看做一个层, 因此会继承nn.Module    </span>
<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="sd">"""位置编码器类的初始化函数, 共有三个参数, 分别是d_model: 词嵌入维度, </span>
<span class="sd">           dropout: 置0比率, max_len: 每个句子的最大长度"""</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># 实例化nn中预定义的Dropout层, 并将dropout传入其中, 获得对象self.dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># 初始化一个位置编码矩阵, 它是一个0阵，矩阵的大小是max_len x d_model.</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># 初始化一个绝对位置矩阵, 在我们这里，词汇的绝对位置就是用它的索引去表示. </span>
        <span class="c1"># 所以我们首先使用arange方法获得一个连续自然数向量，然后再使用unsqueeze方法拓展向量维度使其成为矩阵， </span>
        <span class="c1"># 又因为参数传的是1，代表矩阵拓展的位置，会使向量变成一个max_len x 1 的矩阵， </span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 绝对位置矩阵初始化之后，接下来就是考虑如何将这些位置信息加入到位置编码矩阵中，</span>
        <span class="c1"># 最简单思路就是先将max_len x 1的绝对位置矩阵， 变换成max_len x d_model形状，然后覆盖原来的初始位置编码矩阵即可， </span>
        <span class="c1"># 要做这种矩阵变换，就需要一个1xd_model形状的变换矩阵div_term，我们对这个变换矩阵的要求除了形状外，</span>
        <span class="c1"># 还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于在之后的梯度下降过程中更快的收敛.  这样我们就可以开始初始化这个变换矩阵了.</span>
        <span class="c1"># 首先使用arange获得一个自然数矩阵， 但是细心的同学们会发现， 我们这里并没有按照预计的一样初始化一个1xd_model的矩阵， </span>
        <span class="c1"># 而是有了一个跳跃，只初始化了一半即1xd_model/2 的矩阵。 为什么是一半呢，其实这里并不是真正意义上的初始化了一半的矩阵，</span>
        <span class="c1"># 我们可以把它看作是初始化了两次，而每次初始化的变换矩阵会做不同的处理，第一次初始化的变换矩阵分布在正弦波上， 第二次初始化的变换矩阵分布在余弦波上， </span>
        <span class="c1"># 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上，组成最终的位置编码矩阵.</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span>
                             <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>

        <span class="c1"># 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，</span>
        <span class="c1"># 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢，</span>
        <span class="c1"># 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超参数或者参数，不需要随着优化步骤进行更新的增益对象. </span>
        <span class="c1"># 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">'pe'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">"""forward函数的参数是x, 表示文本序列的词嵌入表示"""</span>
        <span class="c1"># 在相加之前我们对pe做一些适配工作， 将这个三维张量的第二维也就是句子最大长度的那一维将切片到与输入的x的第二维相同即x.size(1)，</span>
        <span class="c1"># 因为我们默认max_len为5000一般来讲实在太大了，很难有一条句子包含5000个词汇，所以要进行与输入张量的适配. </span>
        <span class="c1"># 最后使用Variable进行封装，使其与x的样式相同，但是它是不需要进行梯度求解的，因此把requires_grad设置成false.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> 
                         <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="c1"># 最后使用self.dropout对象进行'丢弃'操作, 并返回结果.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>


<ul>
<li>nn.Dropout演示:</li>
</ul>
<div class="codehilite" id="__code_16"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_16 pre, #__code_16 code"><span class="md-clipboard__message"></span></button><pre id="__code_17"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_17 pre, #__code_17 code"><span class="md-clipboard__message"></span></button><code><span class="o">&gt;&gt;&gt;</span> <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">output</span>
<span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
 <span class="mf">0.0000</span> <span class="o">-</span><span class="mf">0.5856</span> <span class="o">-</span><span class="mf">1.4094</span>  <span class="mf">0.0000</span> <span class="o">-</span><span class="mf">1.0290</span>
 <span class="mf">2.0591</span> <span class="o">-</span><span class="mf">1.3400</span> <span class="o">-</span><span class="mf">1.7247</span> <span class="o">-</span><span class="mf">0.9885</span>  <span class="mf">0.1286</span>
 <span class="mf">0.5099</span>  <span class="mf">1.3715</span>  <span class="mf">0.0000</span>  <span class="mf">2.2079</span> <span class="o">-</span><span class="mf">0.5497</span>
<span class="o">-</span><span class="mf">0.0000</span> <span class="o">-</span><span class="mf">0.7839</span> <span class="o">-</span><span class="mf">1.2434</span> <span class="o">-</span><span class="mf">0.1222</span>  <span class="mf">1.2815</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span><span class="n">x5</span><span class="p">]</span>
</code></pre></div>


<ul>
<li>torch.unsqueeze演示:</li>
</ul>
<div class="codehilite" id="__code_18"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_18 pre, #__code_18 code"><span class="md-clipboard__message"></span></button><pre id="__code_19"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_19 pre, #__code_19 code"><span class="md-clipboard__message"></span></button><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">4</span><span class="p">]])</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_20"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_20 pre, #__code_20 code"><span class="md-clipboard__message"></span></button><pre id="__code_21"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_21 pre, #__code_21 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 词嵌入维度是512维</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>

<span class="c1"># 置0比率为0.1</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># 句子最大长度</span>
<span class="n">max_len</span><span class="o">=</span><span class="mi">60</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_22"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_22 pre, #__code_22 code"><span class="md-clipboard__message"></span></button><pre id="__code_23"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_23 pre, #__code_23 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 输入x是Embedding层的输出的张量, 形状是2 x 4 x 512</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">embr</span>
<span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
<span class="p">(</span> <span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="mf">35.9321</span>   <span class="mf">3.2582</span> <span class="o">-</span><span class="mf">17.7301</span>  <span class="o">...</span>    <span class="mf">3.4109</span>  <span class="mf">13.8832</span>  <span class="mf">39.0272</span>
   <span class="mf">8.5410</span>  <span class="o">-</span><span class="mf">3.5790</span> <span class="o">-</span><span class="mf">12.0460</span>  <span class="o">...</span>   <span class="mf">40.1880</span>  <span class="mf">36.6009</span>  <span class="mf">34.7141</span>
 <span class="o">-</span><span class="mf">17.0650</span>  <span class="o">-</span><span class="mf">1.8705</span> <span class="o">-</span><span class="mf">20.1807</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">12.5556</span> <span class="o">-</span><span class="mf">34.0739</span>  <span class="mf">35.6536</span>
  <span class="mf">20.6105</span>   <span class="mf">4.4314</span>  <span class="mf">14.9912</span>  <span class="o">...</span>   <span class="o">-</span><span class="mf">0.1342</span>  <span class="o">-</span><span class="mf">9.9270</span>  <span class="mf">28.6771</span>

<span class="p">(</span> <span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="mf">27.7016</span>  <span class="mf">16.7183</span>  <span class="mf">46.6900</span>  <span class="o">...</span>   <span class="mf">17.9840</span>  <span class="mf">17.2525</span>  <span class="o">-</span><span class="mf">3.9709</span>
   <span class="mf">3.0645</span>  <span class="o">-</span><span class="mf">5.5105</span>  <span class="mf">10.8802</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">13.0069</span>  <span class="mf">30.8834</span> <span class="o">-</span><span class="mf">38.3209</span>
  <span class="mf">33.1378</span> <span class="o">-</span><span class="mf">32.1435</span>  <span class="o">-</span><span class="mf">3.9369</span>  <span class="o">...</span>   <span class="mf">15.6094</span> <span class="o">-</span><span class="mf">29.7063</span>  <span class="mf">40.1361</span>
 <span class="o">-</span><span class="mf">31.5056</span>   <span class="mf">3.3648</span>   <span class="mf">1.4726</span>  <span class="o">...</span>    <span class="mf">2.8047</span>  <span class="o">-</span><span class="mf">9.6514</span> <span class="o">-</span><span class="mf">23.4909</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x4x512</span><span class="p">]</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_24"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_24 pre, #__code_24 code"><span class="md-clipboard__message"></span></button><pre id="__code_25"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_25 pre, #__code_25 code"><span class="md-clipboard__message"></span></button><code><span class="n">pe</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
<span class="n">pe_result</span> <span class="o">=</span> <span class="n">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"pe_result:"</span><span class="p">,</span> <span class="n">pe_result</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_26"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_26 pre, #__code_26 code"><span class="md-clipboard__message"></span></button><pre id="__code_27"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_27 pre, #__code_27 code"><span class="md-clipboard__message"></span></button><code><span class="n">pe_result</span><span class="p">:</span> <span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
<span class="p">(</span> <span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
 <span class="o">-</span><span class="mf">19.7050</span>   <span class="mf">0.0000</span>   <span class="mf">0.0000</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">11.7557</span>  <span class="o">-</span><span class="mf">0.0000</span>  <span class="mf">23.4553</span>
  <span class="o">-</span><span class="mf">1.4668</span> <span class="o">-</span><span class="mf">62.2510</span>  <span class="o">-</span><span class="mf">2.4012</span>  <span class="o">...</span>   <span class="mf">66.5860</span> <span class="o">-</span><span class="mf">24.4578</span> <span class="o">-</span><span class="mf">37.7469</span>
   <span class="mf">9.8642</span> <span class="o">-</span><span class="mf">41.6497</span> <span class="o">-</span><span class="mf">11.4968</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">21.1293</span> <span class="o">-</span><span class="mf">42.0945</span>  <span class="mf">50.7943</span>
   <span class="mf">0.0000</span>  <span class="mf">34.1785</span> <span class="o">-</span><span class="mf">33.0712</span>  <span class="o">...</span>   <span class="mf">48.5520</span>   <span class="mf">3.2540</span>  <span class="mf">54.1348</span>

<span class="p">(</span> <span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
   <span class="mf">7.7598</span> <span class="o">-</span><span class="mf">21.0359</span>  <span class="mf">15.0595</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">35.6061</span>  <span class="o">-</span><span class="mf">0.0000</span>   <span class="mf">4.1772</span>
 <span class="o">-</span><span class="mf">38.7230</span>   <span class="mf">8.6578</span>  <span class="mf">34.2935</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">43.3556</span>  <span class="mf">26.6052</span>   <span class="mf">4.3084</span>
  <span class="mf">24.6962</span>  <span class="mf">37.3626</span> <span class="o">-</span><span class="mf">26.9271</span>  <span class="o">...</span>   <span class="mf">49.8989</span>   <span class="mf">0.0000</span>  <span class="mf">44.9158</span>
 <span class="o">-</span><span class="mf">28.8435</span> <span class="o">-</span><span class="mf">48.5963</span>  <span class="o">-</span><span class="mf">0.9892</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">52.5447</span>  <span class="o">-</span><span class="mf">4.1475</span>  <span class="o">-</span><span class="mf">3.0450</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x4x512</span><span class="p">]</span>
</code></pre></div>


<hr>
<ul>
<li>绘制词汇向量中特征的分布曲线: </li>
</ul>
<div class="codehilite" id="__code_28"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_28 pre, #__code_28 code"><span class="md-clipboard__message"></span></button><pre id="__code_29"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_29 pre, #__code_29 code"><span class="md-clipboard__message"></span></button><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c1"># 创建一张15 x 5大小的画布</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># 实例化PositionalEncoding类得到pe对象, 输入参数是20和0</span>
<span class="n">pe</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># 然后向pe传入被Variable封装的tensor, 这样pe会直接执行forward函数, </span>
<span class="c1"># 且这个tensor里的数值都是0, 被处理后相当于位置编码张量</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pe</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">)))</span>

<span class="c1"># 然后定义画布的横纵坐标, 横坐标到100的长度, 纵坐标是某一个词汇中的某维特征在不同长度下对应的值</span>
<span class="c1"># 因为总共有20维之多, 我们这里只查看4，5，6，7维的值.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">4</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="c1"># 在画布上填写维度提示信息</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">"dim </span><span class="si">%d</span><span class="s2">"</span><span class="o">%</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]])</span>
</code></pre></div>


<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<p></p><center><img alt="avatar" src="./index2_files/11.png"></center><p></p>
<blockquote>
<ul>
<li>效果分析:<ul>
<li>每条颜色的曲线代表某一个词汇中的特征在不同位置的含义.</li>
<li>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.</li>
<li>正弦波和余弦波的值域范围都是1到-1这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算. </li>
</ul>
</li>
</ul>
</blockquote>
<hr>
<h3 id="_6">小节总结</h3>
<ul>
<li>
<p>学习了文本嵌入层的作用:</p>
<ul>
<li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</li>
</ul>
<hr>
</li>
<li>
<p>学习并实现了文本嵌入层的类: Embeddings</p>
<ul>
<li>初始化函数以d_model, 词嵌入维度, 和vocab, 词汇总数为参数, 内部主要使用了nn中的预定层Embedding进行词嵌入.</li>
<li>在forward函数中, 将输入x传入到Embedding的实例化对象中, 然后乘以一个根号下d_model进行缩放, 控制数值大小.</li>
<li>它的输出是文本嵌入后的结果.</li>
</ul>
<hr>
</li>
<li>
<p>学习了位置编码器的作用:</p>
<ul>
<li>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失.</li>
</ul>
<hr>
</li>
<li>
<p>学习并实现了位置编码器的类: PositionalEncoding</p>
<ul>
<li>初始化函数以d_model, dropout, max_len为参数, 分别代表d_model: 词嵌入维度, dropout: 置0比率, max_len: 每个句子的最大长度.</li>
<li>forward函数中的输入参数为x, 是Embedding层的输出.</li>
<li>最终输出一个加入了位置编码信息的词嵌入张量.</li>
</ul>
<hr>
</li>
<li>
<p>实现了绘制词汇向量中特征的分布曲线:</p>
<ul>
<li>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.</li>
<li>正弦波和余弦波的值域范围都是1到-1, 这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算.</li>
</ul>
</li>
</ul>
<hr>
<hr>
<hr>
<h2 id="23">2.3 编码器部分实现</h2>
<hr>
<h3 id="_7">学习目标</h3>
<ul>
<li>了解编码器中各个组成部分的作用.</li>
<li>掌握编码器中各个组成部分的实现过程.</li>
</ul>
<hr>
<ul>
<li>编码器部分:<ul>
<li>由N个编码器层堆叠而成</li>
<li>每个编码器层由两个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/7.png"></center><p></p>
<hr>
<h3 id="231">2.3.1 掩码张量</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解什么是掩码张量以及它的作用.</li>
<li>掌握生成掩码张量的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<hr>
<ul>
<li>什么是掩码张量:<ul>
<li>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩，也可以说被替换, 它的表现形式是一个张量.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>掩码张量的作用:<ul>
<li>在transformer中, 掩码张量的主要作用在应用attention(将在下一小节讲解)时，有一些生成的attention张量中的值计算有可能已知了未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用. 所以，我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>生成掩码张量的代码分析:</li>
</ul>
<div class="codehilite" id="__code_30"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_30 pre, #__code_30 code"><span class="md-clipboard__message"></span></button><pre id="__code_31"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_31 pre, #__code_31 code"><span class="md-clipboard__message"></span></button><code><span class="k">def</span> <span class="nf">subsequent_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="sd">"""生成向后遮掩的掩码张量, 参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵"""</span>
    <span class="c1"># 在函数中, 首先定义掩码张量的形状</span>
    <span class="n">attn_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>

    <span class="c1"># 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间, </span>
    <span class="c1"># 再使其中的数据类型变为无符号8位整形unit8 </span>
    <span class="n">subsequent_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">attn_shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'uint8'</span><span class="p">)</span>

    <span class="c1"># 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作, </span>
    <span class="c1"># 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减, </span>
    <span class="c1"># 如果是0, subsequent_mask中的该位置由0变成1</span>
    <span class="c1"># 如果是1, subsequent_mask中的该位置由1变成0 </span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">subsequent_mask</span><span class="p">)</span>
</code></pre></div>


<hr>
<ul>
<li>np.triu演示:</li>
</ul>
<div class="codehilite" id="__code_32"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_32 pre, #__code_32 code"><span class="md-clipboard__message"></span></button><pre id="__code_33"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_33 pre, #__code_33 code"><span class="md-clipboard__message"></span></button><code><span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">]],</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">9</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">]])</span>


<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">]],</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">9</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>


<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">]],</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入实例:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_34"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_34 pre, #__code_34 code"><span class="md-clipboard__message"></span></button><pre id="__code_35"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_35 pre, #__code_35 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 生成的掩码张量的最后两维的大小</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">5</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_36"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_36 pre, #__code_36 code"><span class="md-clipboard__message"></span></button><pre id="__code_37"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_37 pre, #__code_37 code"><span class="md-clipboard__message"></span></button><code><span class="n">sm</span> <span class="o">=</span> <span class="n">subsequent_mask</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"sm:"</span><span class="p">,</span> <span class="n">sm</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_38"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_38 pre, #__code_38 code"><span class="md-clipboard__message"></span></button><pre id="__code_39"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_39 pre, #__code_39 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 最后两维形成一个下三角阵</span>
<span class="n">sm</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="mi">1</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>
  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>
  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">0</span>  <span class="mi">0</span>
  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">0</span>
  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">1</span><span class="n">x5x5</span><span class="p">]</span>
</code></pre></div>


<hr>
<ul>
<li>掩码张量的可视化:</li>
</ul>
<div class="codehilite" id="__code_40"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_40 pre, #__code_40 code"><span class="md-clipboard__message"></span></button><pre id="__code_41"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_41 pre, #__code_41 code"><span class="md-clipboard__message"></span></button><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">subsequent_mask</span><span class="p">(</span><span class="mi">20</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<p></p><center><img alt="avatar" src="./index2_files/12.png"></center><p></p>
<hr>
<blockquote>
<ul>
<li>效果分析:<ul>
<li>通过观察可视化方阵, 黄色是1的部分, 这里代表被遮掩, 紫色代表没有被遮掩的信息, 横坐标代表目标词汇的位置, 纵坐标代表可查看的位置; </li>
<li>我们看到, 在0的位置我们一看望过去都是黄色的, 都被遮住了，1的位置一眼望过去还是黄色, 说明第一次词还没有产生, 从第二个位置看过去, 就能看到位置1的词, 其他位置看不到, 以此类推.</li>
</ul>
</li>
</ul>
</blockquote>
<hr>
<ul>
<li>
<p>2.3.1 掩码张量总结:</p>
<ul>
<li>学习了什么是掩码张量:<ul>
<li>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩, 也可以说被替换, 它的表现形式是一个张量.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习了掩码张量的作用:<ul>
<li>在transformer中, 掩码张量的主要作用在应用attention(将在下一小节讲解)时，有一些生成的attetion张量中的值计算有可能已知量未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用. 所以，我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了生成向后遮掩的掩码张量函数: subsequent_mask<ul>
<li>它的输入是size, 代表掩码张量的大小.</li>
<li>它的输出是一个最后两维形成1方阵的下三角阵.</li>
<li>最后对生成的掩码张量进行了可视化分析, 更深一步理解了它的用途.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<h3 id="232">2.3.2 注意力机制</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解什么是注意力计算规则和注意力机制.</li>
<li>掌握注意力计算规则的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是注意力:<ul>
<li>我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是注意力计算规则:<ul>
<li>它需要三个指定的输入Q(query), K(key), V(value), 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>我们这里使用的注意力的计算规则:</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/9.png"></center><p></p>
<hr>
<ul>
<li>Q, K, V的比喻解释:</li>
</ul>
<div class="codehilite" id="__code_42"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_42 pre, #__code_42 code"><span class="md-clipboard__message"></span></button><pre id="__code_43"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_43 pre, #__code_43 code"><span class="md-clipboard__message"></span></button><code><span class="err">假如我们有一个问题</span><span class="o">:</span> <span class="err">给出一段文本，使用一些关键词对它进行描述</span><span class="o">!</span>
<span class="err">为了方便统一正确答案，这道题可能预先已经给大家写出了一些关键词作为提示</span><span class="o">.</span><span class="err">其中这些给出的提示就可以看作是</span><span class="n">key</span><span class="err">，</span> 
<span class="err">而整个的文本信息就相当于是</span><span class="n">query</span><span class="err">，</span><span class="n">value的含义则更抽象</span><span class="err">，可以比作是你看到这段文本信息后，脑子里浮现的答案信息，</span>
<span class="err">这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息，</span>
<span class="err">因此</span><span class="n">key与value基本是相同的</span><span class="err">，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多，</span>
<span class="err">并且能够开始对我们</span><span class="n">query也就是这段文本</span><span class="err">，提取关键信息进行表示</span><span class="o">.</span>  <span class="err">这就是注意力作用的过程，</span> <span class="err">通过这个过程，</span>
<span class="err">我们最终脑子里的</span><span class="n">value发生了变化</span><span class="err">，</span>
<span class="err">根据提示</span><span class="n">key生成了query的关键词表示方法</span><span class="err">，也就是另外一种特征表示方法</span><span class="o">.</span>

<span class="err">刚刚我们说到</span><span class="n">key和value一般情况下默认是相同</span><span class="err">，与</span><span class="n">query是不同的</span><span class="err">，这种是我们一般的注意力输入形式，</span>
<span class="err">但有一种特殊情况，就是我们</span><span class="n">query与key和value相同</span><span class="err">，这种情况我们称为自注意力机制，就如同我们的刚刚的例子，</span> 
<span class="err">使用一般注意力机制，是使用不同于给定文本的关键词表示它</span><span class="o">.</span> <span class="err">而自注意力机制</span><span class="o">,</span>
<span class="err">需要用给定文本自身来表达自己，也就是说你需要从给定文本中抽取关键词来表述它</span><span class="o">,</span> <span class="err">相当于对文本自身的一次特征提取</span><span class="o">.</span>
</code></pre></div>


<hr>
<ul>
<li>什么是注意力机制:<ul>
<li>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制. </li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>注意力机制在网络中实现的图形表示:</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/10.png"></center><p></p>
<hr>
<ul>
<li>注意力计算规则的代码分析:</li>
</ul>
<div class="codehilite" id="__code_44"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_44 pre, #__code_44 code"><span class="md-clipboard__message"></span></button><pre id="__code_45"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_45 pre, #__code_45 code"><span class="md-clipboard__message"></span></button><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">"""注意力机制的实现, 输入分别是query, key, value, mask: 掩码张量, </span>
<span class="sd">       dropout是nn.Dropout层的实例化对象, 默认为None"""</span>
    <span class="c1"># 在函数中, 首先取query的最后一维的大小, 一般情况下就等同于我们的词嵌入维度, 命名为d_k</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># 按照注意力公式, 将query与key的转置相乘, 这里面key是将最后两个维度进行转置, 再除以缩放系数根号下d_k, 这种计算方法也称为缩放点积注意力计算.</span>
    <span class="c1"># 得到注意力得分张量scores</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>

    <span class="c1"># 接着判断是否使用掩码张量</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># 使用tensor的masked_fill方法, 将掩码张量和scores张量每个位置一一比较, 如果掩码张量处为0</span>
        <span class="c1"># 则对应的scores张量用-1e9这个值来替换, 如下演示</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

    <span class="c1"># 对scores的最后一维进行softmax操作, 使用F.softmax方法, 第一个参数是softmax对象, 第二个是目标维度.</span>
    <span class="c1"># 这样获得最终的注意力张量</span>
    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 之后判断是否使用dropout进行随机置0</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># 将p_attn传入dropout对象中进行'丢弃'处理</span>
        <span class="n">p_attn</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">p_attn</span><span class="p">)</span>

    <span class="c1"># 最后, 根据公式将p_attn与value张量相乘获得最终的query注意力表示, 同时返回注意力张量</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span>
</code></pre></div>


<ul>
<li>tensor.masked_fill演示:</li>
</ul>
<div class="codehilite" id="__code_46"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_46 pre, #__code_46 code"><span class="md-clipboard__message"></span></button><pre id="__code_47"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_47 pre, #__code_47 code"><span class="md-clipboard__message"></span></button><code><span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span> 
<span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
 <span class="mf">2.0344</span> <span class="o">-</span><span class="mf">0.5450</span>  <span class="mf">0.3365</span> <span class="o">-</span><span class="mf">0.1888</span> <span class="o">-</span><span class="mf">2.1803</span>
 <span class="mf">1.5221</span> <span class="o">-</span><span class="mf">0.3823</span>  <span class="mf">0.8414</span>  <span class="mf">0.7836</span> <span class="o">-</span><span class="mf">0.8481</span>
<span class="o">-</span><span class="mf">0.0345</span> <span class="o">-</span><span class="mf">0.8643</span>  <span class="mf">0.6476</span> <span class="o">-</span><span class="mf">0.2713</span>  <span class="mf">1.5645</span>
 <span class="mf">0.8788</span> <span class="o">-</span><span class="mf">2.2142</span>  <span class="mf">0.4022</span>  <span class="mf">0.1997</span>  <span class="mf">0.1474</span>
 <span class="mf">2.9109</span>  <span class="mf">0.6006</span> <span class="o">-</span><span class="mf">0.6745</span> <span class="o">-</span><span class="mf">1.7262</span>  <span class="mf">0.6977</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">5</span><span class="n">x5</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mask</span>
<span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
 <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>
 <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>
 <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>
 <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>
 <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">5</span><span class="n">x5</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
<span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
<span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span>
<span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span>
<span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span>
<span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span>
<span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span> <span class="o">-</span><span class="mf">1.0000e+09</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">5</span><span class="n">x5</span><span class="p">]</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_48"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_48 pre, #__code_48 code"><span class="md-clipboard__message"></span></button><pre id="__code_49"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_49 pre, #__code_49 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 我们令输入的query, key, value都相同, 位置编码的输出</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">key</span> <span class="o">=</span> <span class="n">value</span> <span class="o">=</span> <span class="n">pe_result</span>
<span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
<span class="p">(</span> <span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="mf">46.5196</span>  <span class="mf">16.2057</span> <span class="o">-</span><span class="mf">41.5581</span>  <span class="o">...</span>  <span class="o">-</span><span class="mf">16.0242</span> <span class="o">-</span><span class="mf">17.8929</span> <span class="o">-</span><span class="mf">43.0405</span>
 <span class="o">-</span><span class="mf">32.6040</span>  <span class="mf">16.1096</span> <span class="o">-</span><span class="mf">29.5228</span>  <span class="o">...</span>    <span class="mf">4.2721</span>  <span class="mf">20.6034</span>  <span class="o">-</span><span class="mf">1.2747</span>
 <span class="o">-</span><span class="mf">18.6235</span>  <span class="mf">14.5076</span>  <span class="o">-</span><span class="mf">2.0105</span>  <span class="o">...</span>   <span class="mf">15.6462</span> <span class="o">-</span><span class="mf">24.6081</span> <span class="o">-</span><span class="mf">30.3391</span>
   <span class="mf">0.0000</span> <span class="o">-</span><span class="mf">66.1486</span> <span class="o">-</span><span class="mf">11.5123</span>  <span class="o">...</span>   <span class="mf">20.1519</span>  <span class="o">-</span><span class="mf">4.6823</span>   <span class="mf">0.4916</span>

<span class="p">(</span> <span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
 <span class="o">-</span><span class="mf">24.8681</span>   <span class="mf">7.5495</span>  <span class="o">-</span><span class="mf">5.0765</span>  <span class="o">...</span>   <span class="o">-</span><span class="mf">7.5992</span> <span class="o">-</span><span class="mf">26.6630</span>  <span class="mf">40.9517</span>
  <span class="mf">13.1581</span>  <span class="o">-</span><span class="mf">3.1918</span> <span class="o">-</span><span class="mf">30.9001</span>  <span class="o">...</span>   <span class="mf">25.1187</span> <span class="o">-</span><span class="mf">26.4621</span>   <span class="mf">2.9542</span>
 <span class="o">-</span><span class="mf">49.7690</span> <span class="o">-</span><span class="mf">42.5019</span>   <span class="mf">8.0198</span>  <span class="o">...</span>   <span class="o">-</span><span class="mf">5.4809</span>  <span class="mf">25.9403</span> <span class="o">-</span><span class="mf">27.4931</span>
 <span class="o">-</span><span class="mf">52.2775</span>  <span class="mf">10.4006</span>   <span class="mf">0.0000</span>  <span class="o">...</span>   <span class="o">-</span><span class="mf">1.9985</span>   <span class="mf">7.0106</span>  <span class="o">-</span><span class="mf">0.5189</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x4x512</span><span class="p">]</span>
</code></pre></div>


<hr>
<ul>
<li>调用:</li>
</ul>
<div class="codehilite" id="__code_50"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_50 pre, #__code_50 code"><span class="md-clipboard__message"></span></button><pre id="__code_51"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_51 pre, #__code_51 code"><span class="md-clipboard__message"></span></button><code><span class="n">attn</span><span class="p">,</span> <span class="n">p_attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"attn:"</span><span class="p">,</span> <span class="n">attn</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"p_attn:"</span><span class="p">,</span> <span class="n">p_attn</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_52"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_52 pre, #__code_52 code"><span class="md-clipboard__message"></span></button><pre id="__code_53"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_53 pre, #__code_53 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 将得到两个结果</span>
<span class="c1"># query的注意力表示:</span>
<span class="n">attn</span><span class="p">:</span> <span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
<span class="p">(</span> <span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
   <span class="mf">12.8269</span>    <span class="mf">7.7403</span>   <span class="mf">41.2225</span>  <span class="o">...</span>     <span class="mf">1.4603</span>   <span class="mf">27.8559</span>  <span class="o">-</span><span class="mf">12.2600</span>
   <span class="mf">12.4904</span>    <span class="mf">0.0000</span>   <span class="mf">24.1575</span>  <span class="o">...</span>     <span class="mf">0.0000</span>    <span class="mf">2.5838</span>   <span class="mf">18.0647</span>
  <span class="o">-</span><span class="mf">32.5959</span>   <span class="o">-</span><span class="mf">4.6252</span>  <span class="o">-</span><span class="mf">29.1050</span>  <span class="o">...</span>     <span class="mf">0.0000</span>  <span class="o">-</span><span class="mf">22.6409</span>  <span class="o">-</span><span class="mf">11.8341</span>
    <span class="mf">8.9921</span>  <span class="o">-</span><span class="mf">33.0114</span>   <span class="o">-</span><span class="mf">0.7393</span>  <span class="o">...</span>     <span class="mf">4.7871</span>   <span class="o">-</span><span class="mf">5.7735</span>    <span class="mf">8.3374</span>

<span class="p">(</span> <span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="o">-</span><span class="mf">25.6705</span>   <span class="o">-</span><span class="mf">4.0860</span>  <span class="o">-</span><span class="mf">36.8226</span>  <span class="o">...</span>    <span class="mf">37.2346</span>  <span class="o">-</span><span class="mf">27.3576</span>    <span class="mf">2.5497</span>
  <span class="o">-</span><span class="mf">16.6674</span>   <span class="mf">73.9788</span>  <span class="o">-</span><span class="mf">33.3296</span>  <span class="o">...</span>    <span class="mf">28.5028</span>   <span class="o">-</span><span class="mf">5.5488</span>  <span class="o">-</span><span class="mf">13.7564</span>
    <span class="mf">0.0000</span>  <span class="o">-</span><span class="mf">29.9039</span>   <span class="o">-</span><span class="mf">3.0405</span>  <span class="o">...</span>     <span class="mf">0.0000</span>   <span class="mf">14.4408</span>   <span class="mf">14.8579</span>
   <span class="mf">30.7819</span>    <span class="mf">0.0000</span>   <span class="mf">21.3908</span>  <span class="o">...</span>   <span class="o">-</span><span class="mf">29.0746</span>    <span class="mf">0.0000</span>   <span class="o">-</span><span class="mf">5.8475</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x4x512</span><span class="p">]</span>

<span class="c1"># 注意力张量:</span>
<span class="n">p_attn</span><span class="p">:</span> <span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
<span class="p">(</span><span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="mi">1</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>
  <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">0</span>  <span class="mi">0</span>
  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">0</span>
  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">1</span>

<span class="p">(</span><span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="mi">1</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>
  <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">0</span>  <span class="mi">0</span>
  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">0</span>
  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">0</span>  <span class="mi">1</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x4x4</span><span class="p">]</span>
</code></pre></div>


<ul>
<li>带有mask的输入参数：</li>
</ul>
<div class="codehilite" id="__code_54"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_54 pre, #__code_54 code"><span class="md-clipboard__message"></span></button><pre id="__code_55"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_55 pre, #__code_55 code"><span class="md-clipboard__message"></span></button><code><span class="n">query</span> <span class="o">=</span> <span class="n">key</span> <span class="o">=</span> <span class="n">value</span> <span class="o">=</span> <span class="n">pe_result</span>

<span class="c1"># 令mask为一个2x4x4的零张量</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_56"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_56 pre, #__code_56 code"><span class="md-clipboard__message"></span></button><pre id="__code_57"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_57 pre, #__code_57 code"><span class="md-clipboard__message"></span></button><code><span class="n">attn</span><span class="p">,</span> <span class="n">p_attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"attn:"</span><span class="p">,</span> <span class="n">attn</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"p_attn:"</span><span class="p">,</span> <span class="n">p_attn</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>带有mask的输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_58"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_58 pre, #__code_58 code"><span class="md-clipboard__message"></span></button><pre id="__code_59"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_59 pre, #__code_59 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># query的注意力表示:</span>
<span class="n">attn</span><span class="p">:</span> <span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
<span class="p">(</span> <span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
   <span class="mf">0.4284</span>  <span class="o">-</span><span class="mf">7.4741</span>   <span class="mf">8.8839</span>  <span class="o">...</span>    <span class="mf">1.5618</span>   <span class="mf">0.5063</span>   <span class="mf">0.5770</span>
   <span class="mf">0.4284</span>  <span class="o">-</span><span class="mf">7.4741</span>   <span class="mf">8.8839</span>  <span class="o">...</span>    <span class="mf">1.5618</span>   <span class="mf">0.5063</span>   <span class="mf">0.5770</span>
   <span class="mf">0.4284</span>  <span class="o">-</span><span class="mf">7.4741</span>   <span class="mf">8.8839</span>  <span class="o">...</span>    <span class="mf">1.5618</span>   <span class="mf">0.5063</span>   <span class="mf">0.5770</span>
   <span class="mf">0.4284</span>  <span class="o">-</span><span class="mf">7.4741</span>   <span class="mf">8.8839</span>  <span class="o">...</span>    <span class="mf">1.5618</span>   <span class="mf">0.5063</span>   <span class="mf">0.5770</span>

<span class="p">(</span> <span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="o">-</span><span class="mf">2.8890</span>   <span class="mf">9.9972</span> <span class="o">-</span><span class="mf">12.9505</span>  <span class="o">...</span>    <span class="mf">9.1657</span>  <span class="o">-</span><span class="mf">4.6164</span>  <span class="o">-</span><span class="mf">0.5491</span>
  <span class="o">-</span><span class="mf">2.8890</span>   <span class="mf">9.9972</span> <span class="o">-</span><span class="mf">12.9505</span>  <span class="o">...</span>    <span class="mf">9.1657</span>  <span class="o">-</span><span class="mf">4.6164</span>  <span class="o">-</span><span class="mf">0.5491</span>
  <span class="o">-</span><span class="mf">2.8890</span>   <span class="mf">9.9972</span> <span class="o">-</span><span class="mf">12.9505</span>  <span class="o">...</span>    <span class="mf">9.1657</span>  <span class="o">-</span><span class="mf">4.6164</span>  <span class="o">-</span><span class="mf">0.5491</span>
  <span class="o">-</span><span class="mf">2.8890</span>   <span class="mf">9.9972</span> <span class="o">-</span><span class="mf">12.9505</span>  <span class="o">...</span>    <span class="mf">9.1657</span>  <span class="o">-</span><span class="mf">4.6164</span>  <span class="o">-</span><span class="mf">0.5491</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x4x512</span><span class="p">]</span>

<span class="c1"># 注意力张量:</span>
<span class="n">p_attn</span><span class="p">:</span> <span class="n">Variable</span> <span class="n">containing</span><span class="p">:</span>
<span class="p">(</span><span class="mi">0</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>
  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>
  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>
  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>

<span class="p">(</span><span class="mi">1</span> <span class="p">,</span><span class="o">.</span><span class="p">,</span><span class="o">.</span><span class="p">)</span> <span class="o">=</span> 
  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>
  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>
  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>
  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>  <span class="mf">0.2500</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">2</span><span class="n">x4x4</span><span class="p">]</span>
</code></pre></div>


<hr>
<ul>
<li>
<p>2.3.2 注意力机制总结:</p>
<ul>
<li>学习了什么是注意力:<ul>
<li>我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是注意力计算规则:<ul>
<li>它需要三个指定的输入Q(query), K(key), V(value), 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习了Q, K, V的比喻解释:<ul>
<li>Q是一段准备被概括的文本; K是给出的提示; V是大脑中的对提示K的延伸.</li>
<li>当Q=K=V时, 称作自注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是注意力机制:<ul>
<li>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了注意力计算规则的函数: attention<ul>
<li>它的输入就是Q，K，V以及mask和dropout, mask用于掩码, dropout用于随机置0.</li>
<li>它的输出有两个, query的注意力表示以及注意力张量. </li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<h3 id="233">2.3.3 多头注意力机制</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解多头注意力机制的作用.</li>
<li>掌握多头注意力机制的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是多头注意力机制:<ul>
<li>从多头注意力的结构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是，我只有使用了一组线性变化层，即三个变换张量对Q，K，V分别进行线性变换，这些变换不会改变原有张量的尺寸，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>多头注意力机制结构图:</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/13.png"></center><p></p>
<hr>
<ul>
<li>多头注意力机制的作用:<ul>
<li>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>多头注意力机制的代码实现:</li>
</ul>
<div class="codehilite" id="__code_60"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_60 pre, #__code_60 code"><span class="md-clipboard__message"></span></button><pre id="__code_61"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_61 pre, #__code_61 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 用于深度拷贝的copy工具包</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="c1"># 首先需要定义克隆函数, 因为在多头注意力机制的实现中, 用到多个结构相同的线性层.</span>
<span class="c1"># 我们将使用clone函数将他们一同初始化在一个网络层列表对象中. 之后的结构中也会用到该函数.</span>
<span class="k">def</span> <span class="nf">clones</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="sd">"""用于生成相同网络层的克隆函数, 它的参数module表示要克隆的目标网络层, N代表需要克隆的数量"""</span>
    <span class="c1"># 在函数中, 我们通过for循环对module进行N次深度拷贝, 使其每个module成为独立的层,</span>
    <span class="c1"># 然后将其放在nn.ModuleList类型的列表中存放.</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)])</span>




<span class="c1"># 我们使用一个类来实现多头注意力机制的处理</span>
<span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sd">"""在类的初始化时, 会传入三个参数，head代表头数，embedding_dim代表词嵌入的维度， </span>
<span class="sd">           dropout代表进行dropout操作时置0比率，默认是0.1."""</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，</span>
        <span class="c1"># 这是因为我们之后要给每个头分配等量的词特征.也就是embedding_dim/head个.</span>
        <span class="k">assert</span> <span class="n">embedding_dim</span> <span class="o">%</span> <span class="n">head</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="c1"># 得到每个头获得的分割词向量维度d_k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">embedding_dim</span> <span class="o">//</span> <span class="n">head</span>

        <span class="c1"># 传入头数h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">head</span>

        <span class="c1"># 然后获得线性层对象，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用clones函数克隆四个，</span>
        <span class="c1"># 为什么是四个呢，这是因为在多头注意力中，Q，K，V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>

        <span class="c1"># self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="c1"># 最后就是一个self.dropout对象，它通过nn中的Dropout实例化而来，置0比率为传进来的参数dropout.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">"""前向逻辑函数, 它的输入参数有四个，前三个就是注意力机制需要的Q, K, V，</span>
<span class="sd">           最后一个是注意力机制中可能需要的mask掩码张量，默认是None. """</span>

        <span class="c1"># 如果存在掩码张量mask</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># 使用unsqueeze拓展维度</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本.</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 之后就进入多头处理环节</span>
        <span class="c1"># 首先利用zip将输入QKV与三个线性层组到一起，然后使用for循环，将输入QKV分别传到线性层中，</span>
        <span class="c1"># 做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结果进行维度重塑，多加了一个维度h，代表头数，</span>
        <span class="c1"># 这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，</span>
        <span class="c1"># 计算机会根据这种变换自动计算这里的值.然后对第二维和第三维进行转置操作，</span>
        <span class="c1"># 为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，</span>
        <span class="c1"># 从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头的输入.</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> \
           <span class="p">[</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))]</span>

        <span class="c1"># 得到每个头的输入后，接下来就是将他们传入到attention中，</span>
        <span class="c1"># 这里直接调用我们之前实现的attention函数.同时也将mask和dropout传入其中.</span>
        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，</span>
        <span class="c1"># 因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法，</span>
        <span class="c1"># 这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，</span>
        <span class="c1"># 所以，下一步就是使用view重塑形状，变成和输入形状相同.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>

        <span class="c1"># 最后使用线性层列表中的最后一个线性层对输入进行线性变换得到最终的多头注意力结构的输出.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>


<hr>
<ul>
<li>tensor.view演示:</li>
</ul>
<div class="codehilite" id="__code_62"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_62 pre, #__code_62 code"><span class="md-clipboard__message"></span></button><pre id="__code_63"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_63 pre, #__code_63 code"><span class="md-clipboard__message"></span></button><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">16</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Swaps 2nd and 3rd dimension</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Does not change tensor layout in memory</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="bp">False</span>
</code></pre></div>


<hr>
<ul>
<li>torch.transpose演示:</li>
</ul>
<div class="codehilite" id="__code_64"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_64 pre, #__code_64 code"><span class="md-clipboard__message"></span></button><pre id="__code_65"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_65 pre, #__code_65 code"><span class="md-clipboard__message"></span></button><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">1.0028</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9893</span><span class="p">,</span>  <span class="mf">0.5809</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1669</span><span class="p">,</span>  <span class="mf">0.7299</span><span class="p">,</span>  <span class="mf">0.4942</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">1.0028</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1669</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.9893</span><span class="p">,</span>  <span class="mf">0.7299</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5809</span><span class="p">,</span>  <span class="mf">0.4942</span><span class="p">]])</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_66"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_66 pre, #__code_66 code"><span class="md-clipboard__message"></span></button><pre id="__code_67"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_67 pre, #__code_67 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 头数head</span>
<span class="n">head</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># 词嵌入维度embedding_dim</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">512</span>

<span class="c1"># 置零比率dropout</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_68"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_68 pre, #__code_68 code"><span class="md-clipboard__message"></span></button><pre id="__code_69"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_69 pre, #__code_69 code"><span class="md-clipboard__message"></span></button><code><span class="o">#</span> <span class="err">假设输入的</span><span class="n">Q</span><span class="err">，</span><span class="n">K</span><span class="err">，</span><span class="n">V仍然相等</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">value</span> <span class="o">=</span> <span class="k">key</span> <span class="o">=</span> <span class="n">pe_result</span>

<span class="o">#</span> <span class="err">输入的掩码张量</span><span class="n">mask</span>
<span class="n">mask</span> <span class="o">=</span> <span class="k">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_70"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_70 pre, #__code_70 code"><span class="md-clipboard__message"></span></button><pre id="__code_71"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_71 pre, #__code_71 code"><span class="md-clipboard__message"></span></button><code><span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">mha_result</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mha_result</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_72"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_72 pre, #__code_72 code"><span class="md-clipboard__message"></span></button><pre id="__code_73"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_73 pre, #__code_73 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">3075</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">5687</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">5693</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">1098</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">0878</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">3609</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">3</span><span class="p">.</span><span class="mi">8065</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">4538</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">3708</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">5205</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">1488</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">3984</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">2</span><span class="p">.</span><span class="mi">4190</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">5376</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">8475</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">4218</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">4488</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">2984</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">2</span><span class="p">.</span><span class="mi">9356</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">3620</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">8722</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">7996</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">1468</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">0345</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mi">1</span><span class="p">.</span><span class="mi">1423</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">6038</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">0954</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">2679</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">.</span><span class="mi">7749</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">4132</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">2</span><span class="p">.</span><span class="mi">4066</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">2777</span><span class="p">,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">8102</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">1137</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">9517</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">9246</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">5</span><span class="p">.</span><span class="mi">8201</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">1534</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">9191</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">1410</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">6110</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">0046</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">3</span><span class="p">.</span><span class="mi">1209</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">0008</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5317</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">8619</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">3204</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">3435</span><span class="p">]]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="k">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<ul>
<li>
<p>2.3.3 多头注意力机制总结:</p>
<ul>
<li>学习了什么是多头注意力机制:<ul>
<li>每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量. 这就是所谓的多头.将每个头的获得的输入送到注意力机制中, 就形成了多头注意力机制.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习了多头注意力机制的作用:<ul>
<li>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了多头注意力机制的类: MultiHeadedAttention<ul>
<li>因为多头注意力机制中需要使用多个相同的线性层, 首先实现了克隆函数clones.</li>
<li>clones函数的输入是module，N，分别代表克隆的目标层，和克隆个数.</li>
<li>clones函数的输出是装有N个克隆层的Module列表.</li>
<li>接着实现MultiHeadedAttention类, 它的初始化函数输入是h, d_model, dropout分别代表头数，词嵌入维度和置零比率.</li>
<li>它的实例化对象输入是Q, K, V以及掩码张量mask.</li>
<li>它的实例化对象输出是通过多头注意力机制处理的Q的注意力表示.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<h3 id="234">2.3.4 前馈全连接层</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解什么是前馈全连接层及其它的作用.</li>
<li>掌握前馈全连接层的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是前馈全连接层:<ul>
<li>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>前馈全连接层的作用:<ul>
<li>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>前馈全连接层的代码分析:</li>
</ul>
<div class="codehilite" id="__code_74"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_74 pre, #__code_74 code"><span class="md-clipboard__message"></span></button><pre id="__code_75"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_75 pre, #__code_75 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 通过类PositionwiseFeedForward来实现前馈全连接层</span>
<span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sd">"""初始化函数有三个输入参数分别是d_model, d_ff,和dropout=0.1，第一个是线性层的输入维度也是第二个线性层的输出维度，</span>
<span class="sd">           因为我们希望输入通过前馈全连接层后输入和输出的维度不变. 第二个参数d_ff就是第二个线性层的输入维度和第一个线性层的输出维度. </span>
<span class="sd">           最后一个是dropout置0比率."""</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># 首先按照我们预期使用nn实例化了两个线性层对象，self.w1和self.w2</span>
        <span class="c1"># 它们的参数分别是d_model, d_ff和d_ff, d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># 然后使用nn的Dropout实例化了对象self.dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">"""输入参数为x，代表来自上一层的输出"""</span>
        <span class="c1"># 首先经过第一个线性层，然后使用Funtional中relu函数进行激活,</span>
        <span class="c1"># 之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
</code></pre></div>


<hr>
<ul>
<li>ReLU函数公式: ReLU(x)=max(0, x)</li>
</ul>
<hr>
<ul>
<li>ReLU函数图像:</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/ReLU.png"></center><p></p>
<hr>
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_76"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_76 pre, #__code_76 code"><span class="md-clipboard__message"></span></button><pre id="__code_77"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_77 pre, #__code_77 code"><span class="md-clipboard__message"></span></button><code><span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>

<span class="c1"># 线性变化的维度</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_78"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_78 pre, #__code_78 code"><span class="md-clipboard__message"></span></button><pre id="__code_79"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_79 pre, #__code_79 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 输入参数x可以是多头注意力机制的输出</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">mha_result</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">0.3075</span><span class="p">,</span>  <span class="mf">1.5687</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5693</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1098</span><span class="p">,</span>  <span class="mf">0.0878</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3609</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.8065</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4538</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3708</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5205</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1488</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3984</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.4190</span><span class="p">,</span>  <span class="mf">0.5376</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8475</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.4218</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4488</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2984</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.9356</span><span class="p">,</span>  <span class="mf">0.3620</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.8722</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7996</span><span class="p">,</span>  <span class="mf">0.1468</span><span class="p">,</span>  <span class="mf">1.0345</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">1.1423</span><span class="p">,</span>  <span class="mf">0.6038</span><span class="p">,</span>  <span class="mf">0.0954</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">2.2679</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.7749</span><span class="p">,</span>  <span class="mf">1.4132</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.4066</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2777</span><span class="p">,</span>  <span class="mf">2.8102</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1137</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9517</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9246</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">5.8201</span><span class="p">,</span>  <span class="mf">1.1534</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9191</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.1410</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.6110</span><span class="p">,</span>  <span class="mf">1.0046</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.1209</span><span class="p">,</span>  <span class="mf">1.0008</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5317</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">2.8619</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.3204</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3435</span><span class="p">]]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_80"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_80 pre, #__code_80 code"><span class="md-clipboard__message"></span></button><pre id="__code_81"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_81 pre, #__code_81 code"><span class="md-clipboard__message"></span></button><code><span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">ff_result</span> <span class="o">=</span> <span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ff_result</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_82"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_82 pre, #__code_82 code"><span class="md-clipboard__message"></span></button><pre id="__code_83"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_83 pre, #__code_83 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.9488e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4060e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1216e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.8203e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">2.6336e+00</span><span class="p">,</span>  <span class="mf">2.0917e-03</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.5875e-02</span><span class="p">,</span>  <span class="mf">1.1523e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.5437e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6257e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">5.7620e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9225e-01</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">8.7508e-01</span><span class="p">,</span>  <span class="mf">1.0092e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6515e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.4446e-02</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.5933e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1760e-01</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.7507e-01</span><span class="p">,</span>  <span class="mf">4.7225e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0318e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0530e+00</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">3.7910e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.7730e-01</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mf">2.2575e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0904e+00</span><span class="p">,</span>  <span class="mf">2.9427e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.6574e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.9754e+00</span><span class="p">,</span>  <span class="mf">1.2797e+00</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.5114e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7963e-01</span><span class="p">,</span>  <span class="mf">1.2881e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4882e-02</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.5896e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0350e+00</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.7416e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0688e-01</span><span class="p">,</span>  <span class="mf">1.9289e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.9754e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.6320e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5217e+00</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.0874e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3842e-01</span><span class="p">,</span>  <span class="mf">2.9379e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.1276e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.6150e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1295e+00</span><span class="p">]]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<ul>
<li>
<p>2.3.4 前馈全连接层总结:</p>
<ul>
<li>学习了什么是前馈全连接层:<ul>
<li>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习了前馈全连接层的作用:<ul>
<li>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了前馈全连接层的类: PositionwiseFeedForward<ul>
<li>它的实例化参数为d_model, d_ff, dropout, 分别代表词嵌入维度, 线性变换维度, 和置零比率.</li>
<li>它的输入参数x, 表示上层的输出.</li>
<li>它的输出是经过2层线性网络变换的特征表示.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<h3 id="235">2.3.5 规范化层</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解规范化层的作用.</li>
<li>掌握规范化层的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>规范化层的作用:<ul>
<li>它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>规范化层的代码实现:</li>
</ul>
<div class="codehilite" id="__code_84"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_84 pre, #__code_84 code"><span class="md-clipboard__message"></span></button><pre id="__code_85"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_85 pre, #__code_85 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 通过LayerNorm实现规范化层的类</span>
<span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="sd">"""初始化函数有两个参数, 一个是features, 表示词嵌入的维度,</span>
<span class="sd">           另一个是eps它是一个足够小的数, 在规范化公式的分母中出现,</span>
<span class="sd">           防止分母为0.默认是1e-6."""</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># 根据features的形状初始化两个参数张量a2，和b2，第一个初始化为1张量，</span>
        <span class="c1"># 也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，这两个张量就是规范化层的参数，</span>
        <span class="c1"># 因为直接对上一层得到的结果做规范化公式计算，将改变结果的正常表征，因此就需要有参数作为调节因子，</span>
        <span class="c1"># 使其即能满足规范化要求，又能不改变针对目标的表征.最后使用nn.parameter封装，代表他们是模型的参数。</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>

        <span class="c1"># 把eps传到类中</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">"""输入参数x代表来自上一层的输出"""</span>
        <span class="c1"># 在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致.</span>
        <span class="c1"># 接着再求最后一个维度的标准差，然后就是根据规范化公式，用x减去均值除以标准差获得规范化的结果，</span>
        <span class="c1"># 最后对结果乘以我们的缩放参数，即a2，*号代表同型点乘，即对应位置进行乘法操作，加上位移参数b2.返回即可.</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_86"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_86 pre, #__code_86 code"><span class="md-clipboard__message"></span></button><pre id="__code_87"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_87 pre, #__code_87 code"><span class="md-clipboard__message"></span></button><code><span class="n">features</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_88"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_88 pre, #__code_88 code"><span class="md-clipboard__message"></span></button><pre id="__code_89"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_89 pre, #__code_89 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 输入x来自前馈全连接层的输出</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ff_result</span>
<span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.9488e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4060e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1216e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.8203e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">2.6336e+00</span><span class="p">,</span>  <span class="mf">2.0917e-03</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.5875e-02</span><span class="p">,</span>  <span class="mf">1.1523e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.5437e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6257e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">5.7620e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9225e-01</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">8.7508e-01</span><span class="p">,</span>  <span class="mf">1.0092e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6515e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.4446e-02</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.5933e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1760e-01</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.7507e-01</span><span class="p">,</span>  <span class="mf">4.7225e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0318e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.0530e+00</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">3.7910e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.7730e-01</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mf">2.2575e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0904e+00</span><span class="p">,</span>  <span class="mf">2.9427e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">9.6574e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.9754e+00</span><span class="p">,</span>  <span class="mf">1.2797e+00</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.5114e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7963e-01</span><span class="p">,</span>  <span class="mf">1.2881e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4882e-02</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.5896e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0350e+00</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.7416e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0688e-01</span><span class="p">,</span>  <span class="mf">1.9289e+00</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.9754e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.6320e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5217e+00</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.0874e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3842e-01</span><span class="p">,</span>  <span class="mf">2.9379e-01</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.1276e-01</span><span class="p">,</span>
          <span class="o">-</span><span class="mf">1.6150e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1295e+00</span><span class="p">]]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_90"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_90 pre, #__code_90 code"><span class="md-clipboard__message"></span></button><pre id="__code_91"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_91 pre, #__code_91 code"><span class="md-clipboard__message"></span></button><code><span class="n">ln</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
<span class="n">ln_result</span> <span class="o">=</span> <span class="n">ln</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ln_result</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果: </li>
</ul>
</blockquote>
<div class="codehilite" id="__code_92"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_92 pre, #__code_92 code"><span class="md-clipboard__message"></span></button><pre id="__code_93"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_93 pre, #__code_93 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">2.2697</span><span class="p">,</span>  <span class="mf">1.3911</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4417</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9937</span><span class="p">,</span>  <span class="mf">0.6589</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1902</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.5876</span><span class="p">,</span>  <span class="mf">0.5182</span><span class="p">,</span>  <span class="mf">0.6220</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.9836</span><span class="p">,</span>  <span class="mf">0.0338</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3393</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.8261</span><span class="p">,</span>  <span class="mf">2.0161</span><span class="p">,</span>  <span class="mf">0.2272</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.3004</span><span class="p">,</span>  <span class="mf">0.5660</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9044</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.5429</span><span class="p">,</span>  <span class="mf">1.3221</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2933</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0406</span><span class="p">,</span>  <span class="mf">1.0603</span><span class="p">,</span>  <span class="mf">1.4666</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">0.2378</span><span class="p">,</span>  <span class="mf">0.9952</span><span class="p">,</span>  <span class="mf">1.2621</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4334</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1644</span><span class="p">,</span>  <span class="mf">1.2082</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.0209</span><span class="p">,</span>  <span class="mf">0.6435</span><span class="p">,</span>  <span class="mf">0.4235</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3448</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0560</span><span class="p">,</span>  <span class="mf">1.2347</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.8158</span><span class="p">,</span>  <span class="mf">0.7118</span><span class="p">,</span>  <span class="mf">0.4110</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0990</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4833</span><span class="p">,</span>  <span class="mf">1.9434</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.9857</span><span class="p">,</span>  <span class="mf">2.3924</span><span class="p">,</span>  <span class="mf">0.3819</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.0157</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6300</span><span class="p">,</span>  <span class="mf">1.2251</span><span class="p">]]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<ul>
<li>
<p>2.3.5 规范化层总结:</p>
<ul>
<li>学习了规范化层的作用:<ul>
<li>它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了规范化层的类: LayerNorm<ul>
<li>它的实例化参数有两个, features和eps，分别表示词嵌入特征大小，和一个足够小的数.</li>
<li>它的输入参数x代表来自上一层的输出.</li>
<li>它的输出就是经过规范化的特征表示.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<h3 id="236">2.3.6 子层连接结构</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解什么是子层连接结构.</li>
<li>掌握子层连接结构的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>什么是子层连接结构:<ul>
<li>如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构），在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>子层连接结构图:</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/15.png"></center><p></p>
<hr>
<p></p><center><img alt="avatar" src="./index2_files/16.png"></center><p></p>
<hr>
<ul>
<li>子层连接结构的代码分析:</li>
</ul>
<div class="codehilite" id="__code_94"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_94 pre, #__code_94 code"><span class="md-clipboard__message"></span></button><pre id="__code_95"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_95 pre, #__code_95 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 使用SublayerConnection来实现子层连接结构的类</span>
<span class="k">class</span> <span class="nc">SublayerConnection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sd">"""它输入参数有两个, size以及dropout， size一般是都是词嵌入维度的大小， </span>
<span class="sd">           dropout本身是对模型结构中的节点数进行随机抑制的比率， </span>
<span class="sd">           又因为节点被抑制等效就是该节点的输出都是0，因此也可以把dropout看作是对输出矩阵的随机置0的比率.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 实例化了规范化对象self.norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="c1"># 又使用nn中预定义的droupout实例化一个self.dropout对象.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">):</span>
        <span class="sd">"""前向逻辑函数中, 接收上一个层或者子层的输入作为第一个参数，</span>
<span class="sd">           将该子层连接中的子层函数作为第二个参数"""</span>

        <span class="c1"># 我们首先对输出进行规范化，然后将结果传给子层处理，之后再对子层进行dropout操作，</span>
        <span class="c1"># 随机停止一些网络中神经元的作用，来防止过拟合. 最后还有一个add操作， </span>
        <span class="c1"># 因为存在跳跃连接，所以是将输入x与dropout后的子层输出结果相加作为最终的子层连接输出.</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sublayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>实例化参数</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_96"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_96 pre, #__code_96 code"><span class="md-clipboard__message"></span></button><pre id="__code_97"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_97 pre, #__code_97 code"><span class="md-clipboard__message"></span></button><code><span class="n">size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">head</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_98"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_98 pre, #__code_98 code"><span class="md-clipboard__message"></span></button><pre id="__code_99"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_99 pre, #__code_99 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 令x为位置编码器的输出</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">pe_result</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># 假设子层中装的是多头注意力层, 实例化这个类</span>
<span class="n">self_attn</span> <span class="o">=</span>  <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<span class="c1"># 使用lambda获得一个函数类型的子层</span>
<span class="n">sublayer</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_100"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_100 pre, #__code_100 code"><span class="md-clipboard__message"></span></button><pre id="__code_101"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_101 pre, #__code_101 code"><span class="md-clipboard__message"></span></button><code><span class="n">sc</span> <span class="o">=</span> <span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">sc_result</span> <span class="o">=</span> <span class="n">sc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc_result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc_result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_102"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_102 pre, #__code_102 code"><span class="md-clipboard__message"></span></button><pre id="__code_103"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_103 pre, #__code_103 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">14.8830</span><span class="p">,</span>  <span class="mf">22.4106</span><span class="p">,</span> <span class="o">-</span><span class="mf">31.4739</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">21.0882</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0338</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.2588</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">25.1435</span><span class="p">,</span>   <span class="mf">2.9246</span><span class="p">,</span> <span class="o">-</span><span class="mf">16.1235</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">10.5069</span><span class="p">,</span>  <span class="o">-</span><span class="mf">7.1007</span><span class="p">,</span>  <span class="o">-</span><span class="mf">3.7396</span><span class="p">],</span>
         <span class="p">[</span>  <span class="mf">0.1374</span><span class="p">,</span>  <span class="mf">32.6438</span><span class="p">,</span>  <span class="mf">12.3680</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">12.0251</span><span class="p">,</span> <span class="o">-</span><span class="mf">40.5829</span><span class="p">,</span>   <span class="mf">2.2297</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">13.3123</span><span class="p">,</span>  <span class="mf">55.4689</span><span class="p">,</span>   <span class="mf">9.5420</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">12.6622</span><span class="p">,</span>  <span class="mf">23.4496</span><span class="p">,</span>  <span class="mf">21.1531</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">13.3533</span><span class="p">,</span>  <span class="mf">17.5674</span><span class="p">,</span> <span class="o">-</span><span class="mf">13.3354</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">29.1366</span><span class="p">,</span>  <span class="o">-</span><span class="mf">6.4898</span><span class="p">,</span>  <span class="mf">35.8614</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">35.2286</span><span class="p">,</span>  <span class="mf">18.7378</span><span class="p">,</span> <span class="o">-</span><span class="mf">31.4337</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">11.1726</span><span class="p">,</span>  <span class="mf">20.6372</span><span class="p">,</span>  <span class="mf">29.8689</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">30.7627</span><span class="p">,</span>   <span class="mf">0.0000</span><span class="p">,</span> <span class="o">-</span><span class="mf">57.0587</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">15.0724</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.7196</span><span class="p">,</span> <span class="o">-</span><span class="mf">18.6290</span><span class="p">],</span>
         <span class="p">[</span> <span class="o">-</span><span class="mf">2.7757</span><span class="p">,</span> <span class="o">-</span><span class="mf">19.6408</span><span class="p">,</span>   <span class="mf">0.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">12.7660</span><span class="p">,</span>  <span class="mf">21.6843</span><span class="p">,</span> <span class="o">-</span><span class="mf">35.4784</span><span class="p">]]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<ul>
<li>
<p>2.3.6 子层连接结构总结:</p>
<ul>
<li>什么是子层连接结构:<ul>
<li>如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构）, 在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了子层连接结构的类: SublayerConnection<ul>
<li>类的初始化函数输入参数是size, dropout, 分别代表词嵌入大小和置零比率.</li>
<li>它的实例化对象输入参数是x, sublayer, 分别代表上一层输出以及子层的函数表示.</li>
<li>它的输出就是通过子层连接结构处理的输出.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<h3 id="237">2.3.7 编码器层</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解编码器层的作用.</li>
<li>掌握编码器层的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>编码器层的作用:<ul>
<li>作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>编码器层的构成图:</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/17.png"></center><p></p>
<hr>
<ul>
<li>编码器层的代码分析:</li>
</ul>
<div class="codehilite" id="__code_104"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_104 pre, #__code_104 code"><span class="md-clipboard__message"></span></button><pre id="__code_105"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_105 pre, #__code_105 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 使用EncoderLayer类实现编码器层</span>
<span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="sd">"""它的初始化函数参数有四个，分别是size，其实就是我们词嵌入维度的大小，它也将作为我们编码器层的大小, </span>
<span class="sd">           第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制, </span>
<span class="sd">           第三个是feed_froward, 之后我们将传入前馈全连接层实例化对象, 最后一个是置0比率dropout."""</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># 首先将self_attn和feed_forward传入其中.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>

        <span class="c1"># 如图所示, 编码器层中有两个子层连接结构, 所以使用clones函数进行克隆</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># 把size传入其中</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="sd">"""forward函数中有两个输入参数，x和mask，分别代表上一层的输出，和掩码张量mask."""</span>
        <span class="c1"># 里面就是按照结构图左侧的流程. 首先通过第一个子层连接结构，其中包含多头自注意力子层，</span>
        <span class="c1"># 然后通过第二个子层连接结构，其中包含前馈全连接子层. 最后返回结果.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_106"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_106 pre, #__code_106 code"><span class="md-clipboard__message"></span></button><pre id="__code_107"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_107 pre, #__code_107 code"><span class="md-clipboard__message"></span></button><code><span class="k">size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">head</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">pe_result</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">2</span>
<span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="k">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_108"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_108 pre, #__code_108 code"><span class="md-clipboard__message"></span></button><pre id="__code_109"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_109 pre, #__code_109 code"><span class="md-clipboard__message"></span></button><code><span class="n">el</span> <span class="o">=</span> <span class="n">EncoderLayer</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">el_result</span> <span class="o">=</span> <span class="n">el</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">el_result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">el_result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_110"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_110 pre, #__code_110 code"><span class="md-clipboard__message"></span></button><pre id="__code_111"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_111 pre, #__code_111 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">33</span><span class="p">.</span><span class="mi">6988</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">.</span><span class="mi">7224</span><span class="p">,</span>  <span class="mi">20</span><span class="p">.</span><span class="mi">9575</span><span class="p">,</span>  <span class="p">...,</span>   <span class="mi">5</span><span class="p">.</span><span class="mi">2968</span><span class="p">,</span> <span class="o">-</span><span class="mi">48</span><span class="p">.</span><span class="mi">5658</span><span class="p">,</span>  <span class="mi">20</span><span class="p">.</span><span class="mi">0734</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">18</span><span class="p">.</span><span class="mi">1999</span><span class="p">,</span>  <span class="mi">34</span><span class="p">.</span><span class="mi">2358</span><span class="p">,</span>  <span class="mi">40</span><span class="p">.</span><span class="mi">3094</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">10</span><span class="p">.</span><span class="mi">1102</span><span class="p">,</span>  <span class="mi">58</span><span class="p">.</span><span class="mi">3381</span><span class="p">,</span>  <span class="mi">58</span><span class="p">.</span><span class="mi">4962</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">32</span><span class="p">.</span><span class="mi">1243</span><span class="p">,</span>  <span class="mi">16</span><span class="p">.</span><span class="mi">7921</span><span class="p">,</span>  <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">8024</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">23</span><span class="p">.</span><span class="mi">0022</span><span class="p">,</span> <span class="o">-</span><span class="mi">18</span><span class="p">.</span><span class="mi">1463</span><span class="p">,</span> <span class="o">-</span><span class="mi">17</span><span class="p">.</span><span class="mi">1263</span><span class="p">],</span>
         <span class="p">[</span> <span class="o">-</span><span class="mi">9</span><span class="p">.</span><span class="mi">3475</span><span class="p">,</span>  <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">3605</span><span class="p">,</span> <span class="o">-</span><span class="mi">55</span><span class="p">.</span><span class="mi">3494</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">43</span><span class="p">.</span><span class="mi">6333</span><span class="p">,</span>  <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">1900</span><span class="p">,</span>   <span class="mi">0</span><span class="p">.</span><span class="mi">1625</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mi">32</span><span class="p">.</span><span class="mi">8937</span><span class="p">,</span> <span class="o">-</span><span class="mi">46</span><span class="p">.</span><span class="mi">2808</span><span class="p">,</span>   <span class="mi">8</span><span class="p">.</span><span class="mi">5047</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">29</span><span class="p">.</span><span class="mi">1837</span><span class="p">,</span>  <span class="mi">22</span><span class="p">.</span><span class="mi">5962</span><span class="p">,</span> <span class="o">-</span><span class="mi">14</span><span class="p">.</span><span class="mi">4349</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">21</span><span class="p">.</span><span class="mi">3379</span><span class="p">,</span>  <span class="mi">20</span><span class="p">.</span><span class="mi">0657</span><span class="p">,</span> <span class="o">-</span><span class="mi">31</span><span class="p">.</span><span class="mi">7256</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">13</span><span class="p">.</span><span class="mi">4079</span><span class="p">,</span> <span class="o">-</span><span class="mi">44</span><span class="p">.</span><span class="mi">0706</span><span class="p">,</span>  <span class="o">-</span><span class="mi">9</span><span class="p">.</span><span class="mi">9504</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">19</span><span class="p">.</span><span class="mi">7478</span><span class="p">,</span>  <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">0848</span><span class="p">,</span>  <span class="mi">11</span><span class="p">.</span><span class="mi">8884</span><span class="p">,</span>  <span class="p">...,</span>  <span class="o">-</span><span class="mi">9</span><span class="p">.</span><span class="mi">5794</span><span class="p">,</span>   <span class="mi">0</span><span class="p">.</span><span class="mi">0675</span><span class="p">,</span>  <span class="o">-</span><span class="mi">4</span><span class="p">.</span><span class="mi">7123</span><span class="p">],</span>
         <span class="p">[</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">8023</span><span class="p">,</span> <span class="o">-</span><span class="mi">16</span><span class="p">.</span><span class="mi">1176</span><span class="p">,</span>  <span class="mi">20</span><span class="p">.</span><span class="mi">9476</span><span class="p">,</span>  <span class="p">...,</span>  <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">5469</span><span class="p">,</span>  <span class="mi">34</span><span class="p">.</span><span class="mi">8391</span><span class="p">,</span> <span class="o">-</span><span class="mi">14</span><span class="p">.</span><span class="mi">9798</span><span class="p">]]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="k">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<ul>
<li>
<p>2.3.7 编码器层总结:</p>
<ul>
<li>学习了编码器层的作用:<ul>
<li>作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了编码器层的类: EncoderLayer<ul>
<li>类的初始化函数共有4个, 别是size，其实就是我们词嵌入维度的大小. 第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制. 第三个是feed_froward, 之后我们将传入前馈全连接层实例化对象. 最后一个是置0比率dropout.</li>
<li>实例化对象的输入参数有2个，x代表来自上一层的输出, mask代表掩码张量.</li>
<li>它的输出代表经过整个编码层的特征表示.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<h3 id="238">2.3.8 编码器</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解编码器的作用.</li>
<li>掌握编码器的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>编码器的作用:<ul>
<li>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>编码器的结构图:</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/7.png"></center><p></p>
<hr>
<ul>
<li>编码器的代码分析:</li>
</ul>
<div class="codehilite" id="__code_112"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_112 pre, #__code_112 code"><span class="md-clipboard__message"></span></button><pre id="__code_113"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_113 pre, #__code_113 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 使用Encoder类来实现编码器</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="sd">"""初始化函数的两个参数分别代表编码器层和编码器层的个数"""</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 首先使用clones函数克隆N个编码器层放在self.layers中</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="c1"># 再初始化一个规范化层, 它将用在编码器的最后面.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="sd">"""forward函数的输入和编码器层相同, x代表上一层的输出, mask代表掩码张量"""</span>
        <span class="c1"># 首先就是对我们克隆的编码器层进行循环，每次都会得到一个新的x，</span>
        <span class="c1"># 这个循环的过程，就相当于输出的x经过了N个编码器层的处理. </span>
        <span class="c1"># 最后再通过规范化层的对象self.norm进行处理，最后返回结果. </span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_114"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_114 pre, #__code_114 code"><span class="md-clipboard__message"></span></button><pre id="__code_115"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_115 pre, #__code_115 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 第一个实例化参数layer, 它是一个编码器层的实例化对象, 因此需要传入编码器层的参数</span>
<span class="c1"># 又因为编码器层中的子层是不共享的, 因此需要使用深度拷贝各个对象.</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">head</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">layer</span> <span class="o">=</span> <span class="n">EncoderLayer</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">)</span>

<span class="c1"># 编码器中编码器层的个数N</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_116"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_116 pre, #__code_116 code"><span class="md-clipboard__message"></span></button><pre id="__code_117"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_117 pre, #__code_117 code"><span class="md-clipboard__message"></span></button><code><span class="n">en</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">en_result</span> <span class="o">=</span> <span class="n">en</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">en_result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">en_result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_118"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_118 pre, #__code_118 code"><span class="md-clipboard__message"></span></button><pre id="__code_119"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_119 pre, #__code_119 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">2081</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">3586</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">2353</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">5646</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">2851</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">0238</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">7957</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5481</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">2443</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">7927</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">6404</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">0484</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">1212</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">4320</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5644</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">3287</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">0935</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">6861</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">3937</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">6150</span><span class="p">,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">2394</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">5354</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">7981</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">7907</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">3005</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">3757</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">0360</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">4019</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">6493</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">1467</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">5653</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">1569</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">4075</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">3205</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">4774</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5856</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">0555</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">0061</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">8165</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">4339</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">8780</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">2467</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">1617</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">5532</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">4330</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">9433</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5304</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">7022</span><span class="p">]]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="k">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<ul>
<li>
<p>2.3.8 编码器总结:</p>
<ul>
<li>学习了编码器的作用:    <ul>
<li>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了编码器的类: Encoder<ul>
<li>类的初始化函数参数有两个，分别是layer和N，代表编码器层和编码器层的个数.</li>
<li>forward函数的输入参数也有两个, 和编码器层的forward相同, x代表上一层的输出, mask代码掩码张量.</li>
<li>编码器类的输出就是Transformer中编码器的特征提取表示, 它将成为解码器的输入的一部分.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<hr>
<h2 id="24">2.4 解码器部分实现</h2>
<hr>
<h3 id="_8">学习目标</h3>
<ul>
<li>了解解码器中各个组成部分的作用.</li>
<li>掌握解码器中各个组成部分的实现过程.</li>
</ul>
<hr>
<ul>
<li>解码器部分:<ul>
<li>由N个解码器层堆叠而成</li>
<li>每个解码器层由三个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li>
<li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/8.png"></center><p></p>
<hr>
<ul>
<li>说明:<ul>
<li>解码器层中的各个部分，如，多头注意力机制，规范化层，前馈全连接网络，子层连接结构都与编码器中的实现相同. 因此这里可以直接拿来构建解码器层.</li>
</ul>
</li>
</ul>
<hr>
<hr>
<h3 id="241">2.4.1 解码器层</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解解码器层的作用.</li>
<li>掌握解码器层的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>解码器层的作用:<ul>
<li>作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>解码器层的代码实现:</li>
</ul>
<div class="codehilite" id="__code_120"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_120 pre, #__code_120 code"><span class="md-clipboard__message"></span></button><pre id="__code_121"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_121 pre, #__code_121 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 使用DecoderLayer的类实现解码器层</span>
<span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="sd">"""初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，</span>
<span class="sd">            第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V， </span>
<span class="sd">            第三个是src_attn，多头注意力对象，这里Q!=K=V， 第四个是前馈全连接层对象，最后就是droupout置0比率.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 在初始化函数中， 主要就是将这些输入传到类中</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
        <span class="c1"># 按照结构图使用clones函数克隆三个子层连接对象.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">):</span>
        <span class="sd">"""forward函数中的参数有4个，分别是来自上一层的输入x，</span>
<span class="sd">           来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</span>
<span class="sd">        """</span>
        <span class="c1"># 将memory表示成m方便之后使用</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span>

        <span class="c1"># 将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，</span>
        <span class="c1"># 最后一个参数是目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据，</span>
        <span class="c1"># 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，</span>
        <span class="c1"># 但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词汇时，</span>
        <span class="c1"># 模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">))</span>

        <span class="c1"># 接着进入第二个子层，这个子层中常规的注意力机制，q是输入x; k，v是编码层输出memory， </span>
        <span class="c1"># 同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄漏，而是遮蔽掉对结果没有意义的字符而产生的注意力值，</span>
        <span class="c1"># 以此提升模型效果和训练速度. 这样就完成了第二个子层的处理.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">))</span>

        <span class="c1"># 最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果.这就是我们的解码器层结构.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_122"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_122 pre, #__code_122 code"><span class="md-clipboard__message"></span></button><pre id="__code_123"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_123 pre, #__code_123 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 类的实例化参数与解码器层类似, 相比多出了src_attn, 但是和self_attn是同一个类.</span>
<span class="n">head</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">self_attn</span> <span class="o">=</span> <span class="n">src_attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

<span class="c1"># 前馈全连接层也和之前相同 </span>
<span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_124"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_124 pre, #__code_124 code"><span class="md-clipboard__message"></span></button><pre id="__code_125"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_125 pre, #__code_125 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># x是来自目标数据的词嵌入表示, 但形式和源数据的词嵌入表示相同, 这里使用per充当.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">pe_result</span>

<span class="c1"># memory是来自编码器的输出</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">en_result</span>

<span class="c1"># 实际中source_mask和target_mask并不相同, 这里为了方便计算使他们都为mask</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">source_mask</span> <span class="o">=</span> <span class="n">target_mask</span> <span class="o">=</span> <span class="n">mask</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_126"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_126 pre, #__code_126 code"><span class="md-clipboard__message"></span></button><pre id="__code_127"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_127 pre, #__code_127 code"><span class="md-clipboard__message"></span></button><code><span class="n">dl</span> <span class="o">=</span> <span class="n">DecoderLayer</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">dl_result</span> <span class="o">=</span> <span class="n">dl</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">dl_result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">dl_result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_128"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_128 pre, #__code_128 code"><span class="md-clipboard__message"></span></button><pre id="__code_129"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_129 pre, #__code_129 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">1</span><span class="p">.</span><span class="mi">9604</span><span class="n">e</span><span class="o">+</span><span class="mi">00</span><span class="p">,</span>  <span class="mi">3</span><span class="p">.</span><span class="mi">9288</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">.</span><span class="mi">2422</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">1041</span><span class="n">e</span><span class="o">-</span><span class="mi">01</span><span class="p">,</span>
          <span class="o">-</span><span class="mi">5</span><span class="p">.</span><span class="mi">5063</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">5233</span><span class="n">e</span><span class="o">-</span><span class="mi">01</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">1</span><span class="p">.</span><span class="mi">0135</span><span class="n">e</span><span class="o">-</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">7779</span><span class="n">e</span><span class="o">-</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">6</span><span class="p">.</span><span class="mi">5491</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">8062</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>
          <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">7780</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">9577</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">1</span><span class="p">.</span><span class="mi">9526</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">5741</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">6926</span><span class="n">e</span><span class="o">-</span><span class="mi">01</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">5316</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>
           <span class="mi">1</span><span class="p">.</span><span class="mi">4543</span><span class="n">e</span><span class="o">+</span><span class="mi">00</span><span class="p">,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">7714</span><span class="n">e</span><span class="o">+</span><span class="mi">00</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">1528</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">0141</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">1999</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">2</span><span class="p">.</span><span class="mi">2099</span><span class="n">e</span><span class="o">+</span><span class="mi">00</span><span class="p">,</span>
          <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">7267</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">6687</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mi">6</span><span class="p">.</span><span class="mi">7259</span><span class="n">e</span><span class="o">+</span><span class="mi">00</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">6918</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">1807</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">6453</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>
          <span class="o">-</span><span class="mi">2</span><span class="p">.</span><span class="mi">9231</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">1288</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">7</span><span class="p">.</span><span class="mi">7484</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">.</span><span class="mi">0572</span><span class="n">e</span><span class="o">-</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">3096</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">3</span><span class="p">.</span><span class="mi">6302</span><span class="n">e</span><span class="o">-</span><span class="mi">01</span><span class="p">,</span>
           <span class="mi">1</span><span class="p">.</span><span class="mi">9907</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">2160</span><span class="n">e</span><span class="o">+</span><span class="mi">00</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">2</span><span class="p">.</span><span class="mi">6703</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">4</span><span class="p">.</span><span class="mi">4737</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">1590</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">4</span><span class="p">.</span><span class="mi">1540</span><span class="n">e</span><span class="o">-</span><span class="mi">03</span><span class="p">,</span>
           <span class="mi">5</span><span class="p">.</span><span class="mi">2587</span><span class="n">e</span><span class="o">+</span><span class="mi">00</span><span class="p">,</span>  <span class="mi">5</span><span class="p">.</span><span class="mi">2382</span><span class="n">e</span><span class="o">+</span><span class="mi">00</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">4</span><span class="p">.</span><span class="mi">7435</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">.</span><span class="mi">7599</span><span class="n">e</span><span class="o">-</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">5</span><span class="p">.</span><span class="mi">0898</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">5</span><span class="p">.</span><span class="mi">6361</span><span class="n">e</span><span class="o">+</span><span class="mi">00</span><span class="p">,</span>
           <span class="mi">3</span><span class="p">.</span><span class="mi">5891</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">5697</span><span class="n">e</span><span class="o">+</span><span class="mi">01</span><span class="p">]]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="k">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<ul>
<li>
<p>2.4.1 解码器层总结:</p>
<ul>
<li>学习了解码器层的作用:<ul>
<li>作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了解码器层的类: DecoderLayer<ul>
<li>类的初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，第三个是src_attn，多头注意力对象，这里Q!=K=V， 第四个是前馈全连接层对象，最后就是droupout置0比率.</li>
<li>forward函数的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</li>
<li>最终输出了由编码器输入和目标数据一同作用的特征提取结果.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<h3 id="242">2.4.2 解码器</h3>
<hr>
<ul>
<li>学习目标:<ul>
<li>了解解码器的作用.</li>
<li>掌握解码器的实现过程.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>解码器的作用:<ul>
<li>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的'值'进行特征表示.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>解码器的代码分析:</li>
</ul>
<div class="codehilite" id="__code_130"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_130 pre, #__code_130 code"><span class="md-clipboard__message"></span></button><pre id="__code_131"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_131 pre, #__code_131 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 使用类Decoder来实现解码器</span>
<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="sd">"""初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N."""</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 首先使用clones方法克隆了N个layer，然后实例化了一个规范化层. </span>
        <span class="c1"># 因为数据走过了所有的解码器层后最后要做规范化处理. </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">):</span>
        <span class="sd">"""forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，</span>
<span class="sd">           source_mask, target_mask代表源数据和目标数据的掩码张量"""</span>

        <span class="c1"># 然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，</span>
        <span class="c1"># 得出最后的结果，再进行一次规范化返回即可. </span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>


<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_132"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_132 pre, #__code_132 code"><span class="md-clipboard__message"></span></button><pre id="__code_133"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_133 pre, #__code_133 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 分别是解码器层layer和解码器层的个数N</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">head</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">d_ff</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">layer</span> <span class="o">=</span> <span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">8</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_134"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_134 pre, #__code_134 code"><span class="md-clipboard__message"></span></button><pre id="__code_135"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_135 pre, #__code_135 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 输入参数与解码器层的输入参数相同</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">pe_result</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">en_result</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">source_mask</span> <span class="o">=</span> <span class="n">target_mask</span> <span class="o">=</span> <span class="n">mask</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_136"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_136 pre, #__code_136 code"><span class="md-clipboard__message"></span></button><pre id="__code_137"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_137 pre, #__code_137 code"><span class="md-clipboard__message"></span></button><code><span class="n">de</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">de_result</span> <span class="o">=</span> <span class="n">de</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">de_result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">de_result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_138"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_138 pre, #__code_138 code"><span class="md-clipboard__message"></span></button><pre id="__code_139"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_139 pre, #__code_139 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.9898</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3216</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2439</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.7427</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0717</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0814</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.7432</span><span class="p">,</span>  <span class="mf">0.6985</span><span class="p">,</span>  <span class="mf">1.5551</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.5232</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5685</span><span class="p">,</span>  <span class="mf">1.3387</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.2149</span><span class="p">,</span>  <span class="mf">0.5274</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6414</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.7476</span><span class="p">,</span>  <span class="mf">0.5082</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0132</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.4408</span><span class="p">,</span>  <span class="mf">0.9416</span><span class="p">,</span>  <span class="mf">0.4522</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1506</span><span class="p">,</span>  <span class="mf">1.5591</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6453</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mf">0.9027</span><span class="p">,</span>  <span class="mf">0.5874</span><span class="p">,</span>  <span class="mf">0.6981</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">2.2899</span><span class="p">,</span>  <span class="mf">0.2933</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7508</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.2246</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0856</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2497</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2377</span><span class="p">,</span>  <span class="mf">0.0847</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0221</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.4012</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4181</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0968</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5427</span><span class="p">,</span>  <span class="mf">0.1090</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3882</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1050</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5140</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6494</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4358</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2173</span><span class="p">,</span>  <span class="mf">0.4161</span><span class="p">]]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<ul>
<li>
<p>2.4.2 解码器总结:</p>
<ul>
<li>学习了解码器的作用:<ul>
<li>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的'值'进行特征表示.</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>学习并实现了解码器的类: Decoder<ul>
<li>类的初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N.</li>
<li>forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，src_mask, tgt_mask代表源数据和目标数据的掩码张量.</li>
<li>输出解码过程的最终特征表示.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<hr>
<hr>
<h2 id="25">2.5 输出部分实现</h2>
<hr>
<h3 id="_9">学习目标</h3>
<ul>
<li>了解线性层和softmax的作用.</li>
<li>掌握线性层和softmax的实现过程.</li>
</ul>
<hr>
<ul>
<li>输出部分包含:<ul>
<li>线性层</li>
<li>softmax层</li>
</ul>
</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/6.png"></center><p></p>
<hr>
<h3 id="_10">线性层的作用</h3>
<ul>
<li>通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.</li>
</ul>
<hr>
<h3 id="softmax">softmax层的作用</h3>
<ul>
<li>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</li>
</ul>
<hr>
<ul>
<li>线性层和softmax层的代码分析:</li>
</ul>
<div class="codehilite" id="__code_140"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_140 pre, #__code_140 code"><span class="md-clipboard__message"></span></button><pre id="__code_141"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_141 pre, #__code_141 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># nn.functional工具包装载了网络层中那些只进行计算, 而没有参数的层</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>

<span class="c1"># 将线性层和softmax计算层一起实现, 因为二者的共同目标是生成最后的结构</span>
<span class="c1"># 因此把类的名字叫做Generator, 生成器类</span>
<span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
        <span class="sd">"""初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小."""</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 首先就是使用nn中的预定义线性层进行实例化, 得到一个对象self.project等待使用, </span>
        <span class="c1"># 这个线性层的参数有两个, 就是初始化函数传进来的两个参数: d_model, vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">project</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">"""前向逻辑函数中输入是上一层的输出张量x"""</span>
        <span class="c1"># 在函数中, 首先使用上一步得到的self.project对x进行线性变化, </span>
        <span class="c1"># 然后使用F中已经实现的log_softmax进行的softmax处理.</span>
        <span class="c1"># 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复.</span>
        <span class="c1"># log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数, </span>
        <span class="c1"># 因此对最终我们取最大的概率值没有影响. 最后返回结果即可.</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">project</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>


<hr>
<ul>
<li>nn.Linear演示:</li>
</ul>
<div class="codehilite" id="__code_142"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_142 pre, #__code_142 code"><span class="md-clipboard__message"></span></button><pre id="__code_143"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_143 pre, #__code_143 code"><span class="md-clipboard__message"></span></button><code><span class="o">&gt;&gt;&gt;</span> <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_144"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_144 pre, #__code_144 code"><span class="md-clipboard__message"></span></button><pre id="__code_145"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_145 pre, #__code_145 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 词嵌入维度是512维</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>

<span class="c1"># 词表大小是1000</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_146"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_146 pre, #__code_146 code"><span class="md-clipboard__message"></span></button><pre id="__code_147"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_147 pre, #__code_147 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 输入x是上一层网络的输出, 我们使用来自解码器层的输出</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">de_result</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_148"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_148 pre, #__code_148 code"><span class="md-clipboard__message"></span></button><pre id="__code_149"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_149 pre, #__code_149 code"><span class="md-clipboard__message"></span></button><code><span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
<span class="n">gen_result</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">gen_result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">gen_result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_150"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_150 pre, #__code_150 code"><span class="md-clipboard__message"></span></button><pre id="__code_151"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_151 pre, #__code_151 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">8098</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">5260</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">9244</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">6340</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">9026</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">5232</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">9093</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">3295</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">2972</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">6221</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">2268</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">0772</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">0263</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">2229</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">8533</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">7307</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">9294</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">3042</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">5045</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">0504</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">6241</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">5</span><span class="p">.</span><span class="mi">9063</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">5361</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">1484</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">1651</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">0224</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">4931</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">9565</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">.</span><span class="mi">0460</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">6490</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">3779</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">6133</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">.</span><span class="mi">3572</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">6565</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">1867</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">5112</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">4914</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">9289</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">2634</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">2471</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">5348</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">8541</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">8651</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">0460</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">6239</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">1411</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">.</span><span class="mi">5496</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">3749</span><span class="p">]]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">LogSoftmaxBackward</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="k">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
</code></pre></div>


<hr>
<h3 id="_11">小节总结</h3>
<ul>
<li>
<p>学习了输出部分包含:</p>
<ul>
<li>线性层</li>
<li>softmax层</li>
</ul>
<hr>
</li>
<li>
<p>线性层的作用:</p>
<ul>
<li>通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.</li>
</ul>
<hr>
</li>
<li>
<p>softmax层的作用:</p>
<ul>
<li>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</li>
</ul>
<hr>
</li>
<li>
<p>学习并实现了线性层和softmax层的类: Generator</p>
<ul>
<li>初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小.</li>
<li>forward函数接受上一层的输出.</li>
<li>最终获得经过线性层和softmax层处理的结果.</li>
</ul>
</li>
</ul>
<hr>
<hr>
<hr>
<h2 id="26">2.6 模型构建</h2>
<hr>
<h3 id="_12">学习目标</h3>
<ul>
<li>掌握编码器-解码器结构的实现过程.</li>
<li>掌握Transformer模型的构建过程.</li>
</ul>
<hr>
<ul>
<li>通过上面的小节, 我们已经完成了所有组成部分的实现, 接下来就来实现完整的编码器-解码器结构.</li>
</ul>
<hr>
<ul>
<li>Transformer总体架构图:</li>
</ul>
<p></p><center><img alt="avatar" src="./index2_files/4.png"></center><p></p>
<hr>
<h3 id="-">编码器-解码器结构的代码实现</h3>
<div class="codehilite" id="__code_152"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_152 pre, #__code_152 code"><span class="md-clipboard__message"></span></button><pre id="__code_153"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_153 pre, #__code_153 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 使用EncoderDecoder类来实现编码器-解码器结构</span>
<span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">source_embed</span><span class="p">,</span> <span class="n">target_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span>
        <span class="sd">"""初始化函数中有5个参数, 分别是编码器对象, 解码器对象, </span>
<span class="sd">           源数据嵌入函数, 目标数据嵌入函数,  以及输出部分的类别生成器对象</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 将参数传入到类中</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">source_embed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">target_embed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">):</span>
        <span class="sd">"""在forward函数中，有四个参数, source代表源数据, target代表目标数据, </span>
<span class="sd">           source_mask和target_mask代表对应的掩码张量"""</span>

        <span class="c1"># 在函数中, 将source, source_mask传入编码函数, 得到结果后,</span>
        <span class="c1"># 与source_mask，target，和target_mask一同传给解码函数.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">),</span> <span class="n">source_mask</span><span class="p">,</span>
                            <span class="n">target</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">):</span>
        <span class="sd">"""编码函数, 以source和source_mask为参数"""</span>
        <span class="c1"># 使用src_embed对source做处理, 然后和source_mask一起传给self.encoder</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">source</span><span class="p">),</span> <span class="n">source_mask</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">):</span>
        <span class="sd">"""解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数"""</span>
        <span class="c1"># 使用tgt_embed对target做处理, 然后和source_mask, target_mask, memory一起传给self.decoder</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>实例化参数</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_154"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_154 pre, #__code_154 code"><span class="md-clipboard__message"></span></button><pre id="__code_155"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_155 pre, #__code_155 code"><span class="md-clipboard__message"></span></button><code><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">en</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">de</span>
<span class="n">source_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">target_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">gen</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_156"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_156 pre, #__code_156 code"><span class="md-clipboard__message"></span></button><pre id="__code_157"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_157 pre, #__code_157 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 假设源数据与目标数据相同, 实际中并不相同</span>
<span class="n">source</span> <span class="o">=</span> <span class="n">target</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">421</span><span class="p">,</span> <span class="mi">508</span><span class="p">],</span> <span class="p">[</span><span class="mi">491</span><span class="p">,</span> <span class="mi">998</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">221</span><span class="p">]]))</span>

<span class="c1"># 假设src_mask与tgt_mask相同，实际中并不相同</span>
<span class="n">source_mask</span> <span class="o">=</span> <span class="n">target_mask</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_158"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_158 pre, #__code_158 code"><span class="md-clipboard__message"></span></button><pre id="__code_159"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_159 pre, #__code_159 code"><span class="md-clipboard__message"></span></button><code><span class="n">ed</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">source_embed</span><span class="p">,</span> <span class="n">target_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">)</span>
<span class="n">ed_result</span> <span class="o">=</span> <span class="n">ed</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ed_result</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ed_result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_160"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_160 pre, #__code_160 code"><span class="md-clipboard__message"></span></button><pre id="__code_161"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_161 pre, #__code_161 code"><span class="md-clipboard__message"></span></button><code><span class="n">tensor</span><span class="p">([[[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">2102</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">0826</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">0550</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">5555</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">3025</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">6296</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">8270</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5372</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">9559</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">3665</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">4338</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">7505</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">4956</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5133</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">9323</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">0773</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">1913</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">6240</span><span class="p">],</span>
         <span class="p">[</span> <span class="mi">0</span><span class="p">.</span><span class="mi">5770</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">6258</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">4833</span><span class="p">,</span>  <span class="p">...,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">1171</span><span class="p">,</span>  <span class="mi">1</span><span class="p">.</span><span class="mi">0069</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">9030</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">4355</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">7115</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">5685</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">6941</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">1878</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">1137</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">8867</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">2207</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">4151</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">9618</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">1722</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">9562</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">0946</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">9012</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">6388</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">2604</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">3357</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">6436</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">1204</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">4481</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">.</span><span class="mi">5888</span><span class="p">,</span>  <span class="p">...,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">8816</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">6497</span><span class="p">,</span>  <span class="mi">0</span><span class="p">.</span><span class="mi">0606</span><span class="p">]]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="k">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
</code></pre></div>


<hr>
<ul>
<li>接着将基于以上结构构建用于训练的模型.</li>
</ul>
<hr>
<h3 id="tansformer">Tansformer模型构建过程的代码分析</h3>
<div class="codehilite" id="__code_162"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_162 pre, #__code_162 code"><span class="md-clipboard__message"></span></button><pre id="__code_163"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_163 pre, #__code_163 code"><span class="md-clipboard__message"></span></button><code><span class="k">def</span> <span class="nf">make_model</span><span class="p">(</span><span class="n">source_vocab</span><span class="p">,</span> <span class="n">target_vocab</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> 
               <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="sd">"""该函数用来构建模型, 有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，</span>
<span class="sd">       编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，</span>
<span class="sd">       多头注意力结构中的多头数，以及置零比率dropout."""</span>

    <span class="c1"># 首先得到一个深度拷贝命令，接下来很多结构都需要进行深度拷贝，</span>
    <span class="c1"># 来保证他们彼此之间相互独立，不受干扰.</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span>

    <span class="c1"># 实例化了多头注意力类，得到对象attn</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># 然后实例化前馈全连接类，得到对象ff </span>
    <span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># 实例化位置编码类，得到对象position</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># 根据结构图, 最外层是EncoderDecoder，在EncoderDecoder中，</span>
    <span class="c1"># 分别是编码器层，解码器层，源数据Embedding层和位置编码组成的有序结构，</span>
    <span class="c1"># 目标数据Embedding层和位置编码组成的有序结构，以及类别生成器层. </span>
    <span class="c1"># 在编码器层中有attention子层以及前馈全连接子层，</span>
    <span class="c1"># 在解码器层中有两个attention子层以及前馈全连接层.</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span>
        <span class="n">Encoder</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span>
        <span class="n">Decoder</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> 
                             <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">source_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">target_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span>
        <span class="n">Generator</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">target_vocab</span><span class="p">))</span>

    <span class="c1"># 模型结构完成后，接下来就是初始化模型中的参数，比如线性层中的变换矩阵</span>
    <span class="c1"># 这里一但判断参数的维度大于1，则会将其初始化成一个服从均匀分布的矩阵，</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>


<hr>
<ul>
<li>nn.init.xavier_uniform演示:</li>
</ul>
<div class="codehilite" id="__code_164"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_164 pre, #__code_164 code"><span class="md-clipboard__message"></span></button><pre id="__code_165"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_165 pre, #__code_165 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 结果服从均匀分布U(-a, a)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.7742</span><span class="p">,</span>  <span class="mf">0.5413</span><span class="p">,</span>  <span class="mf">0.5478</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4806</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2555</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.8358</span><span class="p">,</span>  <span class="mf">0.4673</span><span class="p">,</span>  <span class="mf">0.3012</span><span class="p">,</span>  <span class="mf">0.3882</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6375</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.4622</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0794</span><span class="p">,</span>  <span class="mf">0.1851</span><span class="p">,</span>  <span class="mf">0.8462</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3591</span><span class="p">]])</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_166"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_166 pre, #__code_166 code"><span class="md-clipboard__message"></span></button><pre id="__code_167"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_167 pre, #__code_167 code"><span class="md-clipboard__message"></span></button><code><span class="n">source_vocab</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">target_vocab</span> <span class="o">=</span> <span class="mi">11</span> 
<span class="n">N</span> <span class="o">=</span> <span class="mi">6</span>
<span class="c1"># 其他参数都使用默认值 </span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_168"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_168 pre, #__code_168 code"><span class="md-clipboard__message"></span></button><pre id="__code_169"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_169 pre, #__code_169 code"><span class="md-clipboard__message"></span></button><code><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">'__main__'</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">make_model</span><span class="p">(</span><span class="n">source_vocab</span><span class="p">,</span> <span class="n">target_vocab</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div>


<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite" id="__code_170"><button class="md-clipboard" title="复制" data-clipboard-target="#__code_170 pre, #__code_170 code"><span class="md-clipboard__message"></span></button><pre id="__code_171"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_171 pre, #__code_171 code"><span class="md-clipboard__message"></span></button><code><span class="o">#</span> <span class="err">根据</span><span class="n">Transformer结构图构建的最终模型结构</span>
<span class="n">EncoderDecoder</span><span class="p">(</span>
  <span class="p">(</span><span class="n">encoder</span><span class="p">):</span> <span class="n">Encoder</span><span class="p">(</span>
    <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">EncoderLayer</span><span class="p">(</span>
        <span class="p">(</span><span class="n">self_attn</span><span class="p">):</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linears</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">feed_forward</span><span class="p">):</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span>
          <span class="p">(</span><span class="n">w_1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2048</span><span class="p">)</span>
          <span class="p">(</span><span class="n">w_2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">sublayer</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">EncoderLayer</span><span class="p">(</span>
        <span class="p">(</span><span class="n">self_attn</span><span class="p">):</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linears</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">feed_forward</span><span class="p">):</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span>
          <span class="p">(</span><span class="n">w_1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2048</span><span class="p">)</span>
          <span class="p">(</span><span class="n">w_2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">sublayer</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">decoder</span><span class="p">):</span> <span class="n">Decoder</span><span class="p">(</span>
    <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">DecoderLayer</span><span class="p">(</span>
        <span class="p">(</span><span class="n">self_attn</span><span class="p">):</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linears</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">src_attn</span><span class="p">):</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linears</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">feed_forward</span><span class="p">):</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span>
          <span class="p">(</span><span class="n">w_1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2048</span><span class="p">)</span>
          <span class="p">(</span><span class="n">w_2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">sublayer</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">DecoderLayer</span><span class="p">(</span>
        <span class="p">(</span><span class="n">self_attn</span><span class="p">):</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linears</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">src_attn</span><span class="p">):</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linears</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">feed_forward</span><span class="p">):</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span>
          <span class="p">(</span><span class="n">w_1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2048</span><span class="p">)</span>
          <span class="p">(</span><span class="n">w_2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
          <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">sublayer</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">SublayerConnection</span><span class="p">(</span>
            <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">(</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">src_embed</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Embeddings</span><span class="p">(</span>
      <span class="p">(</span><span class="n">lut</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">PositionalEncoding</span><span class="p">(</span>
      <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">tgt_embed</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Embeddings</span><span class="p">(</span>
      <span class="p">(</span><span class="n">lut</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">PositionalEncoding</span><span class="p">(</span>
      <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">generator</span><span class="p">):</span> <span class="n">Generator</span><span class="p">(</span>
    <span class="p">(</span><span class="n">proj</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</code></pre></div>


<hr>
<h3 id="_13">小节总结</h3>
<ul>
<li>
<p>学习并实现了编码器-解码器结构的类: EncoderDecoder</p>
<ul>
<li>类的初始化函数传入5个参数, 分别是编码器对象, 解码器对象, 源数据嵌入函数, 目标数据嵌入函数, 以及输出部分的类别生成器对象.</li>
<li>类中共实现三个函数, forward, encode, decode</li>
<li>forward是主要逻辑函数, 有四个参数, source代表源数据, target代表目标数据, source_mask和target_mask代表对应的掩码张量.</li>
<li>encode是编码函数, 以source和source_mask为参数. </li>
<li>decode是解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数</li>
</ul>
<hr>
</li>
<li>
<p>学习并实现了模型构建函数: make_model</p>
<ul>
<li>有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，多头注意力结构中的多头数，以及置零比率dropout.</li>
<li>该函数最后返回一个构建好的模型对象.</li>
</ul>
</li>
</ul>
<hr>
<hr>
<hr>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="./index.html" title="第一章:Transformer背景介绍" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  上一页
                </span>
                第一章:Transformer背景介绍
              </span>
            </div>
          </a>
        
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            ©Copyright 2019, itcast.cn.
          </div>
        
        powered by
        <a href="https://www.mkdocs.org/">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="./index2_files/font-awesome.css">
    
      <a href="https://www.linkedin.com/in/%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8-%E5%8C%97%E4%BA%AC%E6%A9%98%E6%98%9F-6bb7081a1/" class="md-footer-social__link fa fa-linkedin"></a>
    
      <a href="https://weibo.com/u/3469990762?is_all=1" class="md-footer-social__link fa fa-weibo"></a>
    
      <a href="http://bitbucket.org/AITutorials" class="md-footer-social__link fa fa-bitbucket"></a>
    
      <a href="https://github.com/AITutorials/datasets/issues" class="md-footer-social__link fa fa-gitlab"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="./index2_files/application.245445c6.js"></script>
      
        
        
          
          <script src="./index2_files/lunr.stemmer.support.js"></script>
          
            
              
                <script src="./index2_files/tinyseg.js"></script>
              
              
                <script src="./index2_files/lunr.ja.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.1.2",url:{base:".."}})</script>
      
    
  
</body></html>