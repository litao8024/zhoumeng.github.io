
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="canonical" href="https://www.tisv.cn/3/2.html">
      
      <link rel="icon" href="img/AI.jpg">
      <meta name="generator" content="mkdocs-1.2.2, mkdocs-material-7.2.3">
    
    
      
        <title>2. RNN经典案例 - 学习手册</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.f7f47774.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.3f5d1f46.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    <script>function __prefix(e){return new URL(".",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#21-rnn" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="学习手册" class="md-header__button md-logo" aria-label="学习手册" data-md-component="logo">
      
  <img src="img/AI.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            学习手册
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2. RNN经典案例
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
    <img src="assets/images/logo.svg" height="45px" alt="logo">

  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="学习手册" class="md-nav__button md-logo" aria-label="学习手册" data-md-component="logo">
      
  <img src="img/AI.jpg" alt="logo">

    </a>
    学习手册
  </label>
  
    <div class="md-nav__source">
      

<a href="https://github.com/AITutorials/manuals" title="前往 GitHub 仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="1.html" class="md-nav__link">
        1. RNN架构解析
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          2. RNN经典案例
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="2.html" class="md-nav__link md-nav__link--active">
        2. RNN经典案例
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#21-rnn" class="md-nav__link">
    2.1 使用RNN模型构建人名分类器
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-seq2seq" class="md-nav__link">
    2.2 使用seq2seq模型架构实现英译法任务
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#21-rnn" class="md-nav__link">
    2.1 使用RNN模型构建人名分类器
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22-seq2seq" class="md-nav__link">
    2.2 使用seq2seq模型架构实现英译法任务
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>2. RNN经典案例</h1>
                
                <h2 id="21-rnn">2.1 使用RNN模型构建人名分类器</h2>
<hr />
<ul>
<li>学习目标:<ul>
<li>了解有关人名分类问题和有关数据.</li>
<li>掌握使用RNN构建人名分类器实现过程.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>关于人名分类问题:<ul>
<li>以一个人名为输入, 使用模型帮助我们判断它最有可能是来自哪一个国家的人名, 这在某些国际化公司的业务中具有重要意义, 在用户注册过程中, 会根据用户填写的名字直接给他分配可能的国家或地区选项, 以及该国家或地区的国旗, 限制手机号码位数等等.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>人名分类数据:</li>
</ul>
<blockquote>
<ul>
<li>数据下载地址: https://download.pytorch.org/tutorial/data.zip</li>
<li>数据文件预览:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>- data/
    - names/
        Arabic.txt
        Chinese.txt
        Czech.txt
        Dutch.txt
        English.txt
        French.txt
        German.txt
        Greek.txt
        Irish.txt
        Italian.txt
        Japanese.txt
        Korean.txt
        Polish.txt
        Portuguese.txt
        Russian.txt
        Scottish.txt
        Spanish.txt
        Vietnamese.txt
</code></pre></div>

<blockquote>
<ul>
<li>Chiness.txt预览:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>Ang
Au-Yong
Bai
Ban
Bao
Bei
Bian
Bui
Cai
Cao
Cen
Chai
Chaim
Chan
Chang
Chao
Che
Chen
Cheng
</code></pre></div>

<hr />
<ul>
<li>整个案例的实现可分为以下五个步骤:<ul>
<li>第一步: 导入必备的工具包.</li>
<li>第二步: 对data文件中的数据进行处理，满足训练要求.</li>
<li>第三步: 构建RNN模型(包括传统RNN, LSTM以及GRU).</li>
<li>第四步: 构建训练函数并进行训练.</li>
<li>第五步: 构建评估函数并进行预测.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第一步: 导入必备的工具包</li>
</ul>
<blockquote>
<ul>
<li>python版本使用3.6.x, pytorch版本使用1.3.1</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>pip install torch==1.3.1
</code></pre></div>

<hr />
<div class="codehilite"><pre><span></span><code><span class="c1"># 从io中导入文件打开方法</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="nb">open</span>
<span class="c1"># 帮助使用正则表达式进行子目录的查询</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1"># 用于获得常见字母及字符规范化</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="c1"># 导入随机工具random</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="c1"># 导入时间和数学工具包</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="c1"># 导入torch工具</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="c1"># 导入nn准备构建模型</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="c1"># 引入制图工具包        </span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</code></pre></div>

<hr />
<ul>
<li>第二步: 对data文件中的数据进行处理，满足训练要求.</li>
</ul>
<blockquote>
<ul>
<li>获取常用的字符数量:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 获取所有常用字符包括字母和常用标点</span>
<span class="n">all_letters</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_letters</span> <span class="o">+</span> <span class="s2">&quot; .,;&#39;&quot;</span>

<span class="c1"># 获取常用字符数量</span>
<span class="n">n_letters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_letters</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;n_letter:&quot;</span><span class="p">,</span> <span class="n">n_letters</span><span class="p">)</span>
</code></pre></div>

<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">n_letter</span><span class="o">:</span> <span class="mi">57</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>字符规范化之unicode转Ascii函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 关于编码问题我们暂且不去考虑</span>
<span class="c1"># 我们认为这个函数的作用就是去掉一些语言中的重音标记</span>
<span class="c1"># 如: Ślusàrski ---&gt; Slusarski</span>
<span class="k">def</span> <span class="nf">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFD&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s1">&#39;Mn&#39;</span>
        <span class="ow">and</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">all_letters</span>
    <span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>s = &quot;Ślusàrski&quot;
a = unicodeToAscii(s)
print(a)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>Slusarski
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建一个从持久化文件中读取内容到内存的函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">data_path</span> <span class="o">=</span> <span class="s2">&quot;./data/name/&quot;</span>

<span class="k">def</span> <span class="nf">readLines</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;从文件中读取每一行加载到内存中形成列表&quot;&quot;&quot;</span>
    <span class="c1"># 打开指定文件并读取所有内容, 使用strip()去除两侧空白符, 然后以&#39;\n&#39;进行切分</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># 对应每一个lines列表中的名字进行Ascii转换, 使其规范化.最后返回一个名字列表</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">unicodeToAscii</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
</code></pre></div>

<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># filename是数据集中某个具体的文件, 我们这里选择Chinese.txt</span>
<span class="n">filename</span> <span class="o">=</span> <span class="n">data_path</span> <span class="o">+</span> <span class="s2">&quot;Chinese.txt&quot;</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">readLines</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
</code></pre></div>

<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>lines: [&#39;Ang&#39;, &#39;AuYong&#39;, &#39;Bai&#39;, &#39;Ban&#39;, &#39;Bao&#39;, &#39;Bei&#39;, &#39;Bian&#39;, &#39;Bui&#39;, &#39;Cai&#39;, &#39;Cao&#39;, &#39;Cen&#39;, &#39;Chai&#39;, &#39;Chaim&#39;, &#39;Chan&#39;, &#39;Chang&#39;, &#39;Chao&#39;, &#39;Che&#39;, &#39;Chen&#39;, &#39;Cheng&#39;, &#39;Cheung&#39;, &#39;Chew&#39;, &#39;Chieu&#39;, &#39;Chin&#39;, &#39;Chong&#39;, &#39;Chou&#39;, &#39;Chu&#39;, &#39;Cui&#39;, &#39;Dai&#39;, &#39;Deng&#39;, &#39;Ding&#39;, &#39;Dong&#39;, &#39;Dou&#39;, &#39;Duan&#39;, &#39;Eng&#39;, &#39;Fan&#39;, &#39;Fei&#39;, &#39;Feng&#39;, &#39;Foong&#39;, &#39;Fung&#39;, &#39;Gan&#39;, &#39;Gauk&#39;, &#39;Geng&#39;, &#39;Gim&#39;, &#39;Gok&#39;, &#39;Gong&#39;, &#39;Guan&#39;, &#39;Guang&#39;, &#39;Guo&#39;, &#39;Gwock&#39;, &#39;Han&#39;, &#39;Hang&#39;, &#39;Hao&#39;, &#39;Hew&#39;, &#39;Hiu&#39;, &#39;Hong&#39;, &#39;Hor&#39;, &#39;Hsiao&#39;, &#39;Hua&#39;, &#39;Huan&#39;, &#39;Huang&#39;, &#39;Hui&#39;, &#39;Huie&#39;, &#39;Huo&#39;, &#39;Jia&#39;, &#39;Jiang&#39;, &#39;Jin&#39;, &#39;Jing&#39;, &#39;Joe&#39;, &#39;Kang&#39;, &#39;Kau&#39;, &#39;Khoo&#39;, &#39;Khu&#39;, &#39;Kong&#39;, &#39;Koo&#39;, &#39;Kwan&#39;, &#39;Kwei&#39;, &#39;Kwong&#39;, &#39;Lai&#39;, &#39;Lam&#39;, &#39;Lang&#39;, &#39;Lau&#39;, &#39;Law&#39;, &#39;Lew&#39;, &#39;Lian&#39;, &#39;Liao&#39;, &#39;Lim&#39;, &#39;Lin&#39;, &#39;Ling&#39;, &#39;Liu&#39;, &#39;Loh&#39;, &#39;Long&#39;, &#39;Loong&#39;, &#39;Luo&#39;, &#39;Mah&#39;, &#39;Mai&#39;, &#39;Mak&#39;, &#39;Mao&#39;, &#39;Mar&#39;, &#39;Mei&#39;, &#39;Meng&#39;, &#39;Miao&#39;, &#39;Min&#39;, &#39;Ming&#39;, &#39;Moy&#39;, &#39;Mui&#39;, &#39;Nie&#39;, &#39;Niu&#39;, &#39;OuYang&#39;, &#39;OwYang&#39;, &#39;Pan&#39;, &#39;Pang&#39;, &#39;Pei&#39;, &#39;Peng&#39;, &#39;Ping&#39;, &#39;Qian&#39;, &#39;Qin&#39;, &#39;Qiu&#39;, &#39;Quan&#39;, &#39;Que&#39;, &#39;Ran&#39;, &#39;Rao&#39;, &#39;Rong&#39;, &#39;Ruan&#39;, &#39;Sam&#39;, &#39;Seah&#39;, &#39;See &#39;, &#39;Seow&#39;, &#39;Seto&#39;, &#39;Sha&#39;, &#39;Shan&#39;, &#39;Shang&#39;, &#39;Shao&#39;, &#39;Shaw&#39;, &#39;She&#39;, &#39;Shen&#39;, &#39;Sheng&#39;, &#39;Shi&#39;, &#39;Shu&#39;, &#39;Shuai&#39;, &#39;Shui&#39;, &#39;Shum&#39;, &#39;Siew&#39;, &#39;Siu&#39;, &#39;Song&#39;, &#39;Sum&#39;, &#39;Sun&#39;, &#39;Sze &#39;, &#39;Tan&#39;, &#39;Tang&#39;, &#39;Tao&#39;, &#39;Teng&#39;, &#39;Teoh&#39;, &#39;Thean&#39;, &#39;Thian&#39;, &#39;Thien&#39;, &#39;Tian&#39;, &#39;Tong&#39;, &#39;Tow&#39;, &#39;Tsang&#39;, &#39;Tse&#39;, &#39;Tsen&#39;, &#39;Tso&#39;, &#39;Tze&#39;, &#39;Wan&#39;, &#39;Wang&#39;, &#39;Wei&#39;, &#39;Wen&#39;, &#39;Weng&#39;, &#39;Won&#39;, &#39;Wong&#39;, &#39;Woo&#39;, &#39;Xiang&#39;, &#39;Xiao&#39;, &#39;Xie&#39;, &#39;Xing&#39;, &#39;Xue&#39;, &#39;Xun&#39;, &#39;Yan&#39;, &#39;Yang&#39;, &#39;Yao&#39;, &#39;Yap&#39;, &#39;Yau&#39;, &#39;Yee&#39;, &#39;Yep&#39;, &#39;Yim&#39;, &#39;Yin&#39;, &#39;Ying&#39;, &#39;Yong&#39;, &#39;You&#39;, &#39;Yuan&#39;, &#39;Zang&#39;, &#39;Zeng&#39;, &#39;Zha&#39;, &#39;Zhan&#39;, &#39;Zhang&#39;, &#39;Zhao&#39;, &#39;Zhen&#39;, &#39;Zheng&#39;, &#39;Zhong&#39;, &#39;Zhou&#39;, &#39;Zhu&#39;, &#39;Zhuo&#39;, &#39;Zong&#39;, &#39;Zou&#39;, &#39;Bing&#39;, &#39;Chi&#39;, &#39;Chu&#39;, &#39;Cong&#39;, &#39;Cuan&#39;, &#39;Dan&#39;, &#39;Fei&#39;, &#39;Feng&#39;, &#39;Gai&#39;, &#39;Gao&#39;, &#39;Gou&#39;, &#39;Guan&#39;, &#39;Gui&#39;, &#39;Guo&#39;, &#39;Hong&#39;, &#39;Hou&#39;, &#39;Huan&#39;, &#39;Jian&#39;, &#39;Jiao&#39;, &#39;Jin&#39;, &#39;Jiu&#39;, &#39;Juan&#39;, &#39;Jue&#39;, &#39;Kan&#39;, &#39;Kuai&#39;, &#39;Kuang&#39;, &#39;Kui&#39;, &#39;Lao&#39;, &#39;Liang&#39;, &#39;Lu&#39;, &#39;Luo&#39;, &#39;Man&#39;, &#39;Nao&#39;, &#39;Pian&#39;, &#39;Qiao&#39;, &#39;Qing&#39;, &#39;Qiu&#39;, &#39;Rang&#39;, &#39;Rui&#39;, &#39;She&#39;, &#39;Shi&#39;, &#39;Shuo&#39;, &#39;Sui&#39;, &#39;Tai&#39;, &#39;Wan&#39;, &#39;Wei&#39;, &#39;Xian&#39;, &#39;Xie&#39;, &#39;Xin&#39;, &#39;Xing&#39;, &#39;Xiong&#39;, &#39;Xuan&#39;, &#39;Yan&#39;, &#39;Yin&#39;, &#39;Ying&#39;, &#39;Yuan&#39;, &#39;Yue&#39;, &#39;Yun&#39;, &#39;Zha&#39;, &#39;Zhai&#39;, &#39;Zhang&#39;, &#39;Zhi&#39;, &#39;Zhuan&#39;, &#39;Zhui&#39;]
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建人名类别（所属的语言）列表与人名对应关系字典:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 构建的category_lines形如：{&quot;English&quot;:[&quot;Lily&quot;, &quot;Susan&quot;, &quot;Kobe&quot;], &quot;Chinese&quot;:[&quot;Zhang San&quot;, &quot;Xiao Ming&quot;]}</span>
<span class="n">category_lines</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># all_categories形如： [&quot;English&quot;,...,&quot;Chinese&quot;]</span>
<span class="n">all_categories</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 读取指定路径下的txt文件， 使用glob，path中可以使用正则表达式</span>
<span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">data_path</span> <span class="o">+</span> <span class="s1">&#39;*.txt&#39;</span><span class="p">):</span>
    <span class="c1"># 获取每个文件的文件名, 就是对应的名字类别</span>
    <span class="n">category</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filename</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># 将其逐一装到all_categories列表中</span>
    <span class="n">all_categories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">category</span><span class="p">)</span>
    <span class="c1"># 然后读取每个文件的内容，形成名字列表</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">readLines</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="c1"># 按照对应的类别，将名字列表写入到category_lines字典中</span>
    <span class="n">category_lines</span><span class="p">[</span><span class="n">category</span><span class="p">]</span> <span class="o">=</span> <span class="n">lines</span>


<span class="c1"># 查看类别总数</span>
<span class="n">n_categories</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_categories</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;n_categories:&quot;</span><span class="p">,</span> <span class="n">n_categories</span><span class="p">)</span>

<span class="c1"># 随便查看其中的一些内容</span>
<span class="nb">print</span><span class="p">(</span><span class="n">category_lines</span><span class="p">[</span><span class="s1">&#39;Italian&#39;</span><span class="p">][:</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">n_categories</span><span class="p">:</span> <span class="mi">18</span>
<span class="p">[</span><span class="s1">&#39;Abandonato&#39;</span><span class="p">,</span> <span class="s1">&#39;Abatangelo&#39;</span><span class="p">,</span> <span class="s1">&#39;Abatantuono&#39;</span><span class="p">,</span> <span class="s1">&#39;Abate&#39;</span><span class="p">,</span> <span class="s1">&#39;Abategiovanni&#39;</span><span class="p">]</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>将人名转化为对应onehot张量表示:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 将字符串(单词粒度)转化为张量表示，如：&quot;ab&quot; ---&gt;</span>
<span class="c1"># tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span>
<span class="c1">#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span>
<span class="c1">#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span>
<span class="c1">#          0., 0., 0., 0., 0., 0.]],</span>

<span class="c1">#        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span>
<span class="c1">#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span>
<span class="c1">#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,</span>
<span class="c1">#          0., 0., 0., 0., 0., 0.]]])</span>
<span class="k">def</span> <span class="nf">lineToTensor</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;将人名转化为对应onehot张量表示, 参数line是输入的人名&quot;&quot;&quot;</span>
    <span class="c1"># 首先初始化一个0张量, 它的形状(len(line), 1, n_letters) </span>
    <span class="c1"># 代表人名中的每个字母用一个1 x n_letters的张量表示.</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_letters</span><span class="p">)</span>
    <span class="c1"># 遍历这个人名中的每个字符索引和字符</span>
    <span class="k">for</span> <span class="n">li</span><span class="p">,</span> <span class="n">letter</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
        <span class="c1"># 使用字符串方法find找到每个字符在all_letters中的索引</span>
        <span class="c1"># 它也是我们生成onehot张量中1的索引位置</span>
        <span class="n">tensor</span><span class="p">[</span><span class="n">li</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">all_letters</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">letter</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># 返回结果</span>
    <span class="k">return</span> <span class="n">tensor</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">line</span> <span class="o">=</span> <span class="s2">&quot;Bai&quot;</span>
<span class="n">line_tensor</span> <span class="o">=</span> <span class="n">lineToTensor</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;line_tensot:&quot;</span><span class="p">,</span> <span class="n">line_tensor</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">line_tensot</span><span class="o">:</span> <span class="n">tensor</span><span class="o">([[[</span><span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span>
          <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">1</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span>
          <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span>
          <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.]],</span>

        <span class="o">[[</span><span class="mi">1</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span>
          <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span>
          <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span>
          <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.]],</span>

        <span class="o">[[</span><span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">1</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span>
          <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span>
          <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span>
          <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.,</span> <span class="mi">0</span><span class="o">.]]])</span>
</code></pre></div>

<hr />
<ul>
<li>第三步: 构建RNN模型</li>
</ul>
<blockquote>
<ul>
<li>构建传统的RNN模型:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用nn.RNN构建完成传统RNN使用类</span>

<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;初始化函数中有4个参数, 分别代表RNN输入最后一维尺寸, RNN的隐层最后一维尺寸, RNN层数&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>       
        <span class="c1"># 将hidden_size与num_layers传入其中</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>  

        <span class="c1"># 实例化预定义的nn.RNN, 它的三个参数分别是input_size, hidden_size, num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
        <span class="c1"># 实例化nn.Linear, 这个线性层用于将nn.RNN的输出维度转化为指定的输出维度</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="c1"># 实例化nn中预定的Softmax层, 用于从输出层获得类别结果</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;完成传统RNN中的主要逻辑, 输入参数input代表输入张量, 它的形状是1 x n_letters</span>
<span class="sd">           hidden代表RNN的隐层张量, 它的形状是self.num_layers x 1 x self.hidden_size&quot;&quot;&quot;</span>
        <span class="c1"># 因为预定义的nn.RNN要求输入维度一定是三维张量, 因此在这里使用unsqueeze(0)扩展一个维度</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># 将input和hidden输入到传统RNN的实例化对象中，如果num_layers=1, rr恒等于hn</span>
        <span class="n">rr</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="c1"># 将从RNN中获得的结果通过线性变换和softmax返回，同时返回hn作为后续RNN的输入</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">rr</span><span class="p">)),</span> <span class="n">hn</span>


    <span class="k">def</span> <span class="nf">initHidden</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;初始化隐层张量&quot;&quot;&quot;</span>
        <span class="c1"># 初始化一个（self.num_layers, 1, self.hidden_size）形状的0张量     </span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>  
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>torch.unsqueeze演示:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])
&gt;&gt;&gt; torch.unsqueeze(x, 0)
tensor([[ 1,  2,  3,  4]])
&gt;&gt;&gt; torch.unsqueeze(x, 1)
tensor([[ 1],
        [ 2],
        [ 3],
        [ 4]])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建LSTM模型:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用nn.LSTM构建完成LSTM使用类</span>

<span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;初始化函数的参数与传统RNN相同&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 将hidden_size与num_layers传入其中</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

        <span class="c1"># 实例化预定义的nn.LSTM</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
        <span class="c1"># 实例化nn.Linear, 这个线性层用于将nn.RNN的输出维度转化为指定的输出维度</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="c1"># 实例化nn中预定的Softmax层, 用于从输出层获得类别结果</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;在主要逻辑函数中多出一个参数c, 也就是LSTM中的细胞状态张量&quot;&quot;&quot;</span>
        <span class="c1"># 使用unsqueeze(0)扩展一个维度</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># 将input, hidden以及初始化的c传入lstm中</span>
        <span class="n">rr</span><span class="p">,</span> <span class="p">(</span><span class="n">hn</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
        <span class="c1"># 最后返回处理后的rr, hn, c</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">rr</span><span class="p">)),</span> <span class="n">hn</span><span class="p">,</span> <span class="n">c</span>

    <span class="k">def</span> <span class="nf">initHiddenAndC</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  
        <span class="sd">&quot;&quot;&quot;初始化函数不仅初始化hidden还要初始化细胞状态c, 它们形状相同&quot;&quot;&quot;</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">c</span>
</code></pre></div>

<blockquote>
<ul>
<li>构建GRU模型:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用nn.GRU构建完成传统RNN使用类</span>

<span class="c1"># GRU与传统RNN的外部形式相同, 都是只传递隐层张量, 因此只需要更改预定义层的名字</span>


<span class="k">class</span> <span class="nc">GRU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GRU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

        <span class="c1"># 实例化预定义的nn.GRU, 它的三个参数分别是input_size, hidden_size, num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">rr</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">rr</span><span class="p">)),</span> <span class="n">hn</span>

    <span class="k">def</span> <span class="nf">initHidden</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 因为是onehot编码, 输入张量最后一维的尺寸就是n_letters</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="n">n_letters</span>

<span class="c1"># 定义隐层的最后一维尺寸大小</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># 输出尺寸为语言类别总数n_categories</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="n">n_categories</span>

<span class="c1"># num_layer使用默认值, num_layers = 1</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 假如我们以一个字母B作为RNN的首次输入, 它通过lineToTensor转为张量</span>
<span class="c1"># 因为我们的lineToTensor输出是三维张量, 而RNN类需要的二维张量</span>
<span class="c1"># 因此需要使用squeeze(0)降低一个维度</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">lineToTensor</span><span class="p">(</span><span class="s1">&#39;B&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 初始化一个三维的隐层0张量, 也是初始的细胞状态张量</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">n_letters</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_categories</span><span class="p">)</span>
<span class="n">lstm</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">n_letters</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_categories</span><span class="p">)</span>
<span class="n">gru</span> <span class="o">=</span> <span class="n">GRU</span><span class="p">(</span><span class="n">n_letters</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_categories</span><span class="p">)</span>

<span class="n">rnn_output</span><span class="p">,</span> <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rnn:&quot;</span><span class="p">,</span> <span class="n">rnn_output</span><span class="p">)</span>
<span class="n">lstm_output</span><span class="p">,</span> <span class="n">next_hidden</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lstm:&quot;</span><span class="p">,</span> <span class="n">lstm_output</span><span class="p">)</span>
<span class="n">gru_output</span><span class="p">,</span> <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">gru</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gru:&quot;</span><span class="p">,</span> <span class="n">gru_output</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">rnn</span><span class="o">:</span> <span class="n">tensor</span><span class="o">([[[-</span><span class="mf">2.8822</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8615</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9488</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8898</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9205</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8113</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9328</span><span class="o">,</span>
          <span class="o">-</span><span class="mf">2.8239</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8678</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9474</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8724</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9703</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9019</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8871</span><span class="o">,</span>
          <span class="o">-</span><span class="mf">2.9340</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8436</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8442</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9047</span><span class="o">]]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">LogSoftmaxBackward</span><span class="o">&gt;)</span>
<span class="n">lstm</span><span class="o">:</span> <span class="n">tensor</span><span class="o">([[[-</span><span class="mf">2.9427</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8574</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9175</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8492</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8962</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9276</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8500</span><span class="o">,</span>
          <span class="o">-</span><span class="mf">2.9306</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8304</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9559</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9751</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8071</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9138</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8196</span><span class="o">,</span>
          <span class="o">-</span><span class="mf">2.8575</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8416</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9395</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9384</span><span class="o">]]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">LogSoftmaxBackward</span><span class="o">&gt;)</span>
<span class="n">gru</span><span class="o">:</span> <span class="n">tensor</span><span class="o">([[[-</span><span class="mf">2.8042</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8894</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8355</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8951</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8682</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9502</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9056</span><span class="o">,</span>
          <span class="o">-</span><span class="mf">2.8963</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8671</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9109</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9425</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8390</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9229</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8081</span><span class="o">,</span>
          <span class="o">-</span><span class="mf">2.8800</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9561</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9205</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9546</span><span class="o">]]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">LogSoftmaxBackward</span><span class="o">&gt;)</span>
</code></pre></div>

<hr />
<ul>
<li>第四步: 构建训练函数并进行训练</li>
</ul>
<blockquote>
<ul>
<li>从输出结果中获得指定类别函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">categoryFromOutput</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;从输出结果中获得指定类别, 参数为输出张量output&quot;&quot;&quot;</span>
    <span class="c1"># 从输出张量中返回最大的值和索引对象, 我们这里主要需要这个索引</span>
    <span class="n">top_n</span><span class="p">,</span> <span class="n">top_i</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># top_i对象中取出索引的值</span>
    <span class="n">category_i</span> <span class="o">=</span> <span class="n">top_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="c1"># 根据索引值获得对应语言类别, 返回语言类别和索引值</span>
    <span class="k">return</span> <span class="n">all_categories</span><span class="p">[</span><span class="n">category_i</span><span class="p">],</span> <span class="n">category_i</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>torch.topk演示:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>&gt;&gt;&gt; x = torch.arange(1., 6.)
&gt;&gt;&gt; x
tensor([ 1.,  2.,  3.,  4.,  5.])
&gt;&gt;&gt; torch.topk(x, 3)
torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 将上一步中gru的输出作为函数的输入</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">gru_output</span>
<span class="c1"># tensor([[[-2.8042, -2.8894, -2.8355, -2.8951, -2.8682, -2.9502, -2.9056,</span>
<span class="c1">#          -2.8963, -2.8671, -2.9109, -2.9425, -2.8390, -2.9229, -2.8081,</span>
<span class="c1">#          -2.8800, -2.9561, -2.9205, -2.9546]]], grad_fn=&lt;LogSoftmaxBackward&gt;)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">category</span><span class="p">,</span> <span class="n">category_i</span> <span class="o">=</span> <span class="n">categoryFromOutput</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;category:&quot;</span><span class="p">,</span> <span class="n">category</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;category_i:&quot;</span><span class="p">,</span> <span class="n">category_i</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">category</span><span class="o">:</span> <span class="n">Portuguese</span>
<span class="n">category_i</span><span class="o">:</span> <span class="mi">13</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>随机生成训练数据:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">randomTrainingExample</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;该函数用于随机产生训练数据&quot;&quot;&quot;</span>
    <span class="c1"># 首先使用random的choice方法从all_categories随机选择一个类别</span>
    <span class="n">category</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">all_categories</span><span class="p">)</span>
    <span class="c1"># 然后在通过category_lines字典取category类别对应的名字列表</span>
    <span class="c1"># 之后再从列表中随机取一个名字</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">category_lines</span><span class="p">[</span><span class="n">category</span><span class="p">])</span>
    <span class="c1"># 接着将这个类别在所有类别列表中的索引封装成tensor, 得到类别张量category_tensor</span>
    <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">all_categories</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">category</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="c1"># 最后, 将随机取到的名字通过函数lineToTensor转化为onehot张量表示</span>
    <span class="n">line_tensor</span> <span class="o">=</span> <span class="n">lineToTensor</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">category</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 我们随机取出十个进行结果查看</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">category</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span> <span class="o">=</span> <span class="n">randomTrainingExample</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;category =&#39;</span><span class="p">,</span> <span class="n">category</span><span class="p">,</span> <span class="s1">&#39;/ line =&#39;</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="s1">&#39;/ category_tensor =&#39;</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">category</span> <span class="o">=</span> <span class="n">French</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Fontaine</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">])</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">Italian</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Grimaldi</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">])</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">Chinese</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Zha</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">Italian</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Rapallino</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">])</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">Czech</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Sherak</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">Arabic</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Najjar</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">Scottish</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Brown</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">15</span><span class="p">])</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">Arabic</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Sarraf</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">Japanese</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Ibi</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="n">category</span> <span class="o">=</span> <span class="n">Chinese</span> <span class="o">/</span> <span class="n">line</span> <span class="o">=</span> <span class="n">Zha</span> <span class="o">/</span> <span class="n">category_tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建传统RNN训练函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 定义损失函数为nn.NLLLoss，因为RNN的最后一层是nn.LogSoftmax, 两者的内部计算逻辑正好能够吻合.  </span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

<span class="c1"># 设置学习率为0.005</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span> 

<span class="k">def</span> <span class="nf">trainRNN</span><span class="p">(</span><span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;定义训练函数, 它的两个参数是category_tensor类别的张量表示, 相当于训练数据的标签,</span>
<span class="sd">       line_tensor名字的张量表示, 相当于对应训练数据&quot;&quot;&quot;</span>

    <span class="c1"># 在函数中, 首先通过实例化对象rnn初始化隐层张量</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">initHidden</span><span class="p">()</span>

    <span class="c1"># 然后将模型结构中的梯度归0</span>
    <span class="n">rnn</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 下面开始进行训练, 将训练数据line_tensor的每个字符逐个传入rnn之中, 得到最终结果</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">line_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">)</span>

    <span class="c1"># 因为我们的rnn对象由nn.RNN实例化得到, 最终输出形状是三维张量, 为了满足于category_tensor</span>
    <span class="c1"># 进行对比计算损失, 需要减少第一个维度, 这里使用squeeze()方法</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">category_tensor</span><span class="p">)</span>

    <span class="c1"># 损失进行反向传播</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># 更新模型中所有的参数</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="c1"># 将参数的张量表示与参数的梯度乘以学习率的结果相加以此来更新参数</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="c1"># 返回结果和损失的值</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>torch.add演示:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>&gt;&gt;&gt; a = torch.randn(4)
&gt;&gt;&gt; a
tensor([-0.9732, -0.3497,  0.6245,  0.4022])
&gt;&gt;&gt; b = torch.randn(4, 1)
&gt;&gt;&gt; b
tensor([[ 0.3743],
        [-1.7724],
        [-0.5811],
        [-0.8017]])
&gt;&gt;&gt; torch.add(a, b, alpha=10)
tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
        [-18.6971, -18.0736, -17.0994, -17.3216],
        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建LSTM训练函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 与传统RNN相比多出细胞状态c</span>

<span class="k">def</span> <span class="nf">trainLSTM</span><span class="p">(</span><span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span><span class="p">):</span>
    <span class="n">hidden</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">lstm</span><span class="o">.</span><span class="n">initHiddenAndC</span><span class="p">()</span>
    <span class="n">lstm</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">line_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># 返回output, hidden以及细胞状态c</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">category_tensor</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">lstm</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建GRU训练函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 与传统RNN完全相同, 只不过名字改成了GRU</span>

<span class="k">def</span> <span class="nf">trainGRU</span><span class="p">(</span><span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span><span class="p">):</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">gru</span><span class="o">.</span><span class="n">initHidden</span><span class="p">()</span>
    <span class="n">gru</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">line_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span> <span class="n">gru</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">category_tensor</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">gru</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建时间计算函数: </li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">timeSince</span><span class="p">(</span><span class="n">since</span><span class="p">):</span>
    <span class="s2">&quot;获得每次打印的训练耗时, since是训练开始时间&quot;</span>
    <span class="c1"># 获得当前时间</span>
    <span class="n">now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># 获得时间差，就是训练耗时</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">now</span> <span class="o">-</span> <span class="n">since</span>
    <span class="c1"># 将秒转化为分钟, 并取整</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">s</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span>
    <span class="c1"># 计算剩下不够凑成1分钟的秒数</span>
    <span class="n">s</span> <span class="o">-=</span> <span class="n">m</span> <span class="o">*</span> <span class="mi">60</span>
    <span class="c1"># 返回指定格式的耗时</span>
    <span class="k">return</span> <span class="s1">&#39;</span><span class="si">%d</span><span class="s1">m </span><span class="si">%d</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 假定模型训练开始时间是10min之前</span>
<span class="n">since</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="mi">10</span><span class="o">*</span><span class="mi">60</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">period</span> <span class="o">=</span> <span class="n">timeSince</span><span class="p">(</span><span class="n">since</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">period</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="mf">10</span><span class="n">m</span> <span class="mf">0</span><span class="n">s</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建训练过程的日志打印函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 设置训练迭代次数</span>
<span class="n">n_iters</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="c1"># 设置结果的打印间隔</span>
<span class="n">print_every</span> <span class="o">=</span> <span class="mi">50</span>
<span class="c1"># 设置绘制损失曲线上的制图间隔</span>
<span class="n">plot_every</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">train_type_fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;训练过程的日志打印函数, 参数train_type_fn代表选择哪种模型训练函数, 如trainRNN&quot;&quot;&quot;</span>
    <span class="c1"># 每个制图间隔损失保存列表</span>
    <span class="n">all_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># 获得训练开始时间戳</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># 设置初始间隔损失为0</span>
    <span class="n">current_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># 从1开始进行训练迭代, 共n_iters次 </span>
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># 通过randomTrainingExample函数随机获取一组训练数据和对应的类别</span>
        <span class="n">category</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span> <span class="o">=</span> <span class="n">randomTrainingExample</span><span class="p">()</span>
        <span class="c1"># 将训练数据和对应类别的张量表示传入到train函数中</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">train_type_fn</span><span class="p">(</span><span class="n">category_tensor</span><span class="p">,</span> <span class="n">line_tensor</span><span class="p">)</span>      
        <span class="c1"># 计算制图间隔中的总损失</span>
        <span class="n">current_loss</span> <span class="o">+=</span> <span class="n">loss</span>   
        <span class="c1"># 如果迭代数能够整除打印间隔</span>
        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># 取该迭代步上的output通过categoryFromOutput函数获得对应的类别和类别索引</span>
            <span class="n">guess</span><span class="p">,</span> <span class="n">guess_i</span> <span class="o">=</span> <span class="n">categoryFromOutput</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="c1"># 然后和真实的类别category做比较, 如果相同则打对号, 否则打叉号.</span>
            <span class="n">correct</span> <span class="o">=</span> <span class="s1">&#39;✓&#39;</span> <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">category</span> <span class="k">else</span> <span class="s1">&#39;✗ (</span><span class="si">%s</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">category</span>
            <span class="c1"># 打印迭代步, 迭代步百分比, 当前训练耗时, 损失, 该步预测的名字, 以及是否正确                                </span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%d</span><span class="s1"> </span><span class="si">%d%%</span><span class="s1"> (</span><span class="si">%s</span><span class="s1">) </span><span class="si">%.4f</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> / </span><span class="si">%s</span><span class="s1"> </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="nb">iter</span> <span class="o">/</span> <span class="n">n_iters</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">timeSince</span><span class="p">(</span><span class="n">start</span><span class="p">),</span> <span class="n">loss</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">guess</span><span class="p">,</span> <span class="n">correct</span><span class="p">))</span>

        <span class="c1"># 如果迭代数能够整除制图间隔</span>
        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="n">plot_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># 将保存该间隔中的平均损失到all_losses列表中</span>
            <span class="n">all_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_loss</span> <span class="o">/</span> <span class="n">plot_every</span><span class="p">)</span>
            <span class="c1"># 间隔损失重置为0</span>
            <span class="n">current_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># 返回对应的总损失列表和训练耗时</span>
    <span class="k">return</span> <span class="n">all_losses</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>开始训练传统RNN, LSTM, GRU模型并制作对比图:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 调用train函数, 分别进行RNN, LSTM, GRU模型的训练</span>
<span class="c1"># 并返回各自的全部损失, 以及训练耗时用于制图</span>
<span class="n">all_losses1</span><span class="p">,</span> <span class="n">period1</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">trainRNN</span><span class="p">)</span>
<span class="n">all_losses2</span><span class="p">,</span> <span class="n">period2</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">trainLSTM</span><span class="p">)</span>
<span class="n">all_losses3</span><span class="p">,</span> <span class="n">period3</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">trainGRU</span><span class="p">)</span>

<span class="c1"># 绘制损失对比曲线, 训练耗时对比柱张图</span>
<span class="c1"># 创建画布0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># 绘制损失对比曲线</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">all_losses1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;RNN&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">all_losses2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;LSTM&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">all_losses3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;GRU&quot;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span> 


<span class="c1"># 创建画布1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_data</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;RNN&quot;</span><span class="p">,</span> <span class="s2">&quot;LSTM&quot;</span><span class="p">,</span> <span class="s2">&quot;GRU&quot;</span><span class="p">]</span> 
<span class="n">y_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">period1</span><span class="p">,</span> <span class="n">period2</span><span class="p">,</span> <span class="n">period3</span><span class="p">]</span>
<span class="c1"># 绘制训练耗时对比柱状图</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_data</span><span class="p">)),</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">tick_label</span><span class="o">=</span><span class="n">x_data</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>传统RNN训练日志输出:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="mf">5000</span> <span class="mf">5</span><span class="err">%</span> <span class="p">(</span><span class="mf">0</span><span class="n">m</span> <span class="mf">16</span><span class="n">s</span><span class="p">)</span> <span class="mf">3.2264</span> <span class="n">Carr</span> <span class="o">/</span> <span class="n">Chinese</span> <span class="err">✗</span> <span class="p">(</span><span class="n">English</span><span class="p">)</span>
<span class="mf">10000</span> <span class="mf">10</span><span class="err">%</span> <span class="p">(</span><span class="mf">0</span><span class="n">m</span> <span class="mf">30</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.2063</span> <span class="n">Biondi</span> <span class="o">/</span> <span class="n">Italian</span> <span class="err">✓</span>
<span class="mf">15000</span> <span class="mf">15</span><span class="err">%</span> <span class="p">(</span><span class="mf">0</span><span class="n">m</span> <span class="mf">47</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.4010</span> <span class="n">Palmeiro</span> <span class="o">/</span> <span class="n">Italian</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Portuguese</span><span class="p">)</span>
<span class="mf">20000</span> <span class="mf">20</span><span class="err">%</span> <span class="p">(</span><span class="mf">1</span><span class="n">m</span> <span class="mf">0</span><span class="n">s</span><span class="p">)</span> <span class="mf">3.8165</span> <span class="n">Konae</span> <span class="o">/</span> <span class="nb">Fre</span><span class="n">nch</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Japanese</span><span class="p">)</span>
<span class="mf">25000</span> <span class="mf">25</span><span class="err">%</span> <span class="p">(</span><span class="mf">1</span><span class="n">m</span> <span class="mf">17</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.5420</span> <span class="n">Koo</span> <span class="o">/</span> <span class="n">Korean</span> <span class="err">✓</span>
<span class="mf">30000</span> <span class="mf">30</span><span class="err">%</span> <span class="p">(</span><span class="mf">1</span><span class="n">m</span> <span class="mf">31</span><span class="n">s</span><span class="p">)</span> <span class="mf">5.6180</span> <span class="n">Fergus</span> <span class="o">/</span> <span class="n">Portuguese</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Irish</span><span class="p">)</span>
<span class="mf">35000</span> <span class="mf">35</span><span class="err">%</span> <span class="p">(</span><span class="mf">1</span><span class="n">m</span> <span class="mf">45</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.6073</span> <span class="n">Meeuwessen</span> <span class="o">/</span> <span class="n">Dutch</span> <span class="err">✓</span>
<span class="mf">40000</span> <span class="mf">40</span><span class="err">%</span> <span class="p">(</span><span class="mf">1</span><span class="n">m</span> <span class="mf">59</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.1356</span> <span class="n">Olan</span> <span class="o">/</span> <span class="n">Irish</span> <span class="err">✗</span> <span class="p">(</span><span class="n">English</span><span class="p">)</span>
<span class="mf">45000</span> <span class="mf">45</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">13</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.3352</span> <span class="n">Romijnders</span> <span class="o">/</span> <span class="n">Dutch</span> <span class="err">✓</span>
<span class="mf">50000</span> <span class="mf">50</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">26</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.1624</span> <span class="n">Flanagan</span> <span class="o">/</span> <span class="n">Irish</span> <span class="err">✓</span>
<span class="mf">55000</span> <span class="mf">55</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">40</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.4743</span> <span class="n">Dubhshlaine</span> <span class="o">/</span> <span class="n">Irish</span> <span class="err">✓</span>
<span class="mf">60000</span> <span class="mf">60</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">54</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.7260</span> <span class="n">Lee</span> <span class="o">/</span> <span class="n">Chinese</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Korean</span><span class="p">)</span>
<span class="mf">65000</span> <span class="mf">65</span><span class="err">%</span> <span class="p">(</span><span class="mf">3</span><span class="n">m</span> <span class="mf">8</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.2075</span> <span class="n">Rutherford</span> <span class="o">/</span> <span class="n">English</span> <span class="err">✓</span>
<span class="mf">70000</span> <span class="mf">70</span><span class="err">%</span> <span class="p">(</span><span class="mf">3</span><span class="n">m</span> <span class="mf">23</span><span class="n">s</span><span class="p">)</span> <span class="mf">3.6317</span> <span class="n">Han</span> <span class="o">/</span> <span class="n">Chinese</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Vietnamese</span><span class="p">)</span>
<span class="mf">75000</span> <span class="mf">75</span><span class="err">%</span> <span class="p">(</span><span class="mf">3</span><span class="n">m</span> <span class="mf">37</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.1779</span> <span class="n">Accorso</span> <span class="o">/</span> <span class="n">Italian</span> <span class="err">✓</span>
<span class="mf">80000</span> <span class="mf">80</span><span class="err">%</span> <span class="p">(</span><span class="mf">3</span><span class="n">m</span> <span class="mf">52</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.1095</span> <span class="n">O</span><span class="err">&#39;</span><span class="n">Brien</span> <span class="o">/</span> <span class="n">Irish</span> <span class="err">✓</span>
<span class="mf">85000</span> <span class="mf">85</span><span class="err">%</span> <span class="p">(</span><span class="mf">4</span><span class="n">m</span> <span class="mf">6</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.3845</span> <span class="n">Moran</span> <span class="o">/</span> <span class="n">Irish</span> <span class="err">✗</span> <span class="p">(</span><span class="n">English</span><span class="p">)</span>
<span class="mf">90000</span> <span class="mf">90</span><span class="err">%</span> <span class="p">(</span><span class="mf">4</span><span class="n">m</span> <span class="mf">21</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.3871</span> <span class="n">Xuan</span> <span class="o">/</span> <span class="n">Chinese</span> <span class="err">✓</span>
<span class="mf">95000</span> <span class="mf">95</span><span class="err">%</span> <span class="p">(</span><span class="mf">4</span><span class="n">m</span> <span class="mf">36</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.1104</span> <span class="n">Inoguchi</span> <span class="o">/</span> <span class="n">Japanese</span> <span class="err">✓</span>
<span class="mf">100000</span> <span class="mf">100</span><span class="err">%</span> <span class="p">(</span><span class="mf">4</span><span class="n">m</span> <span class="mf">52</span><span class="n">s</span><span class="p">)</span> <span class="mf">4.2142</span> <span class="n">Simon</span> <span class="o">/</span> <span class="nb">Fre</span><span class="n">nch</span> <span class="err">✓</span> <span class="p">(</span><span class="n">Irish</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>LSTM训练日志输出:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="mf">5000</span> <span class="mf">5</span><span class="err">%</span> <span class="p">(</span><span class="mf">0</span><span class="n">m</span> <span class="mf">25</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.8640</span> <span class="n">Fabian</span> <span class="o">/</span> <span class="n">Dutch</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Polish</span><span class="p">)</span>
<span class="mf">10000</span> <span class="mf">10</span><span class="err">%</span> <span class="p">(</span><span class="mf">0</span><span class="n">m</span> <span class="mf">48</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.9079</span> <span class="nb">Log</span><span class="n">in</span> <span class="o">/</span> <span class="n">Russian</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Irish</span><span class="p">)</span>
<span class="mf">15000</span> <span class="mf">15</span><span class="err">%</span> <span class="p">(</span><span class="mf">1</span><span class="n">m</span> <span class="mf">14</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.8223</span> <span class="n">Fernandes</span> <span class="o">/</span> <span class="n">Greek</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Portuguese</span><span class="p">)</span>
<span class="mf">20000</span> <span class="mf">20</span><span class="err">%</span> <span class="p">(</span><span class="mf">1</span><span class="n">m</span> <span class="mf">40</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.7069</span> <span class="n">Hudecek</span> <span class="o">/</span> <span class="n">Polish</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Czech</span><span class="p">)</span>
<span class="mf">25000</span> <span class="mf">25</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">4</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.6162</span> <span class="n">Acciaio</span> <span class="o">/</span> <span class="n">Czech</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Italian</span><span class="p">)</span>
<span class="mf">30000</span> <span class="mf">30</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">27</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.4044</span> <span class="n">Magalhaes</span> <span class="o">/</span> <span class="n">Greek</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Portuguese</span><span class="p">)</span>
<span class="mf">35000</span> <span class="mf">35</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">52</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.3030</span> <span class="n">Antoschenko</span> <span class="o">/</span> <span class="n">Russian</span> <span class="err">✓</span>
<span class="mf">40000</span> <span class="mf">40</span><span class="err">%</span> <span class="p">(</span><span class="mf">3</span><span class="n">m</span> <span class="mf">18</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.8912</span> <span class="n">Xing</span> <span class="o">/</span> <span class="n">Chinese</span> <span class="err">✓</span>
<span class="mf">45000</span> <span class="mf">45</span><span class="err">%</span> <span class="p">(</span><span class="mf">3</span><span class="n">m</span> <span class="mf">42</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.1788</span> <span class="n">Numata</span> <span class="o">/</span> <span class="n">Japanese</span> <span class="err">✓</span>
<span class="mf">50000</span> <span class="mf">50</span><span class="err">%</span> <span class="p">(</span><span class="mf">4</span><span class="n">m</span> <span class="mf">7</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.2863</span> <span class="n">Baz</span> <span class="o">/</span> <span class="n">Vietnamese</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Arabic</span><span class="p">)</span>
<span class="mf">55000</span> <span class="mf">55</span><span class="err">%</span> <span class="p">(</span><span class="mf">4</span><span class="n">m</span> <span class="mf">32</span><span class="n">s</span><span class="p">)</span> <span class="mf">3.2549</span> <span class="kr">Close</span> <span class="o">/</span> <span class="n">Dutch</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Greek</span><span class="p">)</span>
<span class="mf">60000</span> <span class="mf">60</span><span class="err">%</span> <span class="p">(</span><span class="mf">4</span><span class="n">m</span> <span class="mf">54</span><span class="n">s</span><span class="p">)</span> <span class="mf">4.5170</span> <span class="n">Pan</span> <span class="o">/</span> <span class="n">Vietnamese</span> <span class="err">✗</span> <span class="p">(</span><span class="nb">Fre</span><span class="n">nch</span><span class="p">)</span>
<span class="mf">65000</span> <span class="mf">65</span><span class="err">%</span> <span class="p">(</span><span class="mf">5</span><span class="n">m</span> <span class="mf">16</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.1503</span> <span class="n">San</span> <span class="o">/</span> <span class="n">Chinese</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Korean</span><span class="p">)</span>
<span class="mf">70000</span> <span class="mf">70</span><span class="err">%</span> <span class="p">(</span><span class="mf">5</span><span class="n">m</span> <span class="mf">39</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.2357</span> <span class="n">Pavlik</span> <span class="o">/</span> <span class="n">Polish</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Czech</span><span class="p">)</span>
<span class="mf">75000</span> <span class="mf">75</span><span class="err">%</span> <span class="p">(</span><span class="mf">6</span><span class="n">m</span> <span class="mf">2</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.3275</span> <span class="n">Alves</span> <span class="o">/</span> <span class="n">Portuguese</span> <span class="err">✗</span> <span class="p">(</span><span class="n">English</span><span class="p">)</span>
<span class="mf">80000</span> <span class="mf">80</span><span class="err">%</span> <span class="p">(</span><span class="mf">6</span><span class="n">m</span> <span class="mf">28</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.3294</span> <span class="n">Plamondon</span> <span class="o">/</span> <span class="n">Scottish</span> <span class="err">✗</span> <span class="p">(</span><span class="nb">Fre</span><span class="n">nch</span><span class="p">)</span>
<span class="mf">85000</span> <span class="mf">85</span><span class="err">%</span> <span class="p">(</span><span class="mf">6</span><span class="n">m</span> <span class="mf">54</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.7794</span> <span class="n">Water</span> <span class="o">/</span> <span class="nb">Fre</span><span class="n">nch</span> <span class="err">✗</span> <span class="p">(</span><span class="n">English</span><span class="p">)</span>
<span class="mf">90000</span> <span class="mf">90</span><span class="err">%</span> <span class="p">(</span><span class="mf">7</span><span class="n">m</span> <span class="mf">18</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.8021</span> <span class="n">Pereira</span> <span class="o">/</span> <span class="n">Portuguese</span> <span class="err">✓</span>
<span class="mf">95000</span> <span class="mf">95</span><span class="err">%</span> <span class="p">(</span><span class="mf">7</span><span class="n">m</span> <span class="mf">43</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.4374</span> <span class="n">Kunkel</span> <span class="o">/</span> <span class="n">German</span> <span class="err">✓</span>
<span class="mf">100000</span> <span class="mf">100</span><span class="err">%</span> <span class="p">(</span><span class="mf">8</span><span class="n">m</span> <span class="mf">5</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.2792</span> <span class="n">Taylor</span> <span class="o">/</span> <span class="n">Scottish</span> <span class="err">✓</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>GRU训练日志输出:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="mf">5000</span> <span class="mf">5</span><span class="err">%</span> <span class="p">(</span><span class="mf">0</span><span class="n">m</span> <span class="mf">22</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.8182</span> <span class="n">Bernard</span> <span class="o">/</span> <span class="n">Irish</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Polish</span><span class="p">)</span>
<span class="mf">10000</span> <span class="mf">10</span><span class="err">%</span> <span class="p">(</span><span class="mf">0</span><span class="n">m</span> <span class="mf">48</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.8966</span> <span class="n">Macias</span> <span class="o">/</span> <span class="n">Greek</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Spanish</span><span class="p">)</span>
<span class="mf">15000</span> <span class="mf">15</span><span class="err">%</span> <span class="p">(</span><span class="mf">1</span><span class="n">m</span> <span class="mf">13</span><span class="n">s</span><span class="p">)</span> <span class="mf">3.1046</span> <span class="n">Morcos</span> <span class="o">/</span> <span class="n">Greek</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Arabic</span><span class="p">)</span>
<span class="mf">20000</span> <span class="mf">20</span><span class="err">%</span> <span class="p">(</span><span class="mf">1</span><span class="n">m</span> <span class="mf">37</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.5359</span> <span class="n">Davlatov</span> <span class="o">/</span> <span class="n">Russian</span> <span class="err">✓</span>
<span class="mf">25000</span> <span class="mf">25</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">1</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.0999</span> <span class="n">Han</span> <span class="o">/</span> <span class="n">Vietnamese</span> <span class="err">✓</span>
<span class="mf">30000</span> <span class="mf">30</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">26</span><span class="n">s</span><span class="p">)</span> <span class="mf">4.1017</span> <span class="n">Chepel</span> <span class="o">/</span> <span class="n">German</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Russian</span><span class="p">)</span>
<span class="mf">35000</span> <span class="mf">35</span><span class="err">%</span> <span class="p">(</span><span class="mf">2</span><span class="n">m</span> <span class="mf">49</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.8765</span> <span class="n">Klein</span> <span class="o">/</span> <span class="n">Scottish</span> <span class="err">✗</span> <span class="p">(</span><span class="n">English</span><span class="p">)</span>
<span class="mf">40000</span> <span class="mf">40</span><span class="err">%</span> <span class="p">(</span><span class="mf">3</span><span class="n">m</span> <span class="mf">11</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.1265</span> <span class="n">an</span> <span class="o">/</span> <span class="n">Chinese</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Vietnamese</span><span class="p">)</span>
<span class="mf">45000</span> <span class="mf">45</span><span class="err">%</span> <span class="p">(</span><span class="mf">3</span><span class="n">m</span> <span class="mf">34</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.3511</span> <span class="n">Slusarski</span> <span class="o">/</span> <span class="n">Polish</span> <span class="err">✓</span>
<span class="mf">50000</span> <span class="mf">50</span><span class="err">%</span> <span class="p">(</span><span class="mf">3</span><span class="n">m</span> <span class="mf">59</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.9694</span> <span class="n">Than</span> <span class="o">/</span> <span class="n">Vietnamese</span> <span class="err">✓</span>
<span class="mf">55000</span> <span class="mf">55</span><span class="err">%</span> <span class="p">(</span><span class="mf">4</span><span class="n">m</span> <span class="mf">25</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.3576</span> <span class="n">Bokhoven</span> <span class="o">/</span> <span class="n">Russian</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Dutch</span><span class="p">)</span>
<span class="mf">60000</span> <span class="mf">60</span><span class="err">%</span> <span class="p">(</span><span class="mf">4</span><span class="n">m</span> <span class="mf">51</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.1344</span> <span class="n">Filipowski</span> <span class="o">/</span> <span class="n">Polish</span> <span class="err">✓</span>
<span class="mf">65000</span> <span class="mf">65</span><span class="err">%</span> <span class="p">(</span><span class="mf">5</span><span class="n">m</span> <span class="mf">15</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.4070</span> <span class="n">Reuter</span> <span class="o">/</span> <span class="n">German</span> <span class="err">✓</span>
<span class="mf">70000</span> <span class="mf">70</span><span class="err">%</span> <span class="p">(</span><span class="mf">5</span><span class="n">m</span> <span class="mf">37</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.8409</span> <span class="n">Guillory</span> <span class="o">/</span> <span class="n">Irish</span> <span class="err">✗</span> <span class="p">(</span><span class="nb">Fre</span><span class="n">nch</span><span class="p">)</span>
<span class="mf">75000</span> <span class="mf">75</span><span class="err">%</span> <span class="p">(</span><span class="mf">6</span><span class="n">m</span> <span class="mf">0</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.6882</span> <span class="n">Song</span> <span class="o">/</span> <span class="n">Korean</span> <span class="err">✓</span>
<span class="mf">80000</span> <span class="mf">80</span><span class="err">%</span> <span class="p">(</span><span class="mf">6</span><span class="n">m</span> <span class="mf">22</span><span class="n">s</span><span class="p">)</span> <span class="mf">5.0092</span> <span class="n">Maly</span> <span class="o">/</span> <span class="n">Scottish</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Polish</span><span class="p">)</span>
<span class="mf">85000</span> <span class="mf">85</span><span class="err">%</span> <span class="p">(</span><span class="mf">6</span><span class="n">m</span> <span class="mf">43</span><span class="n">s</span><span class="p">)</span> <span class="mf">2.4570</span> <span class="n">Sai</span> <span class="o">/</span> <span class="n">Chinese</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Vietnamese</span><span class="p">)</span>
<span class="mf">90000</span> <span class="mf">90</span><span class="err">%</span> <span class="p">(</span><span class="mf">7</span><span class="n">m</span> <span class="mf">5</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.2006</span> <span class="n">Heel</span> <span class="o">/</span> <span class="n">German</span> <span class="err">✗</span> <span class="p">(</span><span class="n">Dutch</span><span class="p">)</span>
<span class="mf">95000</span> <span class="mf">95</span><span class="err">%</span> <span class="p">(</span><span class="mf">7</span><span class="n">m</span> <span class="mf">27</span><span class="n">s</span><span class="p">)</span> <span class="mf">0.9144</span> <span class="n">Doan</span> <span class="o">/</span> <span class="n">Vietnamese</span> <span class="err">✓</span>
<span class="mf">100000</span> <span class="mf">100</span><span class="err">%</span> <span class="p">(</span><span class="mf">7</span><span class="n">m</span> <span class="mf">50</span><span class="n">s</span><span class="p">)</span> <span class="mf">1.1320</span> <span class="n">Crespo</span> <span class="o">/</span> <span class="n">Portuguese</span> <span class="err">✓</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>损失对比曲线:</li>
</ul>
</blockquote>
<p><center><img alt="avatar" src="img/compared_loss.png" /></center></p>
<hr />
<blockquote>
<ul>
<li>损失对比曲线分析:<ul>
<li>模型训练的损失降低快慢代表模型收敛程度, 由图可知, 传统RNN的模型收敛情况最好, 然后是GRU, 最后是LSTM, 这是因为: 我们当前处理的文本数据是人名, 他们的长度有限, 且长距离字母间基本无特定关联, 因此无法发挥改进模型LSTM和GRU的长距离捕捉语义关联的优势. 所以在以后的模型选用时, 要通过对任务的分析以及实验对比, 选择最适合的模型.</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<blockquote>
<ul>
<li>训练耗时对比图:</li>
</ul>
</blockquote>
<p><center><img alt="avatar" src="img/compared_period.png" /></center></p>
<hr />
<blockquote>
<ul>
<li>训练耗时对比图分析:<ul>
<li>模型训练的耗时长短代表模型的计算复杂度, 由图可知, 也正如我们之前的理论分析, 传统RNN复杂度最低, 耗时几乎只是后两者的一半, 然后是GRU, 最后是复杂度最高的LSTM.</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<blockquote>
<ul>
<li>结论:<ul>
<li>模型选用一般应通过实验对比, 并非越复杂或越先进的模型表现越好, 而是需要结合自己的特定任务, 从对数据的分析和实验结果中获得最佳答案.</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<ul>
<li>第五步: 构建评估函数并进行预测</li>
</ul>
<blockquote>
<ul>
<li>构建传统RNN评估函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">evaluateRNN</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">    </span><span class="ss">&quot;&quot;&quot;评估函数, 和训练函数逻辑相同, 参数是line_tensor代表名字的张量表示&quot;&quot;&quot;</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">初始化隐层张量</span><span class="w"></span>
<span class="w">    </span><span class="n">hidden</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnn</span><span class="p">.</span><span class="n">initHidden</span><span class="p">()</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">将评估数据line_tensor的每个字符逐个传入rnn之中</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">.</span><span class="k">size</span><span class="p">()</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">        </span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="n">hidden</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rnn</span><span class="p">(</span><span class="n">line_tensor</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">hidden</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">获得输出结果</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">output</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建LSTM评估函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">evaluateLSTM</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">):</span>
    <span class="c1"># 初始化隐层张量和细胞状态张量</span>
    <span class="n">hidden</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">lstm</span><span class="o">.</span><span class="n">initHiddenAndC</span><span class="p">()</span>
    <span class="c1"># 将评估数据line_tensor的每个字符逐个传入lstm之中</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">line_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建GRU评估函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">evaluateGRU</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">):</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">gru</span><span class="o">.</span><span class="n">initHidden</span><span class="p">()</span>
    <span class="c1"># 将评估数据line_tensor的每个字符逐个传入gru之中</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">line_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">gru</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">line</span> <span class="o">=</span> <span class="s2">&quot;Bai&quot;</span>
<span class="n">line_tensor</span> <span class="o">=</span> <span class="n">lineToTensor</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">rnn_output</span> <span class="o">=</span> <span class="n">evaluateRNN</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">)</span>
<span class="n">lstm_output</span> <span class="o">=</span> <span class="n">evaluateLSTM</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">)</span>
<span class="n">gru_output</span> <span class="o">=</span> <span class="n">evaluateGRU</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rnn_output:&quot;</span><span class="p">,</span> <span class="n">rnn_output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gru_output:&quot;</span><span class="p">,</span> <span class="n">lstm_output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;gru_output:&quot;</span><span class="p">,</span> <span class="n">gru_output</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">rnn_output</span><span class="o">:</span> <span class="n">tensor</span><span class="o">([[-</span><span class="mf">2.8923</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.7665</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8640</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.7907</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9919</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9482</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8809</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9526</span><span class="o">,</span>
         <span class="o">-</span><span class="mf">2.9445</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8115</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9544</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9043</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8016</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8668</span><span class="o">,</span> <span class="o">-</span><span class="mf">3.0484</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9382</span><span class="o">,</span>
         <span class="o">-</span><span class="mf">2.9935</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.7393</span><span class="o">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SqueezeBackward1</span><span class="o">&gt;)</span>
<span class="n">gru_output</span><span class="o">:</span> <span class="n">tensor</span><span class="o">([[-</span><span class="mf">2.9498</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9455</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8981</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.7791</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8915</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8534</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8637</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8902</span><span class="o">,</span>
         <span class="o">-</span><span class="mf">2.9537</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8834</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8973</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9711</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8622</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9001</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9149</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8762</span><span class="o">,</span>
         <span class="o">-</span><span class="mf">2.8286</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8866</span><span class="o">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SqueezeBackward1</span><span class="o">&gt;)</span>
<span class="n">gru_output</span><span class="o">:</span> <span class="n">tensor</span><span class="o">([[-</span><span class="mf">2.8781</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9347</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.7355</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9662</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9404</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9600</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8810</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8000</span><span class="o">,</span>
         <span class="o">-</span><span class="mf">2.8151</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9132</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.7564</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8849</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9814</span><span class="o">,</span> <span class="o">-</span><span class="mf">3.0499</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9153</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.8190</span><span class="o">,</span>
         <span class="o">-</span><span class="mf">2.8841</span><span class="o">,</span> <span class="o">-</span><span class="mf">2.9706</span><span class="o">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SqueezeBackward1</span><span class="o">&gt;)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>构建预测函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">input_line</span><span class="p">,</span> <span class="n">evaluate</span><span class="p">,</span> <span class="n">n_predictions</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;预测函数, 输入参数input_line代表输入的名字, </span>
<span class="sd">       n_predictions代表需要取最有可能的top个&quot;&quot;&quot;</span>
    <span class="c1"># 首先打印输入</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">input_line</span><span class="p">)</span>

    <span class="c1"># 以下操作的相关张量不进行求梯度</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># 使输入的名字转换为张量表示, 并使用evaluate函数获得预测输出</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">lineToTensor</span><span class="p">(</span><span class="n">input_line</span><span class="p">))</span>

        <span class="c1"># 从预测的输出中取前3个最大的值及其索引</span>
        <span class="n">topv</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">n_predictions</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="c1"># 创建盛装结果的列表</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># 遍历n_predictions</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_predictions</span><span class="p">):</span>
            <span class="c1"># 从topv中取出的output值</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">topv</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="c1"># 取出索引并找到对应的类别</span>
            <span class="n">category_index</span> <span class="o">=</span> <span class="n">topi</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="c1"># 打印ouput的值, 和对应的类别</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(</span><span class="si">%.2f</span><span class="s1">) </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">all_categories</span><span class="p">[</span><span class="n">category_index</span><span class="p">]))</span>
            <span class="c1"># 将结果装进predictions中</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">all_categories</span><span class="p">[</span><span class="n">category_index</span><span class="p">]])</span>
</code></pre></div>

<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="k">for</span> <span class="n">evaluate_fn</span> <span class="ow">in</span> <span class="p">[</span><span class="n">evaluateRNN</span><span class="p">,</span> <span class="n">evaluateLSTM</span><span class="p">,</span> <span class="n">evaluateGRU</span><span class="p">]:</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">predict</span><span class="p">(</span><span class="s1">&#39;Dovesky&#39;</span><span class="p">,</span> <span class="n">evaluate_fn</span><span class="p">)</span>
    <span class="n">predict</span><span class="p">(</span><span class="s1">&#39;Jackson&#39;</span><span class="p">,</span> <span class="n">evaluate_fn</span><span class="p">)</span>
    <span class="n">predict</span><span class="p">(</span><span class="s1">&#39;Satoshi&#39;</span><span class="p">,</span> <span class="n">evaluate_fn</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>------------------
&gt; Dovesky
(-0.58) Russian
(-1.40) Czech
(-2.52) Scottish

&gt; Jackson
(-0.27) Scottish
(-1.71) English
(-4.14) French

&gt; Satoshi
(-0.02) Japanese
(-5.10) Polish
(-5.42) Arabic
------------------

&gt; Dovesky
(-1.03) Russian
(-1.12) Czech
(-2.22) Polish

&gt; Jackson
(-0.37) Scottish
(-2.17) English
(-2.81) Czech

&gt; Satoshi
(-0.29) Japanese
(-1.90) Arabic
(-3.20) Polish
------------------

&gt; Dovesky
(-0.44) Russian
(-1.55) Czech
(-3.06) Polish

&gt; Jackson
(-0.39) Scottish
(-1.91) English
(-3.10) Polish

&gt; Satoshi
(-0.43) Japanese
(-1.22) Arabic
(-3.85) Italian
</code></pre></div>

<hr />
<ul>
<li>
<p>小节总结:</p>
<ul>
<li>学习了关于人名分类问题:
    以一个人名为输入, 使用模型帮助我们判断它最有可能是来自哪一个国家的人名, 这在某些国际化公司的业务中具有重要意义, 在用户注册过程中, 会根据用户填写的名字直接给他分配可能的国家或地区选项, 以及该国家或地区的国旗, 限制手机号码位数等等.</li>
</ul>
<hr />
<ul>
<li>人名分类器的实现可分为以下五个步骤:<ul>
<li>第一步: 导入必备的工具包.</li>
<li>第二步: 对data文件中的数据进行处理，满足训练要求.</li>
<li>第三步: 构建RNN模型(包括传统RNN, LSTM以及GRU).</li>
<li>第四步: 构建训练函数并进行训练.</li>
<li>第五步: 构建评估函数并进行预测.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第一步: 导入必备的工具包<ul>
<li>python版本使用3.6.x, pytorch版本使用1.3.1</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第二步: 对data文件中的数据进行处理，满足训练要求<ul>
<li>定义数据集路径并获取常用的字符数量.</li>
<li>字符规范化之unicode转Ascii函数unicodeToAscii.</li>
<li>构建一个从持久化文件中读取内容到内存的函数readLines.</li>
<li>构建人名类别（所属的语言）列表与人名对应关系字典  </li>
<li>将人名转化为对应onehot张量表示函数lineToTensor</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第三步: 构建RNN模型<ul>
<li>构建传统的RNN模型的类class RNN.</li>
<li>构建LSTM模型的类class LSTM.</li>
<li>构建GRU模型的类class GRU. </li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第四步: 构建训练函数并进行训练<ul>
<li>从输出结果中获得指定类别函数categoryFromOutput.</li>
<li>随机生成训练数据函数randomTrainingExample.</li>
<li>构建传统RNN训练函数trainRNN.</li>
<li>构建LSTM训练函数trainLSTM.</li>
<li>构建GRU训练函数trainGRU.</li>
<li>构建时间计算函数timeSince.</li>
<li>构建训练过程的日志打印函数train.得到损失对比曲线和训练耗时对比图.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>损失对比曲线分析:<ul>
<li>模型训练的损失降低快慢代表模型收敛程度, 由图可知, 传统RNN的模型收敛情况最好, 然后是GRU, 最后是LSTM, 这是因为: 我们当前处理的文本数据是人名, 他们的长度有限, 且长距离字母间基本无特定关联, 因此无法发挥改进模型LSTM和GRU的长距离捕捉语义关联的优势. 所以在以后的模型选用时, 要通过对任务的分析以及实验对比, 选择最适合的模型.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>训练耗时对比图分析:<ul>
<li>模型训练的耗时长短代表模型的计算复杂度, 由图可知, 也正如我们之前的理论分析, 传统RNN复杂度最低, 耗时几乎只是后两者的一半, 然后是GRU, 最后是复杂度最高的LSTM.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>结论:<ul>
<li>模型选用一般应通过实验对比, 并非越复杂或越先进的模型表现越好, 而是需要结合自己的特定任务, 从对数据的分析和实验结果中获得最佳答案.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第五步: 构建评估函数并进行预测<ul>
<li>构建传统RNN评估函数evaluateRNN.</li>
<li>构建LSTM评估函数evaluateLSTM.</li>
<li>构建GRU评估函数evaluateGRU.</li>
<li>构建预测函数predict.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<hr />
<hr />
<h2 id="22-seq2seq">2.2 使用seq2seq模型架构实现英译法任务</h2>
<ul>
<li>学习目标:<ul>
<li>更深一步了解seq2seq模型架构和翻译数据集.</li>
<li>掌握使用基于GRU的seq2seq模型架构实现翻译的过程.</li>
<li>掌握Attention机制在解码器端的实现过程.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>seq2seq模型架构:</li>
</ul>
<p><center><img alt="avatar" src="img/s2s.png" /></center></p>
<hr />
<ul>
<li>seq2seq模型架构分析:<ul>
<li>从图中可知, seq2seq模型架构, 包括两部分分别是encoder(编码器)和decoder(解码器), 编码器和解码器的内部实现都使用了GRU模型, 这里它要完成的是一个中文到英文的翻译: 欢迎 来 北京 --&gt; welcome to BeiJing. 编码器首先处理中文输入"欢迎 来 北京", 通过GRU模型获得每个时间步的输出张量，最后将它们拼接成一个中间语义张量c, 接着解码器将使用这个中间语义张量c以及每一个时间步的隐层张量, 逐个生成对应的翻译语言.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>翻译数据集:</li>
</ul>
<blockquote>
<ul>
<li>下载地址: https://download.pytorch.org/tutorial/data.zip </li>
<li>数据文件预览:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>- data/
        - eng-fra.txt  
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">She</span> <span class="n">feeds</span> <span class="n">her</span> <span class="n">dog</span> <span class="n">a</span> <span class="n">meat</span><span class="o">-</span><span class="n">free</span> <span class="n">diet</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">fait</span> <span class="n">suivre</span> <span class="err">à</span> <span class="n">son</span> <span class="n">chien</span> <span class="n">un</span> <span class="n">régime</span> <span class="n">sans</span> <span class="n">viande</span><span class="p">.</span>
<span class="n">She</span> <span class="n">feeds</span> <span class="n">her</span> <span class="n">dog</span> <span class="n">a</span> <span class="n">meat</span><span class="o">-</span><span class="n">free</span> <span class="n">diet</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">fait</span> <span class="n">suivre</span> <span class="err">à</span> <span class="n">son</span> <span class="n">chien</span> <span class="n">un</span> <span class="n">régime</span> <span class="n">non</span> <span class="n">carné</span><span class="p">.</span>
<span class="n">She</span> <span class="n">folded</span> <span class="n">her</span> <span class="n">handkerchief</span> <span class="n">neatly</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">plia</span> <span class="n">soigneusement</span> <span class="n">son</span> <span class="n">mouchoir</span><span class="p">.</span>
<span class="n">She</span> <span class="n">folded</span> <span class="n">her</span> <span class="n">handkerchief</span> <span class="n">neatly</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">a</span> <span class="n">soigneusement</span> <span class="n">plié</span> <span class="n">son</span> <span class="n">mouchoir</span><span class="p">.</span>
<span class="n">She</span> <span class="n">found</span> <span class="n">a</span> <span class="n">need</span> <span class="k">and</span> <span class="n">she</span> <span class="n">filled</span> <span class="n">it</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">trouva</span> <span class="n">un</span> <span class="n">besoin</span> <span class="n">et</span> <span class="n">le</span> <span class="n">remplit</span><span class="p">.</span>
<span class="n">She</span> <span class="n">gave</span> <span class="n">birth</span> <span class="n">to</span> <span class="n">twins</span> <span class="n">a</span> <span class="n">week</span> <span class="n">ago</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">a</span> <span class="n">donné</span> <span class="n">naissance</span> <span class="err">à</span> <span class="n">des</span> <span class="n">jumeaux</span> <span class="n">il</span> <span class="n">y</span> <span class="n">a</span> <span class="n">une</span> <span class="n">semaine</span><span class="p">.</span>
<span class="n">She</span> <span class="n">gave</span> <span class="n">him</span> <span class="n">money</span> <span class="n">as</span> <span class="n">well</span> <span class="n">as</span> <span class="n">food</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">lui</span> <span class="n">donna</span> <span class="n">de</span> <span class="n">l</span><span class="p">&#39;</span><span class="n">argent</span> <span class="n">aussi</span> <span class="n">bien</span> <span class="n">que</span> <span class="n">de</span> <span class="n">la</span> <span class="n">nourriture</span><span class="p">.</span>
<span class="n">She</span> <span class="n">gave</span> <span class="n">it</span> <span class="n">her</span> <span class="n">personal</span> <span class="n">attention</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">y</span> <span class="n">a</span> <span class="n">prêté</span> <span class="n">son</span> <span class="n">attention</span> <span class="n">personnelle</span><span class="p">.</span>
<span class="n">She</span> <span class="n">gave</span> <span class="n">me</span> <span class="n">a</span> <span class="n">smile</span> <span class="n">of</span> <span class="n">recognition</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">m</span><span class="p">&#39;</span><span class="n">adressa</span> <span class="n">un</span> <span class="n">sourire</span> <span class="n">indiquant</span> <span class="n">qu</span><span class="p">&#39;</span><span class="n">elle</span> <span class="n">me</span> <span class="n">reconnaissait</span><span class="p">.</span>
<span class="n">She</span> <span class="n">glanced</span> <span class="n">shyly</span> <span class="n">at</span> <span class="n">the</span> <span class="n">young</span> <span class="n">man</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">a</span> <span class="n">timidement</span> <span class="n">jeté</span> <span class="n">un</span> <span class="n">regard</span> <span class="n">au</span> <span class="n">jeune</span> <span class="n">homme</span><span class="p">.</span>
<span class="n">She</span> <span class="n">goes</span> <span class="n">to</span> <span class="n">the</span> <span class="n">movies</span> <span class="n">once</span> <span class="n">a</span> <span class="n">week</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">va</span> <span class="n">au</span> <span class="n">cinéma</span> <span class="n">une</span> <span class="n">fois</span> <span class="n">par</span> <span class="n">semaine</span><span class="p">.</span>
<span class="n">She</span> <span class="n">got</span> <span class="n">into</span> <span class="n">the</span> <span class="n">car</span> <span class="k">and</span> <span class="n">drove</span> <span class="n">off</span><span class="p">.</span> <span class="n">Elle</span> <span class="n">s</span><span class="p">&#39;</span><span class="n">introduisit</span> <span class="n">dans</span> <span class="n">la</span> <span class="n">voiture</span> <span class="n">et</span> <span class="n">partit</span><span class="p">.</span>
</code></pre></div>

<hr />
<ul>
<li>
<p>基于GRU的seq2seq模型架构实现翻译的过程:</p>
<ul>
<li>第一步: 导入必备的工具包.</li>
<li>第二步: 对持久化文件中数据进行处理, 以满足模型训练要求.</li>
<li>第三步: 构建基于GRU的编码器和解码器.</li>
<li>第四步: 构建模型训练函数, 并进行训练.</li>
<li>第五步: 构建模型评估函数, 并进行测试以及Attention效果分析.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第一步: 导入必备的工具包</li>
</ul>
<blockquote>
<ul>
<li>python版本使用3.6.x, pytorch版本使用1.3.1</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>pip install torch==1.3.1
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># 从io工具包导入open方法</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="nb">open</span>
<span class="c1"># 用于字符规范化</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="c1"># 用于正则表达式</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="c1"># 用于随机生成数据</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="c1"># 用于构建网络结构和函数的torch工具包</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="c1"># torch中预定义的优化方法工具包</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="c1"># 设备选择, 我们可以选择在cuda或者cpu上运行你的代码</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<p>第二步: 对持久化文件中数据进行处理, 以满足模型训练要求</p>
<blockquote>
<ul>
<li>将指定语言中的词汇映射成数值:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="n">起始标志</span><span class="w"></span>
<span class="n">SOS_token</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"></span>
<span class="err">#</span><span class="w"> </span><span class="n">结束标志</span><span class="w"></span>
<span class="n">EOS_token</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>

<span class="k">class</span><span class="w"> </span><span class="nl">Lang</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">        </span><span class="ss">&quot;&quot;&quot;初始化函数中参数name代表传入某种语言的名字&quot;&quot;&quot;</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">将name传入类中</span><span class="w"></span>
<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">name</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">初始化词汇对应自然数值的字典</span><span class="w"></span>
<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">word2index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{}</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">初始化自然数值对应词汇的字典</span><span class="p">,</span><span class="w"> </span><span class="n">其中0</span><span class="err">，</span><span class="mi">1</span><span class="n">对应的SOS和EOS已经在里面了</span><span class="w"></span>
<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">index2word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{</span><span class="mi">0</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;SOS&quot;</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="ss">&quot;EOS&quot;</span><span class="err">}</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">初始化词汇对应的自然数索引</span><span class="err">，</span><span class="n">这里从2开始</span><span class="err">，</span><span class="n">因为0</span><span class="err">，</span><span class="mi">1</span><span class="n">已经被开始和结束标志占用了</span><span class="w"></span>
<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">n_words</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="w">  </span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">addSentence</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">sentence</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">        </span><span class="ss">&quot;&quot;&quot;添加句子函数, 即将句子转化为对应的数值序列, 输入参数sentence是一条句子&quot;&quot;&quot;</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">根据一般国家的语言特性</span><span class="p">(</span><span class="n">我们这里研究的语言都是以空格分个单词</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">对句子进行分割</span><span class="err">，</span><span class="n">得到对应的词汇列表</span><span class="w"></span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">sentence</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">然后调用addWord进行处理</span><span class="w"></span>
<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="w"></span>


<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">addWord</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">        </span><span class="ss">&quot;&quot;&quot;添加词汇函数, 即将词汇转化为对应的数值, 输入参数word是一个单词&quot;&quot;&quot;</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">首先判断word是否已经在self</span><span class="p">.</span><span class="n">word2index字典的key中</span><span class="w"></span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">word2index</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">如果不在</span><span class="p">,</span><span class="w"> </span><span class="n">则将这个词加入其中</span><span class="p">,</span><span class="w"> </span><span class="n">并为它对应一个数值</span><span class="err">，</span><span class="n">即self</span><span class="p">.</span><span class="n">n_words</span><span class="w"></span>
<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">word2index</span><span class="o">[</span><span class="n">word</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">n_words</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">同时也将它的反转形式加入到self</span><span class="p">.</span><span class="n">index2word中</span><span class="w"></span>
<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">index2word</span><span class="o">[</span><span class="n">self.n_words</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">word</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">n_words一旦被占用之后</span><span class="err">，</span><span class="n">逐次加1</span><span class="p">,</span><span class="w"> </span><span class="n">变成新的self</span><span class="p">.</span><span class="n">n_words</span><span class="w"></span>
<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">n_words</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>name = &quot;eng&quot;
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>sentence = &quot;hello I am Jay&quot;
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>engl = Lang(name)
engl.addSentence(sentence)
print(&quot;word2index:&quot;, engl.word2index)
print(&quot;index2word:&quot;, engl.index2word)
print(&quot;n_words:&quot;, engl.n_words)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>word2index: {&#39;hello&#39;: 2, &#39;I&#39;: 3, &#39;am&#39;: 4, &#39;Jay&#39;: 5}
index2word: {0: &#39;SOS&#39;, 1: &#39;EOS&#39;, 2: &#39;hello&#39;, 3: &#39;I&#39;, 4: &#39;am&#39;, 5: &#39;Jay&#39;}
n_words: 6
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>字符规范化:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 将<span class="nv">unicode</span>转为<span class="nv">Ascii</span>, 我们可以认为是去掉一些语言中的重音标记：Ś<span class="nv">lus</span>à<span class="nv">rski</span>
<span class="nv">def</span> <span class="nv">unicodeToAscii</span><span class="ss">(</span><span class="nv">s</span><span class="ss">)</span>:
    <span class="k">return</span> <span class="s1">&#39;&#39;</span>.<span class="nv">join</span><span class="ss">(</span>
        <span class="nv">c</span> <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">unicodedata</span>.<span class="nv">normalize</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">NFD</span><span class="s1">&#39;</span>, <span class="nv">s</span><span class="ss">)</span>
        <span class="k">if</span> <span class="nv">unicodedata</span>.<span class="nv">category</span><span class="ss">(</span><span class="nv">c</span><span class="ss">)</span> <span class="o">!=</span> <span class="s1">&#39;</span><span class="s">Mn</span><span class="s1">&#39;</span>
    <span class="ss">)</span>


<span class="nv">def</span> <span class="nv">normalizeString</span><span class="ss">(</span><span class="nv">s</span><span class="ss">)</span>:
    <span class="s2">&quot;&quot;&quot;</span><span class="s">字符串规范化函数, 参数s代表传入的字符串</span><span class="s2">&quot;&quot;&quot;</span>
    # 使字符变为小写并去除两侧空白符, <span class="nv">z</span>再使用<span class="nv">unicodeToAscii</span>去掉重音标记
    <span class="nv">s</span> <span class="o">=</span> <span class="nv">unicodeToAscii</span><span class="ss">(</span><span class="nv">s</span>.<span class="nv">lower</span><span class="ss">()</span>.<span class="nv">strip</span><span class="ss">())</span>
    # 在.<span class="o">!</span>?前加一个空格
    <span class="nv">s</span> <span class="o">=</span> <span class="nv">re</span>.<span class="nv">sub</span><span class="ss">(</span><span class="nv">r</span><span class="s2">&quot;</span><span class="s">([.!?])</span><span class="s2">&quot;</span>, <span class="nv">r</span><span class="s2">&quot;</span><span class="s"> \1</span><span class="s2">&quot;</span>, <span class="nv">s</span><span class="ss">)</span>
    # 使用正则表达式将字符串中不是大小写字母和正常标点的都替换成空格
    <span class="nv">s</span> <span class="o">=</span> <span class="nv">re</span>.<span class="nv">sub</span><span class="ss">(</span><span class="nv">r</span><span class="s2">&quot;</span><span class="s">[^a-zA-Z.!?]+</span><span class="s2">&quot;</span>, <span class="nv">r</span><span class="s2">&quot;</span><span class="s"> </span><span class="s2">&quot;</span>, <span class="nv">s</span><span class="ss">)</span>
    <span class="k">return</span> <span class="nv">s</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>s = &quot;Are you kidding me?&quot;
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>nsr = normalizeString(s)
print(nsr)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>are you kidding me ?
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>将持久化文件中的数据加载到内存, 并实例化类Lang</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;../Downloads/data/eng-fra.txt&#39;</span>

<span class="n">def</span> <span class="n">readLangs</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;读取语言函数, 参数lang1是源语言的名字, 参数lang2是目标语言的名字</span>
<span class="sd">       返回对应的class Lang对象, 以及语言对列表&quot;&quot;&quot;</span>
    <span class="c1"># 从文件中读取语言对并以/n划分存到列表lines中</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span>\
        <span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># 对lines列表中的句子进行标准化处理，并以\t进行再次划分, 形成子列表, 也就是语言对</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[[</span><span class="n">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span> 
    <span class="c1"># 然后分别将语言名字传入Lang类中, 获得对应的语言对象, 返回结果</span>
    <span class="n">input_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang1</span><span class="p">)</span>
    <span class="n">output_lang</span> <span class="o">=</span> <span class="n">Lang</span><span class="p">(</span><span class="n">lang2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_lang</span><span class="p">,</span> <span class="n">output_lang</span><span class="p">,</span> <span class="n">pairs</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>lang1 = &quot;eng&quot;
lang2 = &quot;fra&quot;
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>input_lang, output_lang, pairs = readLangs(lang1, lang2)
print(&quot;input_lang:&quot;, input_lang)
print(&quot;output_lang:&quot;, output_lang)
print(&quot;pairs中的前五个:&quot;, pairs[:5])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nl">input_lang</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">__main__</span><span class="p">.</span><span class="n">Lang</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="mh">0x11ecf0828</span><span class="o">&gt;</span><span class="w"></span>
<span class="nl">output_lang</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">__main__</span><span class="p">.</span><span class="n">Lang</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="mh">0x12d420d68</span><span class="o">&gt;</span><span class="w"></span>
<span class="nl">pairs中的前五个</span><span class="p">:</span><span class="w"> </span><span class="o">[</span><span class="n">[&#39;go .&#39;, &#39;va !&#39;</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">&#39;run !&#39;, &#39;cours !&#39;</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">&#39;run !&#39;, &#39;courez !&#39;</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">&#39;wow !&#39;, &#39;ca alors !&#39;</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">&#39;fire !&#39;, &#39;au feu !&#39;</span><span class="o">]</span><span class="err">]</span><span class="w"></span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>过滤出符合我们要求的语言对:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 设置组成句子中单词或标点的最多个数
<span class="nv">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>

# 选择带有指定前缀的语言特征数据作为训练数据
<span class="nv">eng_prefixes</span> <span class="o">=</span> <span class="ss">(</span>
    <span class="s2">&quot;</span><span class="s">i am </span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">i m </span><span class="s2">&quot;</span>,
    <span class="s2">&quot;</span><span class="s">he is</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">he s </span><span class="s2">&quot;</span>,
    <span class="s2">&quot;</span><span class="s">she is</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">she s </span><span class="s2">&quot;</span>,
    <span class="s2">&quot;</span><span class="s">you are</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">you re </span><span class="s2">&quot;</span>,
    <span class="s2">&quot;</span><span class="s">we are</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">we re </span><span class="s2">&quot;</span>,
    <span class="s2">&quot;</span><span class="s">they are</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">they re </span><span class="s2">&quot;</span>
<span class="ss">)</span>


<span class="nv">def</span> <span class="nv">filterPair</span><span class="ss">(</span><span class="nv">p</span><span class="ss">)</span>:
    <span class="s2">&quot;&quot;&quot;</span><span class="s">语言对过滤函数, 参数p代表输入的语言对, 如[&#39;she is afraid.&#39;, &#39;elle malade.&#39;]</span><span class="s2">&quot;&quot;&quot;</span>
    # <span class="nv">p</span>[<span class="mi">0</span>]代表英语句子，对它进行划分，它的长度应小于最大长度<span class="nv">MAX_LENGTH</span>并且要以指定的前缀开头
    # <span class="nv">p</span>[<span class="mi">1</span>]代表法文句子, 对它进行划分，它的长度应小于最大长度<span class="nv">MAX_LENGTH</span>
    <span class="k">return</span> <span class="nv">len</span><span class="ss">(</span><span class="nv">p</span>[<span class="mi">0</span>].<span class="nv">split</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s"> </span><span class="s1">&#39;</span><span class="ss">))</span> <span class="o">&lt;</span> <span class="nv">MAX_LENGTH</span> <span class="nv">and</span> \
        <span class="nv">p</span>[<span class="mi">0</span>].<span class="nv">startswith</span><span class="ss">(</span><span class="nv">eng_prefixes</span><span class="ss">)</span> <span class="nv">and</span> \
        <span class="nv">len</span><span class="ss">(</span><span class="nv">p</span>[<span class="mi">1</span>].<span class="nv">split</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s"> </span><span class="s1">&#39;</span><span class="ss">))</span> <span class="o">&lt;</span> <span class="nv">MAX_LENGTH</span> 


<span class="nv">def</span> <span class="nv">filterPairs</span><span class="ss">(</span><span class="nv">pairs</span><span class="ss">)</span>:
    <span class="s2">&quot;&quot;&quot;</span><span class="s">对多个语言对列表进行过滤, 参数pairs代表语言对组成的列表, 简称语言对列表</span><span class="s2">&quot;&quot;&quot;</span>
    # 函数中直接遍历列表中的每个语言对并调用<span class="nv">filterPair</span>即可
    <span class="k">return</span> [<span class="nv">pair</span> <span class="k">for</span> <span class="nv">pair</span> <span class="nv">in</span> <span class="nv">pairs</span> <span class="k">if</span> <span class="nv">filterPair</span><span class="ss">(</span><span class="nv">pair</span><span class="ss">)</span>]
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 输入参数pairs使用readLangs函数的输出结果pairs
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>fpairs = filterPairs(pairs)
print(&quot;过滤后的pairs前五个:&quot;, fpairs[:5])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>过滤后的pairs前五个: [[&#39;i m .&#39;, &#39;j ai ans .&#39;], [&#39;i m ok .&#39;, &#39;je vais bien .&#39;], [&#39;i m ok .&#39;, &#39;ca va .&#39;], [&#39;i m fat .&#39;, &#39;je suis gras .&#39;], [&#39;i m fat .&#39;, &#39;je suis gros .&#39;]]
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>对以上数据准备函数进行整合, 并使用类Lang对语言对进行数值映射:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nv">def</span> <span class="nv">prepareData</span><span class="ss">(</span><span class="nv">lang1</span>, <span class="nv">lang2</span><span class="ss">)</span>:
    <span class="s2">&quot;&quot;&quot;</span><span class="s">数据准备函数, 完成将所有字符串数据向数值型数据的映射以及过滤语言对</span>
       参数<span class="nv">lang1</span>, <span class="nv">lang2</span>分别代表源语言和目标语言的名字<span class="s2">&quot;&quot;&quot;</span>
    # 首先通过<span class="nv">readLangs</span>函数获得<span class="nv">input_lang</span>, <span class="nv">output_lang</span>对象，以及字符串类型的语言对列表
    <span class="nv">input_lang</span>, <span class="nv">output_lang</span>, <span class="nv">pairs</span> <span class="o">=</span> <span class="nv">readLangs</span><span class="ss">(</span><span class="nv">lang1</span>, <span class="nv">lang2</span><span class="ss">)</span>
    # 对字符串类型的语言对列表进行过滤操作
    <span class="nv">pairs</span> <span class="o">=</span> <span class="nv">filterPairs</span><span class="ss">(</span><span class="nv">pairs</span><span class="ss">)</span>
    # 对过滤后的语言对列表进行遍历
    <span class="k">for</span> <span class="nv">pair</span> <span class="nv">in</span> <span class="nv">pairs</span>:
        # 并使用<span class="nv">input_lang</span>和<span class="nv">output_lang</span>的<span class="nv">addSentence</span>方法对其进行数值映射
        <span class="nv">input_lang</span>.<span class="nv">addSentence</span><span class="ss">(</span><span class="nv">pair</span>[<span class="mi">0</span>]<span class="ss">)</span>
        <span class="nv">output_lang</span>.<span class="nv">addSentence</span><span class="ss">(</span><span class="nv">pair</span>[<span class="mi">1</span>]<span class="ss">)</span>
    # 返回数值映射后的对象, 和过滤后语言对
    <span class="k">return</span> <span class="nv">input_lang</span>, <span class="nv">output_lang</span>, <span class="nv">pairs</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nv">input_lang</span>, <span class="nv">output_lang</span>, <span class="nv">pairs</span> <span class="o">=</span> <span class="nv">prepareData</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">eng</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">fra</span><span class="s1">&#39;</span><span class="ss">)</span>
<span class="nv">print</span><span class="ss">(</span><span class="s2">&quot;</span><span class="s">input_n_words:</span><span class="s2">&quot;</span>, <span class="nv">input_lang</span>.<span class="nv">n_words</span><span class="ss">)</span>
<span class="nv">print</span><span class="ss">(</span><span class="s2">&quot;</span><span class="s">output_n_words:</span><span class="s2">&quot;</span>, <span class="nv">output_lang</span>.<span class="nv">n_words</span><span class="ss">)</span>
<span class="nv">print</span><span class="ss">(</span><span class="k">random</span>.<span class="nv">choice</span><span class="ss">(</span><span class="nv">pairs</span><span class="ss">))</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">input_n_words</span><span class="o">:</span> <span class="mi">2803</span>
<span class="n">output_n_words</span><span class="o">:</span> <span class="mi">4345</span>
<span class="n">pairs随机选择一条</span><span class="o">:</span> <span class="o">[</span><span class="s1">&#39;you re such an idiot !&#39;</span><span class="o">,</span> <span class="s1">&#39;quelle idiote tu es !&#39;</span><span class="o">]</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>将语言对转化为模型输入需要的张量:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">lang</span><span class="p">,</span><span class="w"> </span><span class="n">sentence</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">    </span><span class="ss">&quot;&quot;&quot;将文本句子转换为张量, 参数lang代表传入的Lang的实例化对象, sentence是预转换的句子&quot;&quot;&quot;</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">对句子进行分割并遍历每一个词汇</span><span class="p">,</span><span class="w"> </span><span class="n">然后使用lang的word2index方法找到它对应的索引</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">这样就得到了该句子对应的数值列表</span><span class="w"></span>
<span class="w">    </span><span class="n">indexes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">lang.word2index[word</span><span class="o">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">sentence</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span><span class="err">]</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">然后加入句子结束标志</span><span class="w"></span>
<span class="w">    </span><span class="n">indexes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">EOS_token</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">将其使用torch</span><span class="p">.</span><span class="n">tensor封装成张量</span><span class="p">,</span><span class="w"> </span><span class="n">并改变它的形状为nx1</span><span class="p">,</span><span class="w"> </span><span class="n">以方便后续计算</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">indexes</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">long</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">).</span><span class="k">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"></span>


<span class="n">def</span><span class="w"> </span><span class="n">tensorsFromPair</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">    </span><span class="ss">&quot;&quot;&quot;将语言对转换为张量对, 参数pair为一个语言对&quot;&quot;&quot;</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">调用tensorFromSentence分别将源语言和目标语言分别处理</span><span class="err">，</span><span class="n">获得对应的张量表示</span><span class="w"></span>
<span class="w">    </span><span class="n">input_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span><span class="w"> </span><span class="n">pair</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="n">target_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">output_lang</span><span class="p">,</span><span class="w"> </span><span class="n">pair</span><span class="o">[</span><span class="n">1</span><span class="o">]</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">最后返回它们组成的元组</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span><span class="w"> </span><span class="n">target_tensor</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 取pairs的第一条
pair = pairs[0]
</code></pre></div>

<hr />
<ul>
<li>调用:</li>
</ul>
<div class="codehilite"><pre><span></span><code>pair_tensor = tensorsFromPair(pair)
print(pair_tensor)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>(tensor([[2],
        [3],
        [4],
        [1]]), 
 tensor([[2],
        [3],
        [4],
        [5],
        [1]]))
</code></pre></div>

<hr />
<ul>
<li>第三步: 构建基于GRU的编码器和解码器</li>
</ul>
<blockquote>
<ul>
<li>
<p>构建基于GRU的编码器</p>
</li>
<li>
<p>编码器结构图:</p>
</li>
</ul>
</blockquote>
<p><center><img alt="avatar" src="img/encoder-network.png" /></center></p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="n">EncoderRNN</span>(<span class="n">nn</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">input_size</span>, <span class="n">hidden_size</span>):
        <span class="s">&quot;&quot;&quot;它的初始化参数有两个, input_size代表解码器的输入尺寸即源语言的</span>
<span class="s">            词表大小，hidden_size代表GRU的隐层节点数, 也代表词嵌入维度, 同时又是GRU的输入尺寸&quot;&quot;&quot;</span>
        <span class="n">super</span>(<span class="n">EncoderRNN</span>, <span class="nb">self</span>).<span class="n">__init__</span>()
        <span class="c1"># 将参数hidden_size传入类中</span>
        <span class="nb">self</span>.<span class="n">hidden_size</span> = <span class="n">hidden_size</span>
        <span class="c1"># 实例化nn中预定义的Embedding层, 它的参数分别是input_size, hidden_size</span>
        <span class="c1"># 这里的词嵌入维度即hidden_size</span>
        <span class="c1"># nn.Embedding的演示在该代码下方</span>
        <span class="nb">self</span>.<span class="n">embedding</span> = <span class="n">nn</span>.<span class="n">Embedding</span>(<span class="n">input_size</span>, <span class="n">hidden_size</span>)
        <span class="c1"># 然后实例化nn中预定义的GRU层, 它的参数是hidden_size</span>
        <span class="c1"># nn.GRU的演示在该代码下方</span>
        <span class="nb">self</span>.<span class="n">gru</span> = <span class="n">nn</span>.<span class="n">GRU</span>(<span class="n">hidden_size</span>, <span class="n">hidden_size</span>)

    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, <span class="n">input</span>, <span class="nb">hidden</span>):
        <span class="s">&quot;&quot;&quot;编码器前向逻辑函数中参数有两个, input代表源语言的Embedding层输入张量</span>
<span class="s">           hidden代表编码器层gru的初始隐层张量&quot;&quot;&quot;</span>
        <span class="c1"># 将输入张量进行embedding操作, 并使其形状变为(1,1,-1),-1代表自动计算维度</span>
        <span class="c1"># 理论上，我们的编码器每次只以一个词作为输入, 因此词汇映射后的尺寸应该是[1, embedding]</span>
        <span class="c1"># 而这里转换成三维的原因是因为torch中预定义gru必须使用三维张量作为输入, 因此我们拓展了一个维度</span>
        <span class="n">output</span> = <span class="nb">self</span>.<span class="n">embedding</span>(<span class="n">input</span>).<span class="n">view</span>(<span class="mi">1</span>, <span class="mi">1</span>, -<span class="mi">1</span>)
        <span class="c1"># 然后将embedding层的输出和传入的初始hidden作为gru的输入传入其中, </span>
        <span class="c1"># 获得最终gru的输出output和对应的隐层张量hidden， 并返回结果</span>
        <span class="n">output</span>, <span class="nb">hidden</span> = <span class="nb">self</span>.<span class="n">gru</span>(<span class="n">output</span>, <span class="nb">hidden</span>)
        <span class="k">return</span> <span class="n">output</span>, <span class="nb">hidden</span>

    <span class="n">def</span> <span class="n">initHidden</span>(<span class="nb">self</span>):
        <span class="s">&quot;&quot;&quot;初始化隐层张量函数&quot;&quot;&quot;</span>
        <span class="c1"># 将隐层张量初始化成为1x1xself.hidden_size大小的0张量</span>
        <span class="k">return</span> <span class="n">torch</span>.<span class="n">zeros</span>(<span class="mi">1</span>, <span class="mi">1</span>, <span class="nb">self</span>.<span class="n">hidden_size</span>, <span class="n">device</span>=<span class="n">device</span>)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>hidden_size = 25
input_size = 20
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># pair_tensor[0]代表源语言即英文的句子，pair_tensor[0][0]代表句子中
的第一个词
input = pair_tensor[0][0]
# 初始化第一个隐层张量，1x1xhidden_size的0张量
hidden = torch.zeros(1, 1, hidden_size)
</code></pre></div>

<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>encoder = EncoderRNN(input_size, hidden_size)
encoder_output, hidden = encoder(input, hidden)
print(encoder_output)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[[ 1.9149e-01, -2.0070e-01, -8.3882e-02, -3.3037e-02, -1.3491e-01,
          -8.8831e-02, -1.6626e-01, -1.9346e-01, -4.3996e-01,  1.8020e-02,
           2.8854e-02,  2.2310e-01,  3.5153e-01,  2.9635e-01,  1.5030e-01,
          -8.5266e-02, -1.4909e-01,  2.4336e-04, -2.3522e-01,  1.1359e-01,
           1.6439e-01,  1.4872e-01, -6.1619e-02, -1.0807e-02,  1.1216e-02]]],
       grad_fn=&lt;StackBackward&gt;)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>
<p>构建基于GRU的解码器</p>
</li>
<li>
<p>解码器结构图:</p>
</li>
</ul>
</blockquote>
<p><center><img alt="avatar" src="img/decoder-network.png" /></center></p>
<hr />
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="n">DecoderRNN</span>(<span class="n">nn</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">hidden_size</span>, <span class="n">output_size</span>):
        <span class="s">&quot;&quot;&quot;初始化函数有两个参数，hidden_size代表解码器中GRU的输入尺寸，也是它的隐层节点数</span>
<span class="s">           output_size代表整个解码器的输出尺寸, 也是我们希望得到的指定尺寸即目标语言的词表大小&quot;&quot;&quot;</span>
        <span class="n">super</span>(<span class="n">DecoderRNN</span>, <span class="nb">self</span>).<span class="n">__init__</span>()
        <span class="c1"># 将hidden_size传入到类中</span>
        <span class="nb">self</span>.<span class="n">hidden_size</span> = <span class="n">hidden_size</span>
        <span class="c1"># 实例化一个nn中的Embedding层对象, 它的参数output这里表示目标语言的词表大小</span>
        <span class="c1"># hidden_size表示目标语言的词嵌入维度</span>
        <span class="nb">self</span>.<span class="n">embedding</span> = <span class="n">nn</span>.<span class="n">Embedding</span>(<span class="n">output_size</span>, <span class="n">hidden_size</span>)
        <span class="c1"># 实例化GRU对象，输入参数都是hidden_size，代表它的输入尺寸和隐层节点数相同</span>
        <span class="nb">self</span>.<span class="n">gru</span> = <span class="n">nn</span>.<span class="n">GRU</span>(<span class="n">hidden_size</span>, <span class="n">hidden_size</span>)
        <span class="c1"># 实例化线性层, 对GRU的输出做线性变化, 获我们希望的输出尺寸output_size</span>
        <span class="c1"># 因此它的两个参数分别是hidden_size, output_size</span>
        <span class="nb">self</span>.<span class="n">out</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="n">hidden_size</span>, <span class="n">output_size</span>)
        <span class="c1"># 最后使用softmax进行处理，以便于分类</span>
        <span class="nb">self</span>.<span class="n">softmax</span> = <span class="n">nn</span>.<span class="n">LogSoftmax</span>(<span class="n">dim</span>=<span class="mi">1</span>)


    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, <span class="n">input</span>, <span class="nb">hidden</span>):
        <span class="s">&quot;&quot;&quot;解码器的前向逻辑函数中, 参数有两个, input代表目标语言的Embedding层输入张量</span>
<span class="s">           hidden代表解码器GRU的初始隐层张量&quot;&quot;&quot;</span>
        <span class="c1"># 将输入张量进行embedding操作, 并使其形状变为(1,1,-1),-1代表自动计算维度</span>
        <span class="c1"># 原因和解码器相同，因为torch预定义的GRU层只接受三维张量作为输入</span>
        <span class="n">output</span> = <span class="nb">self</span>.<span class="n">embedding</span>(<span class="n">input</span>).<span class="n">view</span>(<span class="mi">1</span>, <span class="mi">1</span>, -<span class="mi">1</span>)
        <span class="c1"># 然后使用relu函数对输出进行处理，根据relu函数的特性, 将使Embedding矩阵更稀疏，以防止过拟合</span>
        <span class="n">output</span> = <span class="n">F</span>.<span class="n">relu</span>(<span class="n">output</span>)
        <span class="c1"># 接下来, 将把embedding的输出以及初始化的hidden张量传入到解码器gru中</span>
        <span class="n">output</span>, <span class="nb">hidden</span> = <span class="nb">self</span>.<span class="n">gru</span>(<span class="n">output</span>, <span class="nb">hidden</span>)
        <span class="c1"># 因为GRU输出的output也是三维张量，第一维没有意义，因此可以通过output[0]来降维</span>
        <span class="c1"># 再传给线性层做变换, 最后用softmax处理以便于分类</span>
        <span class="n">output</span> = <span class="nb">self</span>.<span class="n">softmax</span>(<span class="nb">self</span>.<span class="n">out</span>(<span class="n">output</span>[<span class="mi">0</span>]))
        <span class="k">return</span> <span class="n">output</span>, <span class="nb">hidden</span>

    <span class="n">def</span> <span class="n">initHidden</span>(<span class="nb">self</span>):
        <span class="s">&quot;&quot;&quot;初始化隐层张量函数&quot;&quot;&quot;</span>
        <span class="c1"># 将隐层张量初始化成为1x1xself.hidden_size大小的0张量</span>
        <span class="k">return</span> <span class="n">torch</span>.<span class="n">zeros</span>(<span class="mi">1</span>, <span class="mi">1</span>, <span class="nb">self</span>.<span class="n">hidden_size</span>, <span class="n">device</span>=<span class="n">device</span>)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>hidden_size = 25
output_size = 10
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># pair_tensor[1]代表目标语言即法文的句子，pair_tensor[1][0]代表句子中的第一个词
input = pair_tensor[1][0]
# 初始化第一个隐层张量，1x1xhidden_size的0张量
hidden = torch.zeros(1, 1, hidden_size)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>decoder = DecoderRNN(hidden_size, output_size)
output, hidden = decoder(input, hidden)
print(output)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[-2.3554, -2.3551, -2.4361, -2.2158, -2.2550, -2.6237, -2.2917, -2.2663,
         -2.2409, -2.0783]], grad_fn=&lt;LogSoftmaxBackward&gt;)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>
<p>构建基于GRU和Attention的解码器</p>
</li>
<li>
<p>解码器结构图:</p>
</li>
</ul>
</blockquote>
<p><center><img alt="avatar" src="img/attention-decoder-network.png" /></center></p>
<hr />
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="n">AttnDecoderRNN</span>(<span class="n">nn</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">hidden_size</span>, <span class="n">output_size</span>, <span class="n">dropout_p</span>=<span class="mf">0.1</span>, <span class="n">max_length</span>=<span class="n">MAX_LENGTH</span>):
        <span class="s">&quot;&quot;&quot;初始化函数中的参数有4个, hidden_size代表解码器中GRU的输入尺寸，也是它的隐层节点数</span>
<span class="s">           output_size代表整个解码器的输出尺寸, 也是我们希望得到的指定尺寸即目标语言的词表大小</span>
<span class="s">           dropout_p代表我们使用dropout层时的置零比率，默认0.1, max_length代表句子的最大长度&quot;&quot;&quot;</span>
        <span class="n">super</span>(<span class="n">AttnDecoderRNN</span>, <span class="nb">self</span>).<span class="n">__init__</span>()
        <span class="c1"># 将以下参数传入类中</span>
        <span class="nb">self</span>.<span class="n">hidden_size</span> = <span class="n">hidden_size</span>
        <span class="nb">self</span>.<span class="n">output_size</span> = <span class="n">output_size</span>
        <span class="nb">self</span>.<span class="n">dropout_p</span> = <span class="n">dropout_p</span>
        <span class="nb">self</span>.<span class="n">max_length</span> = <span class="n">max_length</span>

        <span class="c1"># 实例化一个Embedding层, 输入参数是self.output_size和self.hidden_size</span>
        <span class="nb">self</span>.<span class="n">embedding</span> = <span class="n">nn</span>.<span class="n">Embedding</span>(<span class="nb">self</span>.<span class="n">output_size</span>, <span class="nb">self</span>.<span class="n">hidden_size</span>)
        <span class="c1"># 根据attention的QKV理论，attention的输入参数为三个Q，K，V，</span>
        <span class="c1"># 第一步，使用Q与K进行attention权值计算得到权重矩阵, 再与V做矩阵乘法, 得到V的注意力表示结果.</span>
        <span class="c1"># 这里常见的计算方式有三种:</span>
        <span class="c1"># 1，将Q，K进行纵轴拼接, 做一次线性变化, 再使用softmax处理获得结果最后与V做张量乘法</span>
        <span class="c1"># 2，将Q，K进行纵轴拼接, 做一次线性变化后再使用tanh函数激活, 然后再进行内部求和, 最后使用softmax处理获得结果再与V做张量乘法</span>
        <span class="c1"># 3，将Q与K的转置做点积运算, 然后除以一个缩放系数, 再使用softmax处理获得结果最后与V做张量乘法</span>

        <span class="c1"># 说明：当注意力权重矩阵和V都是三维张量且第一维代表为batch条数时, 则做bmm运算.</span>

        <span class="c1"># 第二步, 根据第一步采用的计算方法, 如果是拼接方法，则需要将Q与第二步的计算结果再进行拼接, </span>
        <span class="c1"># 如果是转置点积, 一般是自注意力, Q与V相同, 则不需要进行与Q的拼接.因此第二步的计算方式与第一步采用的全值计算方法有关.</span>
        <span class="c1"># 第三步，最后为了使整个attention结构按照指定尺寸输出, 使用线性层作用在第二步的结果上做一个线性变换. 得到最终对Q的注意力表示.</span>

        <span class="c1"># 我们这里使用的是第一步中的第一种计算方式, 因此需要一个线性变换的矩阵, 实例化nn.Linear</span>
        <span class="c1"># 因为它的输入是Q，K的拼接, 所以输入的第一个参数是self.hidden_size * 2，第二个参数是self.max_length</span>
        <span class="c1"># 这里的Q是解码器的Embedding层的输出, K是解码器GRU的隐层输出，因为首次隐层还没有任何输出，会使用编码器的隐层输出</span>
        <span class="c1"># 而这里的V是编码器层的输出</span>
        <span class="nb">self</span>.<span class="n">attn</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="nb">self</span>.<span class="n">hidden_size</span> * <span class="mi">2</span>, <span class="nb">self</span>.<span class="n">max_length</span>)
        <span class="c1"># 接着我们实例化另外一个线性层, 它是attention理论中的第四步的线性层，用于规范输出尺寸</span>
        <span class="c1"># 这里它的输入来自第三步的结果, 因为第三步的结果是将Q与第二步的结果进行拼接, 因此输入维度是self.hidden_size * 2</span>
        <span class="nb">self</span>.<span class="n">attn_combine</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="nb">self</span>.<span class="n">hidden_size</span> * <span class="mi">2</span>, <span class="nb">self</span>.<span class="n">hidden_size</span>)
        <span class="c1"># 接着实例化一个nn.Dropout层，并传入self.dropout_p</span>
        <span class="nb">self</span>.<span class="n">dropout</span> = <span class="n">nn</span>.<span class="n">Dropout</span>(<span class="nb">self</span>.<span class="n">dropout_p</span>)
        <span class="c1"># 之后实例化nn.GRU, 它的输入和隐层尺寸都是self.hidden_size</span>
        <span class="nb">self</span>.<span class="n">gru</span> = <span class="n">nn</span>.<span class="n">GRU</span>(<span class="nb">self</span>.<span class="n">hidden_size</span>, <span class="nb">self</span>.<span class="n">hidden_size</span>)
        <span class="c1"># 最后实例化gru后面的线性层，也就是我们的解码器输出层.</span>
        <span class="nb">self</span>.<span class="n">out</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="nb">self</span>.<span class="n">hidden_size</span>, <span class="nb">self</span>.<span class="n">output_size</span>)


    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, <span class="n">input</span>, <span class="nb">hidden</span>, <span class="n">encoder_outputs</span>):
        <span class="s">&quot;&quot;&quot;forward函数的输入参数有三个, 分别是源数据输入张量, 初始的隐层张量, 以及解码器的输出张量&quot;&quot;&quot;</span>

        <span class="c1"># 根据结构计算图, 输入张量进行Embedding层并扩展维度</span>
        <span class="n">embedded</span> = <span class="nb">self</span>.<span class="n">embedding</span>(<span class="n">input</span>).<span class="n">view</span>(<span class="mi">1</span>, <span class="mi">1</span>, -<span class="mi">1</span>)
        <span class="c1"># 使用dropout进行随机丢弃，防止过拟合</span>
        <span class="n">embedded</span> = <span class="nb">self</span>.<span class="n">dropout</span>(<span class="n">embedded</span>)

        <span class="c1"># 进行attention的权重计算, 哦我们呢使用第一种计算方式：</span>
        <span class="c1"># 将Q，K进行纵轴拼接, 做一次线性变化, 最后使用softmax处理获得结果</span>
        <span class="n">attn_weights</span> = <span class="n">F</span>.<span class="n">softmax</span>(
            <span class="nb">self</span>.<span class="n">attn</span>(<span class="n">torch</span>.<span class="n">cat</span>((<span class="n">embedded</span>[<span class="mi">0</span>], <span class="nb">hidden</span>[<span class="mi">0</span>]), <span class="mi">1</span>)), <span class="n">dim</span>=<span class="mi">1</span>)

        <span class="c1"># 然后进行第一步的后半部分, 将得到的权重矩阵与V做矩阵乘法计算, 当二者都是三维张量且第一维代表为batch条数时, 则做bmm运算</span>
        <span class="n">attn_applied</span> = <span class="n">torch</span>.<span class="n">bmm</span>(<span class="n">attn_weights</span>.<span class="n">unsqueeze</span>(<span class="mi">0</span>),
                                 <span class="n">encoder_outputs</span>.<span class="n">unsqueeze</span>(<span class="mi">0</span>))

        <span class="c1"># 之后进行第二步, 通过取[0]是用来降维, 根据第一步采用的计算方法, 需要将Q与第一步的计算结果再进行拼接</span>
        <span class="n">output</span> = <span class="n">torch</span>.<span class="n">cat</span>((<span class="n">embedded</span>[<span class="mi">0</span>], <span class="n">attn_applied</span>[<span class="mi">0</span>]), <span class="mi">1</span>)

        <span class="c1"># 最后是第三步, 使用线性层作用在第三步的结果上做一个线性变换并扩展维度，得到输出</span>
        <span class="n">output</span> = <span class="nb">self</span>.<span class="n">attn_combine</span>(<span class="n">output</span>).<span class="n">unsqueeze</span>(<span class="mi">0</span>)

        <span class="c1"># attention结构的结果使用relu激活</span>
        <span class="n">output</span> = <span class="n">F</span>.<span class="n">relu</span>(<span class="n">output</span>)

        <span class="c1"># 将激活后的结果作为gru的输入和hidden一起传入其中</span>
        <span class="n">output</span>, <span class="nb">hidden</span> = <span class="nb">self</span>.<span class="n">gru</span>(<span class="n">output</span>, <span class="nb">hidden</span>)

        <span class="c1"># 最后将结果降维并使用softmax处理得到最终的结果</span>
        <span class="n">output</span> = <span class="n">F</span>.<span class="n">log_softmax</span>(<span class="nb">self</span>.<span class="n">out</span>(<span class="n">output</span>[<span class="mi">0</span>]), <span class="n">dim</span>=<span class="mi">1</span>)
        <span class="c1"># 返回解码器结果，最后的隐层张量以及注意力权重张量</span>
        <span class="k">return</span> <span class="n">output</span>, <span class="nb">hidden</span>, <span class="n">attn_weights</span>

    <span class="n">def</span> <span class="n">initHidden</span>(<span class="nb">self</span>):
        <span class="s">&quot;&quot;&quot;初始化隐层张量函数&quot;&quot;&quot;</span>
        <span class="c1"># 将隐层张量初始化成为1x1xself.hidden_size大小的0张量</span>
        <span class="k">return</span> <span class="n">torch</span>.<span class="n">zeros</span>(<span class="mi">1</span>, <span class="mi">1</span>, <span class="nb">self</span>.<span class="n">hidden_size</span>, <span class="n">device</span>=<span class="n">device</span>)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>实例化参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>hidden_size = 25
output_size = 10
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>input = pair_tensor[1][0]
hidden = torch.zeros(1, 1, hidden_size)
# encoder_outputs需要是encoder中每一个时间步的输出堆叠而成
# 它的形状应该是10x25, 我们这里直接随机初始化一个张量
encoder_outputs  = torch.randn(10, 25)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>decoder = AttnDecoderRNN(hidden_size, output_size)
output, hidden, attn_weights= decoder(input, hidden, encoder_outputs)
print(output)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[-2.3556, -2.1418, -2.2012, -2.5109, -2.4025, -2.2182, -2.2123, -2.4608,
         -2.2124, -2.3827]], grad_fn=&lt;LogSoftmaxBackward&gt;)
</code></pre></div>

<hr />
<ul>
<li>第四步: 构建模型训练函数, 并进行训练</li>
</ul>
<blockquote>
<ul>
<li>什么是teacher_forcing?<ul>
<li>它是一种用于序列生成任务的训练技巧, 在seq2seq架构中, 根据循环神经网络理论，解码器每次应该使用上一步的结果作为输入的一部分, 但是训练过程中，一旦上一步的结果是错误的，就会导致这种错误被累积，无法达到训练效果, 因此，我们需要一种机制改变上一步出错的情况，因为训练时我们是已知正确的输出应该是什么，因此可以强制将上一步结果设置成正确的输出, 这种方式就叫做teacher_forcing.</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<blockquote>
<ul>
<li>teacher_forcing的作用:<ul>
<li>能够在训练的时候矫正模型的预测，避免在序列生成的过程中误差进一步放大.</li>
<li>teacher_forcing能够极大的加快模型的收敛速度，令模型训练过程更快更平稳.</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<blockquote>
<ul>
<li>构建训练函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="n">设置teacher_forcing比率为0</span><span class="mf">.5</span><span class="w"></span>
<span class="n">teacher_forcing_ratio</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5</span><span class="w"></span>


<span class="n">def</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span><span class="w"> </span><span class="n">target_tensor</span><span class="p">,</span><span class="w"> </span><span class="n">encoder</span><span class="p">,</span><span class="w"> </span><span class="n">decoder</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_optimizer</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_optimizer</span><span class="p">,</span><span class="w"> </span><span class="n">criterion</span><span class="p">,</span><span class="w"> </span><span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">    </span><span class="ss">&quot;&quot;&quot;训练函数, 输入参数有8个, 分别代表input_tensor：源语言输入张量，target_tensor：目标语言输入张量，encoder, decoder：编码器和解码器实例化对象</span>
<span class="ss">       encoder_optimizer, decoder_optimizer：编码器和解码器优化方法，criterion：损失函数计算方法，max_length：句子的最大长度&quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">初始化隐层张量</span><span class="w"></span>
<span class="w">    </span><span class="n">encoder_hidden</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">encoder</span><span class="p">.</span><span class="n">initHidden</span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">编码器和解码器优化器梯度归0</span><span class="w"></span>
<span class="w">    </span><span class="n">encoder_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span><span class="w"></span>
<span class="w">    </span><span class="n">decoder_optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">根据源文本和目标文本张量获得对应的长度</span><span class="w"></span>
<span class="w">    </span><span class="n">input_length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_tensor</span><span class="p">.</span><span class="k">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="n">target_length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">target_tensor</span><span class="p">.</span><span class="k">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">初始化编码器输出张量</span><span class="err">，</span><span class="n">形状是max_lengthxencoder</span><span class="p">.</span><span class="n">hidden_size的0张量</span><span class="w"></span>
<span class="w">    </span><span class="n">encoder_outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span><span class="w"> </span><span class="n">encoder</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">初始设置损失为0</span><span class="w"></span>
<span class="w">    </span><span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">循环遍历输入张量索引</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">ei</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">input_length</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">根据索引从input_tensor取出对应的单词的张量表示</span><span class="err">，</span><span class="n">和初始化隐层张量一同传入encoder对象中</span><span class="w"></span>
<span class="w">        </span><span class="n">encoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_hidden</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">encoder</span><span class="p">(</span><span class="w"></span>
<span class="w">            </span><span class="n">input_tensor</span><span class="o">[</span><span class="n">ei</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_hidden</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">将每次获得的输出encoder_output</span><span class="p">(</span><span class="n">三维张量</span><span class="p">),</span><span class="w"> </span><span class="n">使用</span><span class="o">[</span><span class="n">0, 0</span><span class="o">]</span><span class="n">降两维变成向量依次存入到encoder_outputs</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">这样encoder_outputs每一行存的都是对应的句子中每个单词通过编码器的输出结果</span><span class="w"></span>
<span class="w">        </span><span class="n">encoder_outputs</span><span class="o">[</span><span class="n">ei</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">encoder_output</span><span class="o">[</span><span class="n">0, 0</span><span class="o">]</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">初始化解码器的第一个输入</span><span class="err">，</span><span class="n">即起始符</span><span class="w"></span>
<span class="w">    </span><span class="n">decoder_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">[</span><span class="n">[SOS_token</span><span class="o">]</span><span class="err">]</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">初始化解码器的隐层张量即编码器的隐层输出</span><span class="w"></span>
<span class="w">    </span><span class="n">decoder_hidden</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">encoder_hidden</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">根据随机数与teacher_forcing_ratio对比判断是否使用teacher_forcing</span><span class="w"></span>
<span class="w">    </span><span class="n">use_teacher_forcing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">True</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">teacher_forcing_ratio</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">False</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">如果使用teacher_forcing</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nl">use_teacher_forcing</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">循环遍历目标张量索引</span><span class="w"></span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">di</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">target_length</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">将decoder_input</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_outputs即attention中的QKV</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">传入解码器对象</span><span class="p">,</span><span class="w"> </span><span class="n">获得decoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_attention</span><span class="w"></span>
<span class="w">            </span><span class="n">decoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_attention</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">decoder</span><span class="p">(</span><span class="w"></span>
<span class="w">                </span><span class="n">decoder_input</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_outputs</span><span class="p">)</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">因为使用了teacher_forcing</span><span class="p">,</span><span class="w"> </span><span class="n">无论解码器输出的decoder_output是什么</span><span class="p">,</span><span class="w"> </span><span class="n">我们都只</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">使用</span><span class="err">‘</span><span class="n">正确的答案</span><span class="err">’，</span><span class="n">即target_tensor</span><span class="o">[</span><span class="n">di</span><span class="o">]</span><span class="n">来计算损失</span><span class="w"></span>
<span class="w">            </span><span class="n">loss</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">criterion</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">target_tensor</span><span class="o">[</span><span class="n">di</span><span class="o">]</span><span class="p">)</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">并强制将下一次的解码器输入设置为</span><span class="err">‘</span><span class="n">正确的答案</span><span class="err">’</span><span class="w"></span>
<span class="w">            </span><span class="n">decoder_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">target_tensor</span><span class="o">[</span><span class="n">di</span><span class="o">]</span><span class="w">  </span>

<span class="w">    </span><span class="k">else</span><span class="err">:</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">如果不使用teacher_forcing</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">仍然遍历目标张量索引</span><span class="w"></span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">di</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">target_length</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">将decoder_input</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_outputs传入解码器对象</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">获得decoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_attention</span><span class="w"></span>
<span class="w">            </span><span class="n">decoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_attention</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">decoder</span><span class="p">(</span><span class="w"></span>
<span class="w">                </span><span class="n">decoder_input</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_outputs</span><span class="p">)</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">只不过这里我们将从decoder_output取出答案</span><span class="w"></span>
<span class="w">            </span><span class="n">topv</span><span class="p">,</span><span class="w"> </span><span class="n">topi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">decoder_output</span><span class="p">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">损失计算仍然使用decoder_output和target_tensor</span><span class="o">[</span><span class="n">di</span><span class="o">]</span><span class="w"></span>
<span class="w">            </span><span class="n">loss</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">criterion</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">target_tensor</span><span class="o">[</span><span class="n">di</span><span class="o">]</span><span class="p">)</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">最后如果输出值是终止符</span><span class="err">，</span><span class="n">则循环停止</span><span class="w"></span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">topi</span><span class="p">.</span><span class="n">squeeze</span><span class="p">().</span><span class="n">item</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nl">EOS_token</span><span class="p">:</span><span class="w"></span>
<span class="w">                </span><span class="k">break</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">否则</span><span class="err">，</span><span class="n">并对topi降维并分离赋值给decoder_input以便进行下次运算</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">这里的detach的分离作用使得这个decoder_input与模型构建的张量图无关</span><span class="err">，</span><span class="n">相当于全新的外界输入</span><span class="w"></span>
<span class="w">            </span><span class="n">decoder_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">topi</span><span class="p">.</span><span class="n">squeeze</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span><span class="w"></span>


<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">误差进行反向传播</span><span class="w"></span>
<span class="w">    </span><span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span><span class="w"></span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">编码器和解码器进行优化即参数更新</span><span class="w"></span>
<span class="w">    </span><span class="n">encoder_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span><span class="w"></span>
<span class="w">    </span><span class="n">decoder_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">最后返回平均损失</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">target_length</span><span class="w"></span>
</code></pre></div>

<blockquote>
<ul>
<li>构建时间计算函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 导入时间和数学工具包</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">timeSince</span><span class="p">(</span><span class="n">since</span><span class="p">):</span>
    <span class="s2">&quot;获得每次打印的训练耗时, since是训练开始时间&quot;</span>
    <span class="c1"># 获得当前时间</span>
    <span class="n">now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># 获得时间差，就是训练耗时</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">now</span> <span class="o">-</span> <span class="n">since</span>
    <span class="c1"># 将秒转化为分钟, 并取整</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">s</span> <span class="o">/</span> <span class="mi">60</span><span class="p">)</span>
    <span class="c1"># 计算剩下不够凑成1分钟的秒数</span>
    <span class="n">s</span> <span class="o">-=</span> <span class="n">m</span> <span class="o">*</span> <span class="mi">60</span>
    <span class="c1"># 返回指定格式的耗时</span>
    <span class="k">return</span> <span class="s1">&#39;</span><span class="si">%d</span><span class="s1">m </span><span class="si">%d</span><span class="s1">s&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 假定模型训练开始时间是10min之前
since = time.time() - 10*60
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>period = timeSince(since)
print(period)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="mf">10</span><span class="n">m</span> <span class="mf">0</span><span class="n">s</span> 
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用训练函数并打印日志和制图:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 导入plt以便绘制损失曲线</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">trainIters</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">n_iters</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">plot_every</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;训练迭代函数, 输入参数有6个，分别是encoder, decoder: 编码器和解码器对象，</span>
<span class="sd">       n_iters: 总迭代步数, print_every:打印日志间隔, plot_every:绘制损失曲线间隔, learning_rate学习率&quot;&quot;&quot;</span>
    <span class="c1"># 获得训练开始时间戳</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># 每个损失间隔的平均损失保存列表，用于绘制损失曲线</span>
    <span class="n">plot_losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># 每个打印日志间隔的总损失，初始为0</span>
    <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>  
    <span class="c1"># 每个绘制损失间隔的总损失，初始为0</span>
    <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>  

    <span class="c1"># 使用预定义的SGD作为优化器，将参数和学习率传入其中</span>
    <span class="n">encoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">decoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="c1"># 选择损失函数</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

    <span class="c1"># 根据设置迭代步进行循环</span>
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># 每次从语言对列表中随机取出一条作为训练语句</span>
        <span class="n">training_pair</span> <span class="o">=</span> <span class="n">tensorsFromPair</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">))</span>
        <span class="c1"># 分别从training_pair中取出输入张量和目标张量</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">training_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">training_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># 通过train函数获得模型运行的损失</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span>
                     <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
        <span class="c1"># 将损失进行累和</span>
        <span class="n">print_loss_total</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">plot_loss_total</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="c1"># 当迭代步达到日志打印间隔时</span>
        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># 通过总损失除以间隔得到平均损失</span>
            <span class="n">print_loss_avg</span> <span class="o">=</span> <span class="n">print_loss_total</span> <span class="o">/</span> <span class="n">print_every</span>
            <span class="c1"># 将总损失归0</span>
            <span class="n">print_loss_total</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># 打印日志，日志内容分别是：训练耗时，当前迭代步，当前进度百分比，当前平均损失</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> (</span><span class="si">%d</span><span class="s1"> </span><span class="si">%d%%</span><span class="s1">) </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">timeSince</span><span class="p">(</span><span class="n">start</span><span class="p">),</span>
                                         <span class="nb">iter</span><span class="p">,</span> <span class="nb">iter</span> <span class="o">/</span> <span class="n">n_iters</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">print_loss_avg</span><span class="p">))</span>

        <span class="c1"># 当迭代步达到损失绘制间隔时</span>
        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="n">plot_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># 通过总损失除以间隔得到平均损失</span>
            <span class="n">plot_loss_avg</span> <span class="o">=</span> <span class="n">plot_loss_total</span> <span class="o">/</span> <span class="n">plot_every</span>
            <span class="c1"># 将平均损失装进plot_losses列表</span>
            <span class="n">plot_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">plot_loss_avg</span><span class="p">)</span>
            <span class="c1"># 总损失归0</span>
            <span class="n">plot_loss_total</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># 绘制损失曲线</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>  
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">plot_losses</span><span class="p">)</span>
    <span class="c1"># 保存到指定路径</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;./s2s_loss.png&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输入参数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 设置隐层大小为256 ，也是词嵌入维度      
hidden_size = 256
# 通过input_lang.n_words获取输入词汇总数，与hidden_size一同传入EncoderRNN类中
# 得到编码器对象encoder1
encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)

# 通过output_lang.n_words获取目标词汇总数，与hidden_size和dropout_p一同传入AttnDecoderRNN类中
# 得到解码器对象attn_decoder1
attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)

# 设置迭代步数 
n_iters = 75000
# 设置日志打印间隔
print_every = 5000 
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 调用trainIters进行模型训练，将编码器对象encoder1，码器对象attn_decoder1，迭代步数，日志打印间隔传入其中
trainIters(encoder1, attn_decoder1, n_iters, print_every=print_every)
</code></pre></div>

<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="mf">3</span><span class="n">m</span> <span class="mf">35</span><span class="n">s</span> <span class="p">(</span><span class="mf">5000</span> <span class="mf">6</span><span class="err">%</span><span class="p">)</span> <span class="mf">3.4159</span>
<span class="mf">7</span><span class="n">m</span> <span class="mf">12</span><span class="n">s</span> <span class="p">(</span><span class="mf">10000</span> <span class="mf">13</span><span class="err">%</span><span class="p">)</span> <span class="mf">2.7805</span>
<span class="mf">10</span><span class="n">m</span> <span class="mf">46</span><span class="n">s</span> <span class="p">(</span><span class="mf">15000</span> <span class="mf">20</span><span class="err">%</span><span class="p">)</span> <span class="mf">2.4663</span>
<span class="mf">14</span><span class="n">m</span> <span class="mf">23</span><span class="n">s</span> <span class="p">(</span><span class="mf">20000</span> <span class="mf">26</span><span class="err">%</span><span class="p">)</span> <span class="mf">2.1693</span>
<span class="mf">18</span><span class="n">m</span> <span class="mf">6</span><span class="n">s</span> <span class="p">(</span><span class="mf">25000</span> <span class="mf">33</span><span class="err">%</span><span class="p">)</span> <span class="mf">1.9303</span>
<span class="mf">21</span><span class="n">m</span> <span class="mf">44</span><span class="n">s</span> <span class="p">(</span><span class="mf">30000</span> <span class="mf">40</span><span class="err">%</span><span class="p">)</span> <span class="mf">1.7601</span>
<span class="mf">25</span><span class="n">m</span> <span class="mf">23</span><span class="n">s</span> <span class="p">(</span><span class="mf">35000</span> <span class="mf">46</span><span class="err">%</span><span class="p">)</span> <span class="mf">1.6207</span>
<span class="mf">29</span><span class="n">m</span> <span class="mf">8</span><span class="n">s</span> <span class="p">(</span><span class="mf">40000</span> <span class="mf">53</span><span class="err">%</span><span class="p">)</span> <span class="mf">1.4973</span>
<span class="mf">32</span><span class="n">m</span> <span class="mf">44</span><span class="n">s</span> <span class="p">(</span><span class="mf">45000</span> <span class="mf">60</span><span class="err">%</span><span class="p">)</span> <span class="mf">1.3832</span>
<span class="mf">36</span><span class="n">m</span> <span class="mf">22</span><span class="n">s</span> <span class="p">(</span><span class="mf">50000</span> <span class="mf">66</span><span class="err">%</span><span class="p">)</span> <span class="mf">1.2694</span>
<span class="mf">40</span><span class="n">m</span> <span class="mf">6</span><span class="n">s</span> <span class="p">(</span><span class="mf">55000</span> <span class="mf">73</span><span class="err">%</span><span class="p">)</span> <span class="mf">1.1813</span>
<span class="mf">43</span><span class="n">m</span> <span class="mf">51</span><span class="n">s</span> <span class="p">(</span><span class="mf">60000</span> <span class="mf">80</span><span class="err">%</span><span class="p">)</span> <span class="mf">1.0907</span>
<span class="mf">47</span><span class="n">m</span> <span class="mf">29</span><span class="n">s</span> <span class="p">(</span><span class="mf">65000</span> <span class="mf">86</span><span class="err">%</span><span class="p">)</span> <span class="mf">1.0425</span>
<span class="mf">51</span><span class="n">m</span> <span class="mf">10</span><span class="n">s</span> <span class="p">(</span><span class="mf">70000</span> <span class="mf">93</span><span class="err">%</span><span class="p">)</span> <span class="mf">0.9955</span>
<span class="mf">54</span><span class="n">m</span> <span class="mf">48</span><span class="n">s</span> <span class="p">(</span><span class="mf">75000</span> <span class="mf">100</span><span class="err">%</span><span class="p">)</span> <span class="mf">0.9158</span>
</code></pre></div>

<blockquote>
<ul>
<li>损失下降曲线:</li>
</ul>
</blockquote>
<p><center><img alt="avatar" src="img/s2s_loss.png" /></center></p>
<hr />
<blockquote>
<ul>
<li>损失曲线分析:<ul>
<li>一直下降的损失曲线, 说明模型正在收敛, 能够从数据中找到一些规律应用于数据.</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<p>第五步: 构建模型评估函数, 并进行测试以及Attention效果分析.</p>
<blockquote>
<ul>
<li>构建模型评估函数:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span><span class="w"> </span><span class="n">decoder</span><span class="p">,</span><span class="w"> </span><span class="n">sentence</span><span class="p">,</span><span class="w"> </span><span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">    </span><span class="ss">&quot;&quot;&quot;评估函数，输入参数有4个，分别是encoder, decoder: 编码器和解码器对象，</span>
<span class="ss">       sentence:需要评估的句子，max_length:句子的最大长度&quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">评估阶段不进行梯度计算</span><span class="w"></span>
<span class="w">    </span><span class="k">with</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span><span class="err">:</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">对输入的句子进行张量表示</span><span class="w"></span>
<span class="w">        </span><span class="n">input_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensorFromSentence</span><span class="p">(</span><span class="n">input_lang</span><span class="p">,</span><span class="w"> </span><span class="n">sentence</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">获得输入的句子长度</span><span class="w"></span>
<span class="w">        </span><span class="n">input_length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_tensor</span><span class="p">.</span><span class="k">size</span><span class="p">()</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">初始化编码器隐层张量</span><span class="w"></span>
<span class="w">        </span><span class="n">encoder_hidden</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">encoder</span><span class="p">.</span><span class="n">initHidden</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">初始化编码器输出张量</span><span class="err">，</span><span class="n">是max_lengthxencoder</span><span class="p">.</span><span class="n">hidden_size的0张量</span><span class="w"></span>
<span class="w">        </span><span class="n">encoder_outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span><span class="w"> </span><span class="n">encoder</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">循环遍历输入张量索引</span><span class="w"></span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">ei</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">input_length</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">             </span><span class="err">#</span><span class="w"> </span><span class="n">根据索引从input_tensor取出对应的单词的张量表示</span><span class="err">，</span><span class="n">和初始化隐层张量一同传入encoder对象中</span><span class="w"></span>
<span class="w">            </span><span class="n">encoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_hidden</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">encoder</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">[</span><span class="n">ei</span><span class="o">]</span><span class="p">,</span><span class="w"></span>
<span class="w">                                                     </span><span class="n">encoder_hidden</span><span class="p">)</span><span class="w"></span>
<span class="w">            </span><span class="n">#将每次获得的输出encoder_output</span><span class="p">(</span><span class="n">三维张量</span><span class="p">),</span><span class="w"> </span><span class="n">使用</span><span class="o">[</span><span class="n">0, 0</span><span class="o">]</span><span class="n">降两维变成向量依次存入到encoder_outputs</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">这样encoder_outputs每一行存的都是对应的句子中每个单词通过编码器的输出结果</span><span class="w"></span>
<span class="w">            </span><span class="n">encoder_outputs</span><span class="o">[</span><span class="n">ei</span><span class="o">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">encoder_output</span><span class="o">[</span><span class="n">0, 0</span><span class="o">]</span><span class="w"></span>

<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">初始化解码器的第一个输入</span><span class="err">，</span><span class="n">即起始符</span><span class="w"></span>
<span class="w">        </span><span class="n">decoder_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">[</span><span class="n">[SOS_token</span><span class="o">]</span><span class="err">]</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="w"> </span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">初始化解码器的隐层张量即编码器的隐层输出</span><span class="w"></span>
<span class="w">        </span><span class="n">decoder_hidden</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">encoder_hidden</span><span class="w"></span>

<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">初始化预测的词汇列表</span><span class="w"></span>
<span class="w">        </span><span class="n">decoded_words</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">初始化attention张量</span><span class="w"></span>
<span class="w">        </span><span class="n">decoder_attentions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span><span class="w"> </span><span class="n">max_length</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">开始循环解码</span><span class="w"></span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">di</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span><span class="err">:</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">将decoder_input</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_outputs传入解码器对象</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">获得decoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_attention</span><span class="w"></span>
<span class="w">            </span><span class="n">decoder_output</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_attention</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">decoder</span><span class="p">(</span><span class="w"></span>
<span class="w">                </span><span class="n">decoder_input</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">encoder_outputs</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">取所有的attention结果存入初始化的attention张量中</span><span class="w"></span>
<span class="w">            </span><span class="n">decoder_attentions</span><span class="o">[</span><span class="n">di</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">decoder_attention</span><span class="p">.</span><span class="k">data</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">从解码器输出中获得概率最高的值及其索引对象</span><span class="w"></span>
<span class="w">            </span><span class="n">topv</span><span class="p">,</span><span class="w"> </span><span class="n">topi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">decoder_output</span><span class="p">.</span><span class="k">data</span><span class="p">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"></span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">从索引对象中取出它的值与结束标志值作对比</span><span class="w"></span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">topi</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nl">EOS_token</span><span class="p">:</span><span class="w"></span>
<span class="w">                </span><span class="err">#</span><span class="w"> </span><span class="n">如果是结束标志值</span><span class="err">，</span><span class="n">则将结束标志装进decoded_words列表</span><span class="err">，</span><span class="n">代表翻译结束</span><span class="w"></span>
<span class="w">                </span><span class="n">decoded_words</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;&lt;EOS&gt;&#39;</span><span class="p">)</span><span class="w"></span>
<span class="w">                </span><span class="err">#</span><span class="w"> </span><span class="n">循环退出</span><span class="w"></span>
<span class="w">                </span><span class="k">break</span><span class="w"></span>

<span class="w">            </span><span class="k">else</span><span class="err">:</span><span class="w"></span>
<span class="w">                </span><span class="err">#</span><span class="w"> </span><span class="n">否则</span><span class="err">，</span><span class="n">根据索引找到它在输出语言的index2word字典中对应的单词装进decoded_words</span><span class="w"></span>
<span class="w">                </span><span class="n">decoded_words</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_lang</span><span class="p">.</span><span class="n">index2word</span><span class="o">[</span><span class="n">topi.item()</span><span class="o">]</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">最后将本次预测的索引降维并分离赋值给decoder_input</span><span class="err">，</span><span class="n">以便下次进行预测</span><span class="w"></span>
<span class="w">            </span><span class="n">decoder_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">topi</span><span class="p">.</span><span class="n">squeeze</span><span class="p">().</span><span class="n">detach</span><span class="p">()</span><span class="w"></span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">返回结果decoded_words</span><span class="err">，</span><span class="w"> </span><span class="n">以及完整注意力张量</span><span class="p">,</span><span class="w"> </span><span class="n">把没有用到的部分切掉</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">decoded_words</span><span class="p">,</span><span class="w"> </span><span class="n">decoder_attentions</span><span class="o">[</span><span class="n">:di + 1</span><span class="o">]</span><span class="w"></span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>随机选择指定数量的数据进行评估:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nv">def</span> <span class="nv">evaluateRandomly</span><span class="ss">(</span><span class="nv">encoder</span>, <span class="nv">decoder</span>, <span class="nv">n</span><span class="o">=</span><span class="mi">6</span><span class="ss">)</span>:
    <span class="s2">&quot;&quot;&quot;</span><span class="s">随机测试函数, 输入参数encoder, decoder代表编码器和解码器对象，n代表测试数</span><span class="s2">&quot;&quot;&quot;</span>
    # 对测试数进行循环
    <span class="k">for</span> <span class="nv">i</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="nv">n</span><span class="ss">)</span>:
        # 从<span class="nv">pairs</span>随机选择语言对
        <span class="nv">pair</span> <span class="o">=</span> <span class="k">random</span>.<span class="nv">choice</span><span class="ss">(</span><span class="nv">pairs</span><span class="ss">)</span>
        # <span class="o">&gt;</span> 代表输入
        <span class="nv">print</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">&gt;</span><span class="s1">&#39;</span>, <span class="nv">pair</span>[<span class="mi">0</span>]<span class="ss">)</span>
        # <span class="o">=</span> 代表正确的输出
        <span class="nv">print</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">=</span><span class="s1">&#39;</span>, <span class="nv">pair</span>[<span class="mi">1</span>]<span class="ss">)</span>
        # 调用<span class="nv">evaluate</span>进行预测
        <span class="nv">output_words</span>, <span class="nv">attentions</span> <span class="o">=</span> <span class="nv">evaluate</span><span class="ss">(</span><span class="nv">encoder</span>, <span class="nv">decoder</span>, <span class="nv">pair</span>[<span class="mi">0</span>]<span class="ss">)</span>
        # 将结果连成句子
        <span class="nv">output_sentence</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="s"> </span><span class="s1">&#39;</span>.<span class="nv">join</span><span class="ss">(</span><span class="nv">output_words</span><span class="ss">)</span>
        # <span class="o">&lt;</span> 代表模型的输出
        <span class="nv">print</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">&lt;</span><span class="s1">&#39;</span>, <span class="nv">output_sentence</span><span class="ss">)</span>
        <span class="nv">print</span><span class="ss">(</span><span class="s1">&#39;&#39;</span><span class="ss">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>调用:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code># 调用evaluateRandomly进行模型测试，将编码器对象encoder1，码器对象attn_decoder1传入其中
evaluateRandomly(encoder1, attn_decoder1)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>&gt; i m impressed with your french .
= je suis impressionne par votre francais .
&lt; je suis impressionnee par votre francais . &lt;EOS&gt;

&gt; i m more than a friend .
= je suis plus qu une amie .
&lt; je suis plus qu une amie . &lt;EOS&gt;

&gt; she is beautiful like her mother .
= elle est belle comme sa mere .
&lt; elle est sa sa mere . &lt;EOS&gt;

&gt; you re winning aren t you ?
= vous gagnez n est ce pas ?
&lt; tu restez n est ce pas ? &lt;EOS&gt;

&gt; he is angry with you .
= il est en colere apres toi .
&lt; il est en colere apres toi . &lt;EOS&gt;

&gt; you re very timid .
= vous etes tres craintifs .
&lt; tu es tres craintive . &lt;EOS&gt;
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>Attention张量制图:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>sentence = &quot;we re both teachers .&quot;
# 调用评估函数
output_words, attentions = evaluate(
encoder1, attn_decoder1, sentence)
print(output_words)
# 将attention张量转化成numpy, 使用matshow绘制
plt.matshow(attentions.numpy())
# 保存图像
plt.savefig(&quot;./s2s_attn.png&quot;)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[&#39;nous&#39;, &#39;sommes&#39;, &#39;toutes&#39;, &#39;deux&#39;, &#39;enseignantes&#39;, &#39;.&#39;, &#39;&lt;EOS&gt;&#39;]
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>Attention可视化:</li>
</ul>
</blockquote>
<p><center><img alt="avatar" src="img/s2s_attn.png" /></center></p>
<hr />
<blockquote>
<ul>
<li>分析:<ul>
<li>Attention图像的纵坐标代表输入的源语言各个词汇对应的索引, 0-6分别对应["we", "re", "both", "teachers", ".", "<EOS>"], 纵坐标代表生成的目标语言各个词汇对应的索引, 0-7代表['nous', 'sommes', 'toutes', 'deux', 'enseignantes', '.', '<EOS>'], 图中浅色小方块(颜色越浅说明影响越大)代表词汇之间的影响关系, 比如源语言的第1个词汇对生成目标语言的第1个词汇影响最大, 源语言的第4，5个词对生成目标语言的第5个词会影响最大, 通过这样的可视化图像, 我们可以知道Attention的效果好坏, 与我们人为去判定到底还有多大的差距. 进而衡量我们训练模型的可用性.</li>
</ul>
</li>
</ul>
</blockquote>
<hr />
<ul>
<li>
<p>小节总结:</p>
<ul>
<li>seq2seq模型架构分析:<ul>
<li>从图中可知, seq2seq模型架构, 包括两部分分别是encoder(编码器)和decoder(解码器), 编码器和解码器的内部实现都使用了GRU模型, 这里它要完成的是一个中文到英文的翻译: 欢迎 来 北京 --&gt; welcome to BeiJing. 编码器首先处理中文输入"欢迎 来 北京", 通过GRU模型获得每个时间步的输出张量，最后将它们拼接成一个中间语义张量c, 接着解码器将使用这个中间语义张量c以及每一个时间步的隐层张量, 逐个生成对应的翻译语言.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>基于GRU的seq2seq模型架构实现翻译的过程:<ul>
<li>第一步: 导入必备的工具包.</li>
<li>第二步: 对持久化文件中数据进行处理, 以满足模型训练要求.</li>
<li>第三步: 构建基于GRU的编码器和解码器.</li>
<li>第四步: 构建模型训练函数, 并进行训练.</li>
<li>第五步: 构建模型评估函数, 并进行测试以及Attention效果分析.</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第一步: 导入必备的工具包<ul>
<li>python版本使用3.6.x, pytorch版本使用1.3.1</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第二步: 对持久化文件中数据进行处理, 以满足模型训练要求<ul>
<li>将指定语言中的词汇映射成数值</li>
<li>字符规范化</li>
<li>将持久化文件中的数据加载到内存, 并实例化类Lang</li>
<li>过滤出符合我们要求的语言对</li>
<li>对以上数据准备函数进行整合, 并使用类Lang对语言对进行数值映射</li>
<li>将语言对转化为模型输入需要的张量</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第三步: 构建基于GRU的编码器和解码器<ul>
<li>构建基于GRU的编码器</li>
<li>构建基于GRU的解码器</li>
<li>构建基于GRU和Attention的解码器</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第四步: 构建模型训练函数, 并进行训练<ul>
<li>什么是teacher_forcing: 它是一种用于序列生成任务的训练技巧, 在seq2seq架构中, 根据循环神经网络理论，解码器每次应该使用上一步的结果作为输入的一部分, 但是训练过程中，一旦上一步的结果是错误的，就会导致这种错误被累积，无法达到训练效果, 因此，我们需要一种机制改变上一步出错的情况，因为训练时我们是已知正确的输出应该是什么，因此可以强制将上一步结果设置成正确的输出, 这种方式就叫做teacher_forcing.</li>
<li>teacher_forcing的作用: 能够在训练的时候矫正模型的预测，避免在序列生成的过程中误差进一步放大. 另外, teacher_forcing能够极大的加快模型的收敛速度，令模型训练过程更快更平稳.</li>
<li>构建训练函数train</li>
<li>构建时间计算函数timeSince</li>
<li>调用训练函数并打印日志和制图</li>
<li>损失曲线分析: 一直下降的损失曲线, 说明模型正在收敛, 能够从数据中找到一些规律应用于数据</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li>第五步: 构建模型评估函数, 并进行测试以及Attention效果分析<ul>
<li>构建模型评估函数evaluate</li>
<li>随机选择指定数量的数据进行评估</li>
<li>进行了Attention可视化分析</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<hr />
<hr />
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="1.html" class="md-footer__link md-footer__link--prev" aria-label="上一页: 1. RNN架构解析" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              1. RNN架构解析
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": ".", "features": [], "translations": {"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "assets/javascripts/workers/search.709b4209.min.js", "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.29db7785.min.js"></script>
      
    
  </body>
</html>