<!DOCTYPE html>
<!-- saved from url=(0029)http://121.199.45.168:8008/6/ -->
<html lang="zh" class="js json svg checked target dataset details fetch supports csstransforms3d no-ios" style=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
      
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="">
      
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="en, zh">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="./index_files/AI.jpg">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-4.4.0">
    
    
      
        <title>ResNet模型在GPU上的并行实践 - NLP案例集</title>
      
    
    
      <link rel="stylesheet" href="./index_files/application.0284f74d.css">
      
      
    
    
      <script src="./index_files/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin="">
        <link rel="stylesheet" href="./index_files/css">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="./index_files/material-icons.css">
    
      <link rel="manifest" href="http://121.199.45.168:8008/manifest.webmanifest">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-XXXXXXXX-X", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async="" src="./index_files/analytics.js"></script>
      
    
    
  <script type="text/javascript">(function(){var s=document.createElement("script");var port=window.location.port;s.src="//"+window.location.hostname+":"+port+ "/livereload.js?port=" + port;document.head.appendChild(s);})();</script><script src="./index_files/livereload.js"></script></head>
  
    <body dir="ltr" data-md-state="">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"></path></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#_1" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header" data-md-state="shadow">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="" title="NLP案例集" class="md-header-nav__button md-logo">
          
            <img src="./index_files/AI.jpg" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic" style="width: 717px;">
              NLP案例集
            </span>
            <span class="md-header-nav__topic" style="width: 717px;">
              
                ResNet模型在GPU上的并行实践
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix="">
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/AITutorials/manuals" title="前往 Github 仓库" class="md-source" data-md-source="github" data-md-state="done">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Github | Give Me A Star
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation" style="height: 810px;">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="" title="NLP案例集" class="md-nav__button md-logo">
      
        <img src="./index_files/AI.jpg" width="48" height="48">
      
    </a>
    NLP案例集
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/AITutorials/manuals" title="前往 Github 仓库" class="md-source" data-md-source="github" data-md-state="done">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    Github | Give Me A Star
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix="">
    
      
      
      


  <li class="md-nav__item">
    <a href="./index.html" title="图片的描述生成任务" class="md-nav__link">
      图片的描述生成任务
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="./index2.html" title="IMDB影评的情感分析任务" class="md-nav__link">
      IMDB影评的情感分析任务
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="./index3.html" title="莎士比亚风格的文本生成任务" class="md-nav__link">
      莎士比亚风格的文本生成任务
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="./index4.html" title="应用于bert模型的动态量化技术" class="md-nav__link">
      应用于bert模型的动态量化技术
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="./index5.html" title="基于seq2seq的西班牙语到英语的机器翻译任务" class="md-nav__link">
      基于seq2seq的西班牙语到英语的机器翻译任务
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        ResNet模型在GPU上的并行实践
      </label>
    
    <a href="" title="ResNet模型在GPU上的并行实践" class="md-nav__link md-nav__link--active">
      ResNet模型在GPU上的并行实践
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix="">
      
        <li class="md-nav__item">
  <a href="#_1" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" title="相关知识" class="md-nav__link">
    相关知识
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gpu" title="单机多GPU的模型并行" class="md-nav__link">
    单机多GPU的模型并行
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" title="第一步: 查看硬件配置并以一个简单示例理解模型分配" class="md-nav__link">
    第一步: 查看硬件配置并以一个简单示例理解模型分配
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resnet50gpu" title="第二步: 将大型模型ResNet50结构分配到多个GPU上" class="md-nav__link">
    第二步: 将大型模型ResNet50结构分配到多个GPU上
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpugpu" title="第三步: 对比模型多GPU并行和单GPU的耗时" class="md-nav__link">
    第三步: 对比模型多GPU并行和单GPU的耗时
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_1" title="第四步: 使用流水线技术加速多GPU训练" class="md-nav__link">
    第四步: 使用流水线技术加速多GPU训练
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc" style="height: 810px;">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix="">
      
        <li class="md-nav__item">
  <a href="#_1" title="学习目标" class="md-nav__link">
    学习目标
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" title="相关知识" class="md-nav__link">
    相关知识
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gpu" title="单机多GPU的模型并行" class="md-nav__link">
    单机多GPU的模型并行
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" title="第一步: 查看硬件配置并以一个简单示例理解模型分配" class="md-nav__link">
    第一步: 查看硬件配置并以一个简单示例理解模型分配
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resnet50gpu" title="第二步: 将大型模型ResNet50结构分配到多个GPU上" class="md-nav__link">
    第二步: 将大型模型ResNet50结构分配到多个GPU上
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpugpu" title="第三步: 对比模型多GPU并行和单GPU的耗时" class="md-nav__link">
    第三步: 对比模型多GPU并行和单GPU的耗时
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu_1" title="第四步: 使用流水线技术加速多GPU训练" class="md-nav__link">
    第四步: 使用流水线技术加速多GPU训练
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>ResNet模型在GPU上的并行实践</h1>
                
                <!-- Single-Machine Model Parallel Best Practices -->

<h3 id="_1">学习目标<a class="headerlink" href="#_1" title="Permanent link">¶</a></h3>
<ul>
<li>了解模型并行与数据并行的区别.</li>
<li>了解分布式训练与并行训练的关系.</li>
<li>掌握在单机多GPU上进行模型并行训练的解决方案.</li>
</ul>
<hr>
<p></p><center><img alt="avatar" src="./index_files/distribute.png"></center><p></p>
<hr>
<h3 id="_2">相关知识<a class="headerlink" href="#_2" title="Permanent link">¶</a></h3>
<ul>
<li>并行/分布训练及其两者的关系：<ul>
<li>在机器学习领域（深度学习），并行/分布方式一般主要应用在模型的训练阶段以加速模型的训练效率。因此，利用计算机系统的多线程或多进程来提升模型训练效率的方式都可以称作并行训练。其中，利用多进程训练的方式又可以叫做并行分布式训练，简称分布式训练（因为单台计算机多进程间的通信等同于多台计算机间的通信）。由此可见，分布式训练是并行训练的一种特殊形式。</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>数据并行训练：<ul>
<li>数据并行是一般指训练数据的每个批次数据被分割成n等份，分别送给同一模型，此时模型被复制了n份以接受不同数据，之后每个模型都会计算对应数据的梯度，然后所有的梯度求均值用以更新每个模型的参数，进而进行下个批次数据的并行（因为我们常用的批次SGD优化方法，就是求解该批次数据的平均梯度来更新参数）。</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>模型并行训练:<ul>
<li>模型并行是指模型网络结构被分割成n个部分，每一部分都会在处理完一条数据后立即处理下一条（如果模型不被分割成独立的各个部分，模型中的每一部分必须等待该条数据全部处理后，才能开始下一条数据处理）。</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>本案例着重讲解单机多GPU的模型并行方案，解决大型模型无法在单GPU上整体加载的问题。</li>
</ul>
<hr>
<h3 id="gpu">单机多GPU的模型并行<a class="headerlink" href="#gpu" title="Permanent link">¶</a></h3>
<ul>
<li>第一步: 查看硬件配置并以一个简单示例理解模型分配</li>
<li>第二步: 将大型模型ResNet50结构分配到多个GPU上</li>
<li>第三步: 对比模型多GPU并行和单GPU的耗时</li>
<li>第四步: 使用流水线技术加速多GPU训练</li>
<li>第五步: 寻找流水线参数以进一步加速多GPU训练</li>
</ul>
<hr>
<h4 id="_3">第一步: 查看硬件配置并以一个简单示例理解模型分配<a class="headerlink" href="#_3" title="Permanent link">¶</a></h4>
<ul>
<li>查看硬件配置</li>
</ul>
<div class="highlight"><pre id="__code_0"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_0 pre, #__code_0 code"><span class="md-clipboard__message"></span></button><code><span class="kn">import</span> <span class="nn">subprocess</span>

<span class="c1"># 打印nvidia显卡信息，包括cuda版本，显卡数量，当前使用情况等等</span>
<span class="k">print</span><span class="p">(</span><span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span><span class="s2">"nvidia-smi"</span><span class="p">,</span> <span class="n">universal_newlines</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
</code></pre></div>

<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<div class="highlight"><pre id="__code_1"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_1 pre, #__code_1 code"><span class="md-clipboard__message"></span></button><code># 这里我们可以看到:
# GPU Driver和CUDA的版本信息
# 两台GTX1080Ti的GPU运行情况 

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080Ti  Off  | 00000000:03:00.0 Off |                  N/A |
| 20%   38C    P0    54W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 1080Ti  Off  | 00000000:04:00.0 Off |                  N/A |
| 26%   45C    P0    53W / 250W |      0MiB / 11178MiB |      3%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre></div>

<ul>
<li>定义只有两个线性层的玩具模型:</li>
</ul>
<div class="highlight"><pre id="__code_2"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_2 pre, #__code_2 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 导入构建模型的必备工具包</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>


<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""定义一个玩具模型类"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ToyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 实例化第一个线性层(参数)，放在'0'号GPU上</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:0'</span><span class="p">)</span>
        <span class="c1"># 实例化ReLU层，无参数计算层不需要任何分配</span>
        <span class="c1"># 不占任何存储空间，只是一条计算指令</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="c1"># 实例化第二个线性层(参数)，放在'1'号GPU上</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 输入x需要与第一个线性层参数相乘，因此需要发送到'0'号GPU上</span>
        <span class="c1"># 接着在'0'号GPU上被ReLU函数激活</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:0'</span><span class="p">)))</span>
        <span class="c1"># 为了继续和第二个线性层参数相乘，因此需要发送到'1'号GPU上</span>
        <span class="c1"># 最后在'1'号GPU上返回计算结果</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net2</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">))</span>
</code></pre></div>

<ul>
<li>定义玩具模型的训练配置:</li>
</ul>
<div class="highlight"><pre id="__code_3"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_3 pre, #__code_3 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 实例化模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ToyModel</span><span class="p">()</span>
<span class="c1"># 选择损失函数</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="c1"># 选择优化方法</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># 梯度初始化为0</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="c1"># 使用随机张量输入模型获得输出</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># 因为模型的结果是在'1'号GPU上返回</span>
<span class="c1"># 所以也要将真实标签分配给'1'号GPU</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">)</span>

<span class="c1"># 计算损失</span>
<span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="c1"># 更新权重</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<hr>
<h4 id="resnet50gpu">第二步: 将大型模型ResNet50结构分配到多个GPU上<a class="headerlink" href="#resnet50gpu" title="Permanent link">¶</a></h4>
<div class="highlight"><pre id="__code_4"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_4 pre, #__code_4 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 导入ResNet的主结构，和ResNet50的组成单元Bottleneck</span>
<span class="kn">from</span> <span class="nn">torchvision.models.resnet</span> <span class="kn">import</span> <span class="n">ResNet</span><span class="p">,</span> <span class="n">Bottleneck</span>

<span class="c1"># 原生ResNet50输出类别为1000</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">1000</span>


<span class="k">class</span> <span class="nc">ModelParallelResNet50</span><span class="p">(</span><span class="n">ResNet</span><span class="p">):</span>
    <span class="sd">"""在两台GPU上分配的并行ResNet50模型"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 从ResNet主结构中初始化特定参数使其成为ResNet50</span>
        <span class="c1"># 第一个初始化参数Bottleneck是ResNet50的特定块单元</span>
        <span class="c1"># 第二个初始化参数[3, 4, 6, 3]是指ResNet50四个块单元对应的层数</span>
        <span class="c1"># [3, 4, 6, 3]对于ResNet50是固定的，如果ResNet101，则对应[3, 4, 23, 3]</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ModelParallelResNet50</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">Bottleneck</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># 重写ResNet50结构，使其分配在两台GPU上</span>
        <span class="c1"># 内部的计算层和顺序都是固定的</span>
        <span class="c1"># 前两个块单元(layer1, layer2)在'0'号GPU上</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:0'</span><span class="p">)</span>

        <span class="c1"># 后两个块单元(layer3, layer4)在'1'号GPU上</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># seq1处理后，将结果发送到'1'号GPU上</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>

<ul>
<li>定义ResNet50模型训练配置:</li>
</ul>
<div class="highlight"><pre id="__code_5"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_5 pre, #__code_5 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 定义模型训练的相关配置</span>
<span class="n">num_batches</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">120</span>
<span class="n">image_w</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">image_h</span> <span class="o">=</span> <span class="mi">128</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="sd">"""模型训练函数"""</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># 定义损失函数</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="c1"># 定义优化方法</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    <span class="c1"># 生成一个[batch, 1]形状的张量，里面的每个值都是[0, 1000)值域内的随机数</span>
    <span class="c1"># 这个张量将用于之后生成真实标签</span>
    <span class="n">one_hot_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> \
                           <span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span> \
                           <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 开始batch循环</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
        <span class="c1"># 随机初始化指定尺寸的输入 </span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">image_w</span><span class="p">,</span> <span class="n">image_h</span><span class="p">)</span>
        <span class="c1"># 初始化一个[batch_size, num_classes]大小的零张量</span>
        <span class="c1"># 使用scatter_方法向这个张量中填充数值</span>
        <span class="c1"># 第一个参数为1，代表每次按照纵轴方向填充</span>
        <span class="c1"># 第二个参数为one_hot_indices，代表每一列填充的位置索引</span>
        <span class="c1"># 第三个参数为1，填充的值为1</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span> \
                      <span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">one_hot_indices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 梯度归零</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># 首先还是将输入发送到'0'号GPU上</span>
        <span class="c1"># 再调用模型得到输出</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:0'</span><span class="p">))</span>

        <span class="c1"># 为了计算损失，需要把真实标签发送到输出结果的设备上</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># 在指定设备上计算损失</span>
        <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># 根据梯度更新参数</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<h4 id="gpugpu">第三步: 对比模型多GPU并行和单GPU的耗时<a class="headerlink" href="#gpugpu" title="Permanent link">¶</a></h4>
<ul>
<li>绘制模型双GPU并行和单GPU的耗时图</li>
</ul>
<div class="highlight"><pre id="__code_6"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_6 pre, #__code_6 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 导入matplotlib用于绘图</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="c1"># 设置绘图风格</span>
<span class="n">plt</span><span class="o">.</span><span class="n">switch_backend</span><span class="p">(</span><span class="s1">'Agg'</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># 导入timeit，这是专门用于并行计算统计模型耗时的工具包</span>
<span class="kn">import</span> <span class="nn">timeit</span>

<span class="c1"># 设定timeit的重复参数，为了凸显训练的时间的差异，将重复10次</span>
<span class="n">num_repeat</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># 设定timeit的目标函数(将计算该函数的耗时)</span>
<span class="n">stmt</span> <span class="o">=</span> <span class="s2">"train(model)"</span>

<span class="c1"># 设定timeit的启动语句，即计算耗时开始前运行的语句</span>
<span class="c1"># 启动语句为实例化并行的ResNet50模型</span>
<span class="n">setup</span> <span class="o">=</span> <span class="s2">"model = ModelParallelResNet50()"</span>

<span class="c1"># 连续计算10次并行的ResNet50模型的耗时</span>
<span class="c1"># stmt为执行的目标函数字符串形式</span>
<span class="c1"># setup为执行前的启动语句</span>
<span class="c1"># number为目标函数执行的次数，number=1表示目标函数只执行一次就计算耗时</span>
<span class="c1"># repeat为计算耗时的次数，number=1，repeat=10表示目标函数执行一次并计算该次耗时；</span>
<span class="c1"># 反复进行10次，得到10个结果</span>
<span class="c1"># globals=globals()表示代码能在当前的全局名称空间中执行，使用所有变量</span>
<span class="n">mp_run_times</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
    <span class="n">stmt</span><span class="p">,</span> <span class="n">setup</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="n">num_repeat</span><span class="p">,</span> <span class="nb">globals</span><span class="o">=</span><span class="nb">globals</span><span class="p">())</span>

<span class="c1"># 计算10次结果的平均值和标准差</span>
<span class="n">mp_mean</span><span class="p">,</span> <span class="n">mp_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mp_run_times</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mp_run_times</span><span class="p">)</span>

<span class="c1"># 启动语句为实例化单GPU的ResNet50模型</span>
<span class="n">setup</span> <span class="o">=</span> <span class="s2">"import torchvision.models as models;"</span> <span class="o">+</span> \
        <span class="s2">"model = models.resnet50(num_classes=num_classes).to('cuda:0')"</span>

<span class="c1"># 计算单GPU的ResNet50模型耗时</span>
<span class="n">rn_run_times</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
    <span class="n">stmt</span><span class="p">,</span> <span class="n">setup</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="n">num_repeat</span><span class="p">,</span> <span class="nb">globals</span><span class="o">=</span><span class="nb">globals</span><span class="p">())</span>
<span class="c1"># 计算10次结果的平均值和标准差</span>
<span class="n">rn_mean</span><span class="p">,</span> <span class="n">rn_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rn_run_times</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">rn_run_times</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">fig_name</span><span class="p">):</span>
    <span class="sd">"""绘图函数"""</span>
    <span class="c1"># 创建子图画布</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="c1"># 在画布上绘制柱状图, 设置相关配置</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">means</span><span class="p">)),</span> <span class="n">means</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">stds</span><span class="p">,</span>
           <span class="n">align</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ecolor</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="c1"># 设置纵轴说明</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'ResNet50 Execution Time (Second)'</span><span class="p">)</span>
    <span class="c1"># 设置横轴刻度</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">means</span><span class="p">)))</span>
    <span class="c1"># 设置横轴刻度标签</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="c1"># 设置y轴网格线</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># 设置布局</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="c1"># 保存图片</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fig_name</span><span class="p">)</span>
    <span class="c1"># 关闭图片</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>


<span class="c1"># 向函数中传入对应参数</span>
<span class="n">plot</span><span class="p">([</span><span class="n">mp_mean</span><span class="p">,</span> <span class="n">rn_mean</span><span class="p">],</span>
     <span class="p">[</span><span class="n">mp_std</span><span class="p">,</span> <span class="n">rn_std</span><span class="p">],</span>
     <span class="p">[</span><span class="s1">'Model Parallel'</span><span class="p">,</span> <span class="s1">'Single GPU'</span><span class="p">],</span>
     <span class="s1">'mp_vs_rn.png'</span><span class="p">)</span>
</code></pre></div>

<hr>
<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<p><img alt="" src="./index_files/mp_vs_rn.png"></p>
<hr>
<blockquote>
<ul>
<li>分析:<ul>
<li>由图可知，单GPU的运行时间小于模型分配在两台GPU上的运行时间，这是因为: 在当前状态下，两台GPU上的模型同一时间仅有一台GPU工作，并他们还要花费时间在相互的数据传输上。为了改善这种状况，我们将使用模型训练的流水线技术，下面将详细讲解。</li>
</ul>
</li>
</ul>
</blockquote>
<hr>
<h4 id="gpu_1">第四步: 使用流水线技术加速多GPU训练<a class="headerlink" href="#gpu_1" title="Permanent link">¶</a></h4>
<ul>
<li>模型训练的流水线技术:<ul>
<li>流水线技术旨在使分布在不同GPU上的模型能够在同一时间都在处理对应工作，以此提升训练效率。流水线技术的原理是通过将数据划分为N份(N&gt;1)，每份数据称作一个数据堆。当第一个GPU处理完第一个数据堆后，将数据发送给第二个GPU，之后第一个GPU不会像之前一样等待第二个GPU处理完成，而是马上处理第二个数据堆，此时间点上，两个GPU都在运行处理对应的工作，直到将所有数据堆处理完成。</li>
<li>以上是标准的流水线过程，必须开启与GPU等数量的线程来控制这些异步行为。而在实际工程中，为了避免代码的复杂度过高，往往不去使用异步的处理机制，这是因为当我们把批次数据切分为足够小的数据堆时，单个GPU处理它们的速度已经非常快，其他GPU的等待时间可以忽略。也就是说，第二个GPU在处理第一个数据堆时，不需要使用其他线程让第一个GPU异步处理数据，而只是等待其完成后，再继续处理第二个数据堆。接下来，我们将按照这种方式实现流水线并对比效果。</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>使用流水线技术加速多GPU训练的实现:</li>
</ul>
<div class="highlight"><pre id="__code_7"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_7 pre, #__code_7 code"><span class="md-clipboard__message"></span></button><code><span class="k">class</span> <span class="nc">PipelineParallelResNet50</span><span class="p">(</span><span class="n">ModelParallelResNet50</span><span class="p">):</span>
    <span class="sd">"""带有流水线技术的并行模型ResNet50"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 继承ModelParallelResNet50的初始化函数</span>
        <span class="c1"># 加入了新的初始化参数split_size，代表每个批次数据划分的大小</span>
        <span class="c1"># 如: batch_size=120, split_size=20说明将120条数据划分成6份，</span>
        <span class="c1"># 每份20条作为流水线处理的条数</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PipelineParallelResNet50</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_size</span> <span class="o">=</span> <span class="n">split_size</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">"""重写流水线的forward函数"""</span>
        <span class="c1"># 将输入的批次数据按照split_size划分，并使用迭代器封装</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="c1"># 使用next方法取出迭代器中的第一份数据(第一个数据堆)</span>
        <span class="n">s_next</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">splits</span><span class="p">)</span>
        <span class="c1"># 将数据在'0'号GPU上处理后发送给'1'号GPU</span>
        <span class="n">s_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq1</span><span class="p">(</span><span class="n">s_next</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">)</span>
        <span class="c1"># 创建一个存储最终处理结果的列表</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># 循环遍历迭代器中的所有数据堆</span>
        <span class="k">for</span> <span class="n">s_next</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
            <span class="c1"># 在'1'号GPU上处理'0'号GPU上发来的数据</span>
            <span class="n">s_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq2</span><span class="p">(</span><span class="n">s_prev</span><span class="p">)</span>
            <span class="c1"># 将结果view成指定维度输入到全连接层</span>
            <span class="c1"># 最后装进结果列表</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">s_prev</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s_prev</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
            <span class="c1"># 继续将数据在'0'号GPU上处理后发送给'1'号GPU</span>
            <span class="n">s_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq1</span><span class="p">(</span><span class="n">s_next</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda:1'</span><span class="p">)</span>

        <span class="c1"># 当最后一个数据堆循环遍历完成后，只是发送给'1'号GPU并没有处理</span>
        <span class="c1"># 所以这里要在'1'号GPU上处理完成</span>
        <span class="n">s_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq2</span><span class="p">(</span><span class="n">s_prev</span><span class="p">)</span>
        <span class="c1"># 同样将结果view成指定维度输入到全连接层</span>
        <span class="c1"># 最后装进结果列表</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">s_prev</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">s_prev</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="c1"># 返回结果的张量形式</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>


<span class="c1"># 启动语句为实例化带有流水线的多GPU并行ResNet50模型</span>
<span class="n">setup</span> <span class="o">=</span> <span class="s2">"model = PipelineParallelResNet50()"</span>

<span class="c1"># 使用timeit进行耗时计算，参数与上述使用时相同</span>
<span class="n">pp_run_times</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
    <span class="n">stmt</span><span class="p">,</span> <span class="n">setup</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="n">num_repeat</span><span class="p">,</span> <span class="nb">globals</span><span class="o">=</span><span class="nb">globals</span><span class="p">())</span>
<span class="c1"># 计算均值和标准差</span>
<span class="n">pp_mean</span><span class="p">,</span> <span class="n">pp_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pp_run_times</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pp_run_times</span><span class="p">)</span>

<span class="c1"># 绘制耗时对比图</span>
<span class="n">plot</span><span class="p">([</span><span class="n">mp_mean</span><span class="p">,</span> <span class="n">rn_mean</span><span class="p">,</span> <span class="n">pp_mean</span><span class="p">],</span>
     <span class="p">[</span><span class="n">mp_std</span><span class="p">,</span> <span class="n">rn_std</span><span class="p">,</span> <span class="n">pp_std</span><span class="p">],</span>
     <span class="p">[</span><span class="s1">'Model Parallel'</span><span class="p">,</span> <span class="s1">'Single GPU'</span><span class="p">,</span> <span class="s1">'Pipelining Model Parallel'</span><span class="p">],</span>
     <span class="s1">'mp_vs_rn_vs_pp.png'</span><span class="p">)</span>
</code></pre></div>

<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<p><img alt="" src="./index_files/mp_vs_rn_vs_pp.png"></p>
<blockquote>
<ul>
<li>分析:<ul>
<li>从图中可知，带有流水线技术的模型训练耗时(运行时间)最短，相对比单GPU运行已经有了明显改善。但是我们发现，流水线技术引进了一个新的参数split_size，它代表数据堆的大小，也直接影响了模型训练的耗时，我们可以使用两个极端的例子来解释这种影响，当split_size与batch_size大小相同时，即等效是没有使用流水线的情况，耗时大于单GPU。而当split_size=1时，计算时间和等待时间虽然足够小，但是GPU之间的数据传输时间将被放大，导致训练耗时变长，下面我们将从实验中寻找最佳的split_size。</li>
</ul>
</li>
</ul>
</blockquote>
<hr>
<ul>
<li>第五步: 寻找流水线参数以进一步加速多GPU训练</li>
</ul>
<div class="highlight"><pre id="__code_8"><span></span><button class="md-clipboard" title="复制" data-clipboard-target="#__code_8 pre, #__code_8 code"><span class="md-clipboard__message"></span></button><code><span class="c1"># 创建存储均值和标准差的列表</span>
<span class="n">means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">stds</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 设置一组split_size的采样点</span>
<span class="n">split_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">60</span><span class="p">]</span>

<span class="c1"># 遍历采样点 </span>
<span class="k">for</span> <span class="n">split_size</span> <span class="ow">in</span> <span class="n">split_sizes</span><span class="p">:</span>
    <span class="c1"># 启动语句为实例化带有流水线的多GPU并行ResNet50模型</span>
    <span class="n">setup</span> <span class="o">=</span> <span class="s2">"model = PipelineParallelResNet50(split_size=</span><span class="si">%d</span><span class="s2">)"</span> <span class="o">%</span> <span class="n">split_size</span>
    <span class="c1"># 使用timeit计算各个采样点的耗时</span>
    <span class="n">pp_run_times</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span>
        <span class="n">stmt</span><span class="p">,</span> <span class="n">setup</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="n">num_repeat</span><span class="p">,</span> <span class="nb">globals</span><span class="o">=</span><span class="nb">globals</span><span class="p">())</span>
    <span class="c1"># 保存均值和标准差</span>
    <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pp_run_times</span><span class="p">))</span>
    <span class="n">stds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">pp_run_times</span><span class="p">))</span>

<span class="c1"># 创建画布</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="c1"># 绘制均值曲线</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">split_sizes</span><span class="p">,</span> <span class="n">means</span><span class="p">)</span>
<span class="c1"># 绘制均值点的上下浮动范围(标准差)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">split_sizes</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">stds</span><span class="p">,</span> <span class="n">ecolor</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">'ro'</span><span class="p">)</span>
<span class="c1"># 设置横纵坐标名称</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'ResNet50 Execution Time (Second)'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Pipeline Split Size'</span><span class="p">)</span>
<span class="c1"># 设置刻度</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">split_sizes</span><span class="p">)</span>
<span class="c1"># 设置网格显示</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># 设置布局</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="c1"># 保存图片</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">"split_size_tradeoff.png"</span><span class="p">)</span>
<span class="c1"># 关闭画布</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</code></pre></div>

<blockquote>
<ul>
<li>输出效果:</li>
</ul>
</blockquote>
<p><img alt="" src="./index_files/split_size_tradeoff.png"></p>
<blockquote>
<ul>
<li>分析:<ul>
<li>从图中可以看出，最佳的split_size是12，此时耗时最短。如果继续减小split_size的值，硬件间的数据传输时间将显著增加。所以，在使用模型并行的流水线技术时，一般应该先通过采样点找到合适的split_size值作为参数，再进行模型并行训练。</li>
</ul>
</li>
</ul>
</blockquote>
<hr>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="./index5.html" title="基于seq2seq的西班牙语到英语的机器翻译任务" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  上一页
                </span>
                基于seq2seq的西班牙语到英语的机器翻译任务
              </span>
            </div>
          </a>
        
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org/">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="./index_files/application.245445c6.js"></script>
      
        
        
          
          <script src="./index_files/lunr.stemmer.support.js"></script>
          
            
          
            
              
              
            
          
          
            <script src="./index_files/lunr.multi.js"></script>
          
        
      
      <script>app.initialize({version:"1.1.2",url:{base:".."}})</script>
      
    
  
</body></html>