
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="canonical" href="https://www.tisv.cn/3/1.html">
      
      <link rel="icon" href="img/AI.jpg">
      <meta name="generator" content="mkdocs-1.2.2, mkdocs-material-7.2.3">
    
    
      
        <title>1. Pytorch基本语法 - Pytorch</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.f7f47774.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.3f5d1f46.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    <script>function __prefix(e){return new URL(".",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#11-pytorch" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="Pytorch" class="md-header__button md-logo" aria-label="Pytorch" data-md-component="logo">
      
  <img src="img/AI.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Pytorch
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              1. Pytorch基本语法
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
    <img src="assets/images/logo.svg" height="45px" alt="logo">

  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="Pytorch" class="md-nav__button md-logo" aria-label="Pytorch" data-md-component="logo">
      
  <img src="img/AI.jpg" alt="logo">

    </a>
    Pytorch
  </label>
  
    <div class="md-nav__source">
      

<a href="https://github.com/AITutorials/manuals" title="前往 GitHub 仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Github
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          1. Pytorch基本语法
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="1.html" class="md-nav__link md-nav__link--active">
        1. Pytorch基本语法
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#11-pytorch" class="md-nav__link">
    1.1 认识Pytorch
  </a>
  
    <nav class="md-nav" aria-label="1.1 认识Pytorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch" class="md-nav__link">
    什么是Pytorch
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_1" class="md-nav__link">
    Pytorch的基本元素操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_2" class="md-nav__link">
    Pytorch的基本运算操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torch-tensornumpy-array" class="md-nav__link">
    关于Torch Tensor和Numpy array之间的相互转换
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-pytorchautograd" class="md-nav__link">
    1.2 Pytorch中的autograd
  </a>
  
    <nav class="md-nav" aria-label="1.2 Pytorch中的autograd">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchtensor" class="md-nav__link">
    关于torch.Tensor
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensor" class="md-nav__link">
    关于Tensor的操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradients" class="md-nav__link">
    关于梯度Gradients
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="2.html" class="md-nav__link">
        2. Pytorch初步应用
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#11-pytorch" class="md-nav__link">
    1.1 认识Pytorch
  </a>
  
    <nav class="md-nav" aria-label="1.1 认识Pytorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch" class="md-nav__link">
    什么是Pytorch
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_1" class="md-nav__link">
    Pytorch的基本元素操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_2" class="md-nav__link">
    Pytorch的基本运算操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torch-tensornumpy-array" class="md-nav__link">
    关于Torch Tensor和Numpy array之间的相互转换
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-pytorchautograd" class="md-nav__link">
    1.2 Pytorch中的autograd
  </a>
  
    <nav class="md-nav" aria-label="1.2 Pytorch中的autograd">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    学习目标
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchtensor" class="md-nav__link">
    关于torch.Tensor
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensor" class="md-nav__link">
    关于Tensor的操作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradients" class="md-nav__link">
    关于梯度Gradients
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    小节总结
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>1. Pytorch基本语法</h1>
                
                <h2 id="11-pytorch">1.1 认识Pytorch</h2>
<hr />
<h3 id="_1">学习目标</h3>
<ul>
<li>了解什么是Pytorch.</li>
<li>掌握Pytorch的基本元素操作.</li>
<li>掌握Pytorch的基本运算操作.</li>
</ul>
<hr />
<p><center><img alt="" src="img/pytorch.png" /></center></p>
<h3 id="pytorch">什么是Pytorch</h3>
<ul>
<li>Pytorch是一个基于Numpy的科学计算包, 向它的使用者提供了两大功能.<ul>
<li>作为Numpy的替代者, 向用户提供使用GPU强大功能的能力.</li>
<li>做为一款深度学习的平台, 向用户提供最大的灵活性和速度.</li>
</ul>
</li>
</ul>
<hr />
<h3 id="pytorch_1">Pytorch的基本元素操作</h3>
<ul>
<li>
<p>Tensors张量: 张量的概念类似于Numpy中的ndarray数据结构, 最大的区别在于Tensor可以利用GPU的加速功能.</p>
</li>
<li>
<p>我们使用Pytorch的时候, 常规步骤是先将torch引用进来, 如下所示:</p>
</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">torch</span>
</code></pre></div>

<hr />
<ul>
<li>创建矩阵的操作</li>
</ul>
<blockquote>
<ul>
<li>创建一个没有初始化的矩阵:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[2.4835e+27, 2.5428e+30, 1.0877e-19],
        [1.5163e+23, 2.2012e+12, 3.7899e+22],
        [5.2480e+05, 1.0175e+31, 9.7056e+24],
        [1.6283e+32, 3.7913e+22, 3.9653e+28],
        [1.0876e-19, 6.2027e+26, 2.3685e+21]])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>创建一个有初始化的矩阵:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[0.1368, 0.8070, 0.4567],
        [0.4369, 0.8278, 0.5552],
        [0.6848, 0.4473, 0.1031],
        [0.5308, 0.9194, 0.2761],
        [0.0484, 0.9941, 0.2227]])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>对比有无初始化的矩阵: 当声明一个未初始化的矩阵时, 它本身不包含任何确切的值. 当创建一个未初始化的矩阵时, 分配给矩阵的内存中有什么数值就赋值给了这个矩阵, 本质上是毫无意义的数据.</li>
</ul>
</blockquote>
<hr />
<blockquote>
<ul>
<li>创建一个全零矩阵并可指定数据元素的类型为long</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>直接通过数据创建张量</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([2.5000, 3.3000])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>通过已有的一个张量创建相同尺寸的新张量</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 利用news_methods方法得到一个张量</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 利用randn_like方法得到相同张量尺寸的一个新张量, 并且采用随机初始化来对其赋值</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)

tensor([[-0.1497, -0.5832, -0.3805],
        [ 0.9001,  2.0637,  1.3299],
        [-0.8813, -0.6579, -0.9135],
        [-0.1374,  0.1000, -0.9343],
        [-1.1278, -0.9140, -1.5910]])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>得到张量的尺寸:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>torch.Size([5, 3])
</code></pre></div>

<hr />
<ul>
<li>注意: <ul>
<li>torch.Size函数本质上返回的是一个tuple, 因此它支持一切元组的操作.</li>
</ul>
</li>
</ul>
<hr />
<h3 id="pytorch_2">Pytorch的基本运算操作</h3>
<ul>
<li>加法操作:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[ 1.6978, -1.6979,  0.3093],
        [ 0.4953,  0.3954,  0.0595],
        [-0.9540,  0.3353,  0.1251],
        [ 0.6883,  0.9775,  1.1764],
        [ 2.6784,  0.1209,  1.5542]])
</code></pre></div>

<hr />
<ul>
<li>第二种加法方式:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[ 1.6978, -1.6979,  0.3093],
        [ 0.4953,  0.3954,  0.0595],
        [-0.9540,  0.3353,  0.1251],
        [ 0.6883,  0.9775,  1.1764],
        [ 2.6784,  0.1209,  1.5542]])
</code></pre></div>

<hr />
<ul>
<li>第三种加法方式:</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="c1"># 提前设定一个空的张量</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># 将空的张量作为加法的结果存储张量</span>
<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">result</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[ 1.6978, -1.6979,  0.3093],
        [ 0.4953,  0.3954,  0.0595],
        [-0.9540,  0.3353,  0.1251],
        [ 0.6883,  0.9775,  1.1764],
        [ 2.6784,  0.1209,  1.5542]])
</code></pre></div>

<hr />
<ul>
<li>第四种加法方式: in-place (原地置换)</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">y</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[ 1.6978, -1.6979,  0.3093],
        [ 0.4953,  0.3954,  0.0595],
        [-0.9540,  0.3353,  0.1251],
        [ 0.6883,  0.9775,  1.1764],
        [ 2.6784,  0.1209,  1.5542]])
</code></pre></div>

<hr />
<ul>
<li>注意: <ul>
<li>所有in-place的操作函数都有一个下划线的后缀.</li>
<li>比如x.copy_(y), x.add_(y), 都会直接改变x的值.</li>
</ul>
</li>
</ul>
<hr />
<blockquote>
<ul>
<li>用类似于Numpy的方式对张量进行操作:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([-2.0902, -0.4489, -0.1441,  0.8035, -0.8341])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>改变张量的形状: torch.view()</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="c1"># tensor.view()操作需要保证数据元素的总数量不变</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="c1"># -1代表自动匹配个数</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>如果张量中只有一个元素, 可以用.item()将值取出, 作为一个python number</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([-0.3531])
-0.3530771732330322
</code></pre></div>

<hr />
<h3 id="torch-tensornumpy-array">关于Torch Tensor和Numpy array之间的相互转换</h3>
<ul>
<li>Torch Tensor和Numpy array共享底层的内存空间, 因此改变其中一个的值, 另一个也会随之被改变.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([1., 1., 1., 1., 1.])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>将Torch Tensor转换为Numpy array</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[1. 1. 1. 1. 1.]
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>对其中一个进行加法操作, 另一个也随之被改变:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">a</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>将Numpy array转换为Torch Tensor:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
</code></pre></div>

<hr />
<ul>
<li>注意: <ul>
<li>所有在CPU上的Tensors, 除了CharTensor, 都可以转换为Numpy array并可以反向转换.</li>
</ul>
</li>
</ul>
<hr />
<blockquote>
<ul>
<li>关于Cuda Tensor: Tensors可以用.to()方法来将其移动到任意设备上.</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># 如果服务器上已经安装了GPU和CUDA</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="c1"># 定义一个设备对象, 这里指定成CUDA, 即使用GPU</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="c1"># 直接在GPU上创建一个Tensor</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># 将在CPU上面的x张量移动到GPU上面</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># x和y都在GPU上面, 才能支持加法运算</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="c1"># 此处的张量z在GPU上面</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="c1"># 也可以将z转移到CPU上面, 并同时指定张量元素的数据类型</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">))</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([0.6469], device=&#39;cuda:0&#39;)
tensor([0.6469], dtype=torch.float64)
</code></pre></div>

<hr />
<h3 id="_2">小节总结</h3>
<ul>
<li>
<p>学习了什么是Pytorch.</p>
<ul>
<li>Pytorch是一个基于Numpy的科学计算包, 作为Numpy的替代者, 向用户提供使用GPU强大功能的能力.</li>
<li>做为一款深度学习的平台, 向用户提供最大的灵活性和速度.</li>
</ul>
</li>
<li>
<p>学习了Pytorch的基本元素操作.</p>
<ul>
<li>矩阵的初始化:<ul>
<li>torch.empty()</li>
<li>torch.rand(n, m)</li>
<li>torch.zeros(n, m, dtype=torch.long)</li>
</ul>
</li>
<li>其他若干操作:<ul>
<li>x.new_ones(n, m, dtype=torch.double)</li>
<li>torch.randn_like(x, dtype=torch.float)</li>
<li>x.size()</li>
</ul>
</li>
</ul>
</li>
<li>
<p>学习了Pytorch的基本运算操作.</p>
<ul>
<li>加法操作:<ul>
<li>x + y</li>
<li>torch.add(x, y)</li>
<li>torch.add(x, y, out=result)</li>
<li>y.add_(x)</li>
</ul>
</li>
<li>其他若干操作:<ul>
<li>x.view()</li>
<li>x.item()</li>
</ul>
</li>
</ul>
</li>
<li>
<p>学习了Torch Tensor和Numpy Array之间的相互转换.</p>
<ul>
<li>将Torch Tensor转换为Numpy Array:<ul>
<li>b = a.numpy()</li>
</ul>
</li>
<li>将Numpy Array转换为Torch Tensor:<ul>
<li>b = torch.from_numpy(a)</li>
</ul>
</li>
<li>注意: 所有才CPU上的Tensor, 除了CharTensor, 都可以转换为Numpy Array并可以反向转换.</li>
</ul>
</li>
<li>
<p>学习了任意的Tensors可以用.to()方法来将其移动到任意设备上.</p>
<ul>
<li>x = x.to(device)</li>
</ul>
</li>
</ul>
<hr />
<hr />
<hr />
<h2 id="12-pytorchautograd">1.2 Pytorch中的autograd</h2>
<hr />
<h3 id="_3">学习目标</h3>
<ul>
<li>掌握自动求导中的Tensor概念和操作.</li>
<li>掌握自动求导中的梯度Gradients概念和操作.</li>
</ul>
<hr />
<ul>
<li>在整个Pytorch框架中, 所有的神经网络本质上都是一个autograd package(自动求导工具包)<ul>
<li>autograd package提供了一个对Tensors上所有的操作进行自动微分的功能.</li>
</ul>
</li>
</ul>
<hr />
<h3 id="torchtensor">关于torch.Tensor</h3>
<ul>
<li>torch.Tensor是整个package中的核心类, 如果将属性.requires_grad设置为True, 它将追踪在这个类上定义的所有操作. 当代码要进行反向传播的时候, 直接调用.backward()就可以自动计算所有的梯度. 在这个Tensor上的所有梯度将被累加进属性.grad中.</li>
<li>如果想终止一个Tensor在计算图中的追踪回溯, 只需要执行.detach()就可以将该Tensor从计算图中撤下, 在未来的回溯计算中也不会再计算该Tensor.</li>
<li>除了.detach(), 如果想终止对计算图的回溯, 也就是不再进行方向传播求导数的过程, 也可以采用代码块的方式with torch.no_grad():, 这种方式非常适用于对模型进行预测的时候, 因为预测阶段不再需要对梯度进行计算.</li>
</ul>
<hr />
<ul>
<li>关于torch.Function:<ul>
<li>Function类是和Tensor类同等重要的一个核心类, 它和Tensor共同构建了一个完整的类, 每一个Tensor拥有一个.grad_fn属性, 代表引用了哪个具体的Function创建了该Tensor.</li>
<li>如果某个张量Tensor是用户自定义的, 则其对应的grad_fn is None.</li>
</ul>
</li>
</ul>
<hr />
<h3 id="tensor">关于Tensor的操作</h3>
<div class="codehilite"><pre><span></span><code><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])

tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>在具有requires_grad=True的Tensor上执行一个加法操作</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>打印Tensor的grad_fn属性:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>None
&lt;AddBackward0 object at 0x10db11208&gt;
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>在Tensor上执行更复杂的操作:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>关于方法.requires_grad_(): 该方法可以原地改变Tensor的属性.requires_grad的值. 如果没有主动设定默认为False.</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>False
True
&lt;SumBackward0 object at 0x7f191afd6be0&gt;
</code></pre></div>

<hr />
<h3 id="gradients">关于梯度Gradients</h3>
<ul>
<li>在Pytorch中, 反向传播是依靠.backward()实现的.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>关于自动求导的属性设置: 可以通过设置.requires_grad=True来执行自动求导, 也可以通过代码块的限制来停止自动求导.</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>True
True
False
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>可以通过.detach()获得一个新的Tensor, 拥有相同的内容但不需要自动求导.</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">())</span>
</code></pre></div>

<hr />
<blockquote>
<ul>
<li>输出结果:</li>
</ul>
</blockquote>
<div class="codehilite"><pre><span></span><code>True
False
tensor(True)
</code></pre></div>

<hr />
<h3 id="_4">小节总结</h3>
<ul>
<li>
<p>学习了torch.Tensor类的相关概念.</p>
<ul>
<li>torch.Tensor是整个package中的核心类, 如果将属性.requires_grad设置为True, 它将追踪在这个类上定义的所有操作. 当代码要进行反向传播的时候, 直接调用.backward()就可以自动计算所有的梯度. 在这个Tensor上的所有梯度将被累加进属性.grad中.</li>
<li>执行.detach()命令, 可以将该Tensor从计算图中撤下, 在未来的回溯计算中不会再计算该Tensor.</li>
<li>采用代码块的方式也可以终止对计算图的回溯:<ul>
<li>with torch.no_grad():</li>
</ul>
</li>
</ul>
</li>
<li>
<p>学习了关于Tensor的若干操作:</p>
<ul>
<li>torch.ones(n, n, requires_grad=True)</li>
<li>x.grad_fn</li>
<li>a.requires_grad_(True)</li>
</ul>
</li>
<li>
<p>学习了关于Gradients的属性:</p>
<ul>
<li>x.grad</li>
<li>可以通过.detach()获得一个新的Tensor, 拥有相同的内容但不需要自动求导.</li>
</ul>
</li>
</ul>
<hr />
<hr />
<hr />
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
      
        
        <a href="2.html" class="md-footer__link md-footer__link--next" aria-label="下一页: 2. Pytorch初步应用" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              2. Pytorch初步应用
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            ©Copyright 2020, AITutorials.CN This website has been reviewed by the review agency. 京ICP备19006137号
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": ".", "features": [], "translations": {"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "assets/javascripts/workers/search.709b4209.min.js", "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.29db7785.min.js"></script>
      
    
  </body>
</html>