{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"\u8be6\u89c1\uff1ahttp://52.83.69.131:8020/1/","title":"Home"},{"location":"index.html#http52836913180201","text":"","title":"\u8be6\u89c1\uff1ahttp://52.83.69.131:8020/1/"},{"location":"1.html","text":"1.1 \u8ba4\u8bc6Pytorch \u5b66\u4e60\u76ee\u6807 \u4e86\u89e3\u4ec0\u4e48\u662fPytorch. \u638c\u63e1Pytorch\u7684\u57fa\u672c\u5143\u7d20\u64cd\u4f5c. \u638c\u63e1Pytorch\u7684\u57fa\u672c\u8fd0\u7b97\u64cd\u4f5c. \u4ec0\u4e48\u662fPytorch Pytorch\u662f\u4e00\u4e2a\u57fa\u4e8eNumpy\u7684\u79d1\u5b66\u8ba1\u7b97\u5305, \u5411\u5b83\u7684\u4f7f\u7528\u8005\u63d0\u4f9b\u4e86\u4e24\u5927\u529f\u80fd. \u4f5c\u4e3aNumpy\u7684\u66ff\u4ee3\u8005, \u5411\u7528\u6237\u63d0\u4f9b\u4f7f\u7528GPU\u5f3a\u5927\u529f\u80fd\u7684\u80fd\u529b. \u505a\u4e3a\u4e00\u6b3e\u6df1\u5ea6\u5b66\u4e60\u7684\u5e73\u53f0, \u5411\u7528\u6237\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6. Pytorch\u7684\u57fa\u672c\u5143\u7d20\u64cd\u4f5c Tensors\u5f20\u91cf: \u5f20\u91cf\u7684\u6982\u5ff5\u7c7b\u4f3c\u4e8eNumpy\u4e2d\u7684ndarray\u6570\u636e\u7ed3\u6784, \u6700\u5927\u7684\u533a\u522b\u5728\u4e8eTensor\u53ef\u4ee5\u5229\u7528GPU\u7684\u52a0\u901f\u529f\u80fd. \u6211\u4eec\u4f7f\u7528Pytorch\u7684\u65f6\u5019, \u5e38\u89c4\u6b65\u9aa4\u662f\u5148\u5c06torch\u5f15\u7528\u8fdb\u6765, \u5982\u4e0b\u6240\u793a: from __future__ import print_function import torch \u521b\u5efa\u77e9\u9635\u7684\u64cd\u4f5c \u521b\u5efa\u4e00\u4e2a\u6ca1\u6709\u521d\u59cb\u5316\u7684\u77e9\u9635: x = torch . empty ( 5 , 3 ) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([[2.4835e+27, 2.5428e+30, 1.0877e-19], [1.5163e+23, 2.2012e+12, 3.7899e+22], [5.2480e+05, 1.0175e+31, 9.7056e+24], [1.6283e+32, 3.7913e+22, 3.9653e+28], [1.0876e-19, 6.2027e+26, 2.3685e+21]]) \u521b\u5efa\u4e00\u4e2a\u6709\u521d\u59cb\u5316\u7684\u77e9\u9635: x = torch . rand ( 5 , 3 ) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([[0.1368, 0.8070, 0.4567], [0.4369, 0.8278, 0.5552], [0.6848, 0.4473, 0.1031], [0.5308, 0.9194, 0.2761], [0.0484, 0.9941, 0.2227]]) \u5bf9\u6bd4\u6709\u65e0\u521d\u59cb\u5316\u7684\u77e9\u9635: \u5f53\u58f0\u660e\u4e00\u4e2a\u672a\u521d\u59cb\u5316\u7684\u77e9\u9635\u65f6, \u5b83\u672c\u8eab\u4e0d\u5305\u542b\u4efb\u4f55\u786e\u5207\u7684\u503c. \u5f53\u521b\u5efa\u4e00\u4e2a\u672a\u521d\u59cb\u5316\u7684\u77e9\u9635\u65f6, \u5206\u914d\u7ed9\u77e9\u9635\u7684\u5185\u5b58\u4e2d\u6709\u4ec0\u4e48\u6570\u503c\u5c31\u8d4b\u503c\u7ed9\u4e86\u8fd9\u4e2a\u77e9\u9635, \u672c\u8d28\u4e0a\u662f\u6beb\u65e0\u610f\u4e49\u7684\u6570\u636e. \u521b\u5efa\u4e00\u4e2a\u5168\u96f6\u77e9\u9635\u5e76\u53ef\u6307\u5b9a\u6570\u636e\u5143\u7d20\u7684\u7c7b\u578b\u4e3along x = torch . zeros ( 5 , 3 , dtype = torch . long ) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) \u76f4\u63a5\u901a\u8fc7\u6570\u636e\u521b\u5efa\u5f20\u91cf x = torch . tensor ([ 2.5 , 3.5 ]) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([2.5000, 3.3000]) \u901a\u8fc7\u5df2\u6709\u7684\u4e00\u4e2a\u5f20\u91cf\u521b\u5efa\u76f8\u540c\u5c3a\u5bf8\u7684\u65b0\u5f20\u91cf # \u5229\u7528news_methods\u65b9\u6cd5\u5f97\u5230\u4e00\u4e2a\u5f20\u91cf x = x . new_ones ( 5 , 3 , dtype = torch . double ) print ( x ) # \u5229\u7528randn_like\u65b9\u6cd5\u5f97\u5230\u76f8\u540c\u5f20\u91cf\u5c3a\u5bf8\u7684\u4e00\u4e2a\u65b0\u5f20\u91cf, \u5e76\u4e14\u91c7\u7528\u968f\u673a\u521d\u59cb\u5316\u6765\u5bf9\u5176\u8d4b\u503c y = torch . randn_like ( x , dtype = torch . float ) print ( y ) \u8f93\u51fa\u7ed3\u679c: tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[-0.1497, -0.5832, -0.3805], [ 0.9001, 2.0637, 1.3299], [-0.8813, -0.6579, -0.9135], [-0.1374, 0.1000, -0.9343], [-1.1278, -0.9140, -1.5910]]) \u5f97\u5230\u5f20\u91cf\u7684\u5c3a\u5bf8: print ( x . size ()) \u8f93\u51fa\u7ed3\u679c: torch.Size([5, 3]) \u6ce8\u610f: torch.Size\u51fd\u6570\u672c\u8d28\u4e0a\u8fd4\u56de\u7684\u662f\u4e00\u4e2atuple, \u56e0\u6b64\u5b83\u652f\u6301\u4e00\u5207\u5143\u7ec4\u7684\u64cd\u4f5c. Pytorch\u7684\u57fa\u672c\u8fd0\u7b97\u64cd\u4f5c \u52a0\u6cd5\u64cd\u4f5c: y = torch . rand ( 5 , 3 ) print ( x + y ) \u8f93\u51fa\u7ed3\u679c: tensor([[ 1.6978, -1.6979, 0.3093], [ 0.4953, 0.3954, 0.0595], [-0.9540, 0.3353, 0.1251], [ 0.6883, 0.9775, 1.1764], [ 2.6784, 0.1209, 1.5542]]) \u7b2c\u4e8c\u79cd\u52a0\u6cd5\u65b9\u5f0f: print ( torch . add ( x , y )) \u8f93\u51fa\u7ed3\u679c: tensor([[ 1.6978, -1.6979, 0.3093], [ 0.4953, 0.3954, 0.0595], [-0.9540, 0.3353, 0.1251], [ 0.6883, 0.9775, 1.1764], [ 2.6784, 0.1209, 1.5542]]) \u7b2c\u4e09\u79cd\u52a0\u6cd5\u65b9\u5f0f: # \u63d0\u524d\u8bbe\u5b9a\u4e00\u4e2a\u7a7a\u7684\u5f20\u91cf result = torch . empty ( 5 , 3 ) # \u5c06\u7a7a\u7684\u5f20\u91cf\u4f5c\u4e3a\u52a0\u6cd5\u7684\u7ed3\u679c\u5b58\u50a8\u5f20\u91cf torch . add ( x , y , out = result ) print ( result ) \u8f93\u51fa\u7ed3\u679c: tensor([[ 1.6978, -1.6979, 0.3093], [ 0.4953, 0.3954, 0.0595], [-0.9540, 0.3353, 0.1251], [ 0.6883, 0.9775, 1.1764], [ 2.6784, 0.1209, 1.5542]]) \u7b2c\u56db\u79cd\u52a0\u6cd5\u65b9\u5f0f: in-place (\u539f\u5730\u7f6e\u6362) y . add_ ( x ) print ( y ) \u8f93\u51fa\u7ed3\u679c: tensor([[ 1.6978, -1.6979, 0.3093], [ 0.4953, 0.3954, 0.0595], [-0.9540, 0.3353, 0.1251], [ 0.6883, 0.9775, 1.1764], [ 2.6784, 0.1209, 1.5542]]) \u6ce8\u610f: \u6240\u6709in-place\u7684\u64cd\u4f5c\u51fd\u6570\u90fd\u6709\u4e00\u4e2a\u4e0b\u5212\u7ebf\u7684\u540e\u7f00. \u6bd4\u5982x.copy_(y), x.add_(y), \u90fd\u4f1a\u76f4\u63a5\u6539\u53d8x\u7684\u503c. \u7528\u7c7b\u4f3c\u4e8eNumpy\u7684\u65b9\u5f0f\u5bf9\u5f20\u91cf\u8fdb\u884c\u64cd\u4f5c: print ( x [:, 1 ]) \u8f93\u51fa\u7ed3\u679c: tensor([-2.0902, -0.4489, -0.1441, 0.8035, -0.8341]) \u6539\u53d8\u5f20\u91cf\u7684\u5f62\u72b6: torch.view() x = torch . randn ( 4 , 4 ) # tensor.view()\u64cd\u4f5c\u9700\u8981\u4fdd\u8bc1\u6570\u636e\u5143\u7d20\u7684\u603b\u6570\u91cf\u4e0d\u53d8 y = x . view ( 16 ) # -1\u4ee3\u8868\u81ea\u52a8\u5339\u914d\u4e2a\u6570 z = x . view ( - 1 , 8 ) print ( x . size (), y . size (), z . size ()) \u8f93\u51fa\u7ed3\u679c: torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) \u5982\u679c\u5f20\u91cf\u4e2d\u53ea\u6709\u4e00\u4e2a\u5143\u7d20, \u53ef\u4ee5\u7528.item()\u5c06\u503c\u53d6\u51fa, \u4f5c\u4e3a\u4e00\u4e2apython number x = torch . randn ( 1 ) print ( x ) print ( x . item ()) \u8f93\u51fa\u7ed3\u679c: tensor([-0.3531]) -0.3530771732330322 \u5173\u4e8eTorch Tensor\u548cNumpy array\u4e4b\u95f4\u7684\u76f8\u4e92\u8f6c\u6362 Torch Tensor\u548cNumpy array\u5171\u4eab\u5e95\u5c42\u7684\u5185\u5b58\u7a7a\u95f4, \u56e0\u6b64\u6539\u53d8\u5176\u4e2d\u4e00\u4e2a\u7684\u503c, \u53e6\u4e00\u4e2a\u4e5f\u4f1a\u968f\u4e4b\u88ab\u6539\u53d8. a = torch . ones ( 5 ) print ( a ) \u8f93\u51fa\u7ed3\u679c: tensor([1., 1., 1., 1., 1.]) \u5c06Torch Tensor\u8f6c\u6362\u4e3aNumpy array b = a . numpy () print ( b ) \u8f93\u51fa\u7ed3\u679c: [1. 1. 1. 1. 1.] \u5bf9\u5176\u4e2d\u4e00\u4e2a\u8fdb\u884c\u52a0\u6cd5\u64cd\u4f5c, \u53e6\u4e00\u4e2a\u4e5f\u968f\u4e4b\u88ab\u6539\u53d8: a . add_ ( 1 ) print ( a ) print ( b ) \u8f93\u51fa\u7ed3\u679c: tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.] \u5c06Numpy array\u8f6c\u6362\u4e3aTorch Tensor: import numpy as np a = np . ones ( 5 ) b = torch . from_numpy ( a ) np . add ( a , 1 , out = a ) print ( a ) print ( b ) \u8f93\u51fa\u7ed3\u679c: [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64) \u6ce8\u610f: \u6240\u6709\u5728CPU\u4e0a\u7684Tensors, \u9664\u4e86CharTensor, \u90fd\u53ef\u4ee5\u8f6c\u6362\u4e3aNumpy array\u5e76\u53ef\u4ee5\u53cd\u5411\u8f6c\u6362. \u5173\u4e8eCuda Tensor: Tensors\u53ef\u4ee5\u7528.to()\u65b9\u6cd5\u6765\u5c06\u5176\u79fb\u52a8\u5230\u4efb\u610f\u8bbe\u5907\u4e0a. # \u5982\u679c\u670d\u52a1\u5668\u4e0a\u5df2\u7ecf\u5b89\u88c5\u4e86GPU\u548cCUDA if torch . cuda . is_available (): # \u5b9a\u4e49\u4e00\u4e2a\u8bbe\u5907\u5bf9\u8c61, \u8fd9\u91cc\u6307\u5b9a\u6210CUDA, \u5373\u4f7f\u7528GPU device = torch . device ( \"cuda\" ) # \u76f4\u63a5\u5728GPU\u4e0a\u521b\u5efa\u4e00\u4e2aTensor y = torch . ones_like ( x , device = device ) # \u5c06\u5728CPU\u4e0a\u9762\u7684x\u5f20\u91cf\u79fb\u52a8\u5230GPU\u4e0a\u9762 x = x . to ( device ) # x\u548cy\u90fd\u5728GPU\u4e0a\u9762, \u624d\u80fd\u652f\u6301\u52a0\u6cd5\u8fd0\u7b97 z = x + y # \u6b64\u5904\u7684\u5f20\u91cfz\u5728GPU\u4e0a\u9762 print ( z ) # \u4e5f\u53ef\u4ee5\u5c06z\u8f6c\u79fb\u5230CPU\u4e0a\u9762, \u5e76\u540c\u65f6\u6307\u5b9a\u5f20\u91cf\u5143\u7d20\u7684\u6570\u636e\u7c7b\u578b print ( z . to ( \"cpu\" , torch . double )) \u8f93\u51fa\u7ed3\u679c: tensor([0.6469], device='cuda:0') tensor([0.6469], dtype=torch.float64) \u5c0f\u8282\u603b\u7ed3 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fPytorch. Pytorch\u662f\u4e00\u4e2a\u57fa\u4e8eNumpy\u7684\u79d1\u5b66\u8ba1\u7b97\u5305, \u4f5c\u4e3aNumpy\u7684\u66ff\u4ee3\u8005, \u5411\u7528\u6237\u63d0\u4f9b\u4f7f\u7528GPU\u5f3a\u5927\u529f\u80fd\u7684\u80fd\u529b. \u505a\u4e3a\u4e00\u6b3e\u6df1\u5ea6\u5b66\u4e60\u7684\u5e73\u53f0, \u5411\u7528\u6237\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6. \u5b66\u4e60\u4e86Pytorch\u7684\u57fa\u672c\u5143\u7d20\u64cd\u4f5c. \u77e9\u9635\u7684\u521d\u59cb\u5316: torch.empty() torch.rand(n, m) torch.zeros(n, m, dtype=torch.long) \u5176\u4ed6\u82e5\u5e72\u64cd\u4f5c: x.new_ones(n, m, dtype=torch.double) torch.randn_like(x, dtype=torch.float) x.size() \u5b66\u4e60\u4e86Pytorch\u7684\u57fa\u672c\u8fd0\u7b97\u64cd\u4f5c. \u52a0\u6cd5\u64cd\u4f5c: x + y torch.add(x, y) torch.add(x, y, out=result) y.add_(x) \u5176\u4ed6\u82e5\u5e72\u64cd\u4f5c: x.view() x.item() \u5b66\u4e60\u4e86Torch Tensor\u548cNumpy Array\u4e4b\u95f4\u7684\u76f8\u4e92\u8f6c\u6362. \u5c06Torch Tensor\u8f6c\u6362\u4e3aNumpy Array: b = a.numpy() \u5c06Numpy Array\u8f6c\u6362\u4e3aTorch Tensor: b = torch.from_numpy(a) \u6ce8\u610f: \u6240\u6709\u624dCPU\u4e0a\u7684Tensor, \u9664\u4e86CharTensor, \u90fd\u53ef\u4ee5\u8f6c\u6362\u4e3aNumpy Array\u5e76\u53ef\u4ee5\u53cd\u5411\u8f6c\u6362. \u5b66\u4e60\u4e86\u4efb\u610f\u7684Tensors\u53ef\u4ee5\u7528.to()\u65b9\u6cd5\u6765\u5c06\u5176\u79fb\u52a8\u5230\u4efb\u610f\u8bbe\u5907\u4e0a. x = x.to(device) 1.2 Pytorch\u4e2d\u7684autograd \u5b66\u4e60\u76ee\u6807 \u638c\u63e1\u81ea\u52a8\u6c42\u5bfc\u4e2d\u7684Tensor\u6982\u5ff5\u548c\u64cd\u4f5c. \u638c\u63e1\u81ea\u52a8\u6c42\u5bfc\u4e2d\u7684\u68af\u5ea6Gradients\u6982\u5ff5\u548c\u64cd\u4f5c. \u5728\u6574\u4e2aPytorch\u6846\u67b6\u4e2d, \u6240\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u672c\u8d28\u4e0a\u90fd\u662f\u4e00\u4e2aautograd package(\u81ea\u52a8\u6c42\u5bfc\u5de5\u5177\u5305) autograd package\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5bf9Tensors\u4e0a\u6240\u6709\u7684\u64cd\u4f5c\u8fdb\u884c\u81ea\u52a8\u5fae\u5206\u7684\u529f\u80fd. \u5173\u4e8etorch.Tensor torch.Tensor\u662f\u6574\u4e2apackage\u4e2d\u7684\u6838\u5fc3\u7c7b, \u5982\u679c\u5c06\u5c5e\u6027.requires_grad\u8bbe\u7f6e\u4e3aTrue, \u5b83\u5c06\u8ffd\u8e2a\u5728\u8fd9\u4e2a\u7c7b\u4e0a\u5b9a\u4e49\u7684\u6240\u6709\u64cd\u4f5c. \u5f53\u4ee3\u7801\u8981\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019, \u76f4\u63a5\u8c03\u7528.backward()\u5c31\u53ef\u4ee5\u81ea\u52a8\u8ba1\u7b97\u6240\u6709\u7684\u68af\u5ea6. \u5728\u8fd9\u4e2aTensor\u4e0a\u7684\u6240\u6709\u68af\u5ea6\u5c06\u88ab\u7d2f\u52a0\u8fdb\u5c5e\u6027.grad\u4e2d. \u5982\u679c\u60f3\u7ec8\u6b62\u4e00\u4e2aTensor\u5728\u8ba1\u7b97\u56fe\u4e2d\u7684\u8ffd\u8e2a\u56de\u6eaf, \u53ea\u9700\u8981\u6267\u884c.detach()\u5c31\u53ef\u4ee5\u5c06\u8be5Tensor\u4ece\u8ba1\u7b97\u56fe\u4e2d\u64a4\u4e0b, \u5728\u672a\u6765\u7684\u56de\u6eaf\u8ba1\u7b97\u4e2d\u4e5f\u4e0d\u4f1a\u518d\u8ba1\u7b97\u8be5Tensor. \u9664\u4e86.detach(), \u5982\u679c\u60f3\u7ec8\u6b62\u5bf9\u8ba1\u7b97\u56fe\u7684\u56de\u6eaf, \u4e5f\u5c31\u662f\u4e0d\u518d\u8fdb\u884c\u65b9\u5411\u4f20\u64ad\u6c42\u5bfc\u6570\u7684\u8fc7\u7a0b, \u4e5f\u53ef\u4ee5\u91c7\u7528\u4ee3\u7801\u5757\u7684\u65b9\u5f0fwith torch.no_grad():, \u8fd9\u79cd\u65b9\u5f0f\u975e\u5e38\u9002\u7528\u4e8e\u5bf9\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u7684\u65f6\u5019, \u56e0\u4e3a\u9884\u6d4b\u9636\u6bb5\u4e0d\u518d\u9700\u8981\u5bf9\u68af\u5ea6\u8fdb\u884c\u8ba1\u7b97. \u5173\u4e8etorch.Function: Function\u7c7b\u662f\u548cTensor\u7c7b\u540c\u7b49\u91cd\u8981\u7684\u4e00\u4e2a\u6838\u5fc3\u7c7b, \u5b83\u548cTensor\u5171\u540c\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u7c7b, \u6bcf\u4e00\u4e2aTensor\u62e5\u6709\u4e00\u4e2a.grad_fn\u5c5e\u6027, \u4ee3\u8868\u5f15\u7528\u4e86\u54ea\u4e2a\u5177\u4f53\u7684Function\u521b\u5efa\u4e86\u8be5Tensor. \u5982\u679c\u67d0\u4e2a\u5f20\u91cfTensor\u662f\u7528\u6237\u81ea\u5b9a\u4e49\u7684, \u5219\u5176\u5bf9\u5e94\u7684grad_fn is None. \u5173\u4e8eTensor\u7684\u64cd\u4f5c x1 = torch . ones ( 3 , 3 ) print ( x1 ) x = torch . ones ( 2 , 2 , requires_grad = True ) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) tensor([[1., 1.], [1., 1.]], requires_grad=True) \u5728\u5177\u6709requires_grad=True\u7684Tensor\u4e0a\u6267\u884c\u4e00\u4e2a\u52a0\u6cd5\u64cd\u4f5c y = x + 2 print ( y ) \u8f93\u51fa\u7ed3\u679c: tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward0>) \u6253\u5370Tensor\u7684grad_fn\u5c5e\u6027: print ( x . grad_fn ) print ( y . grad_fn ) \u8f93\u51fa\u7ed3\u679c: None <AddBackward0 object at 0x10db11208> \u5728Tensor\u4e0a\u6267\u884c\u66f4\u590d\u6742\u7684\u64cd\u4f5c: z = y * y * 3 out = z . mean () print ( z , out ) \u8f93\u51fa\u7ed3\u679c: tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>) \u5173\u4e8e\u65b9\u6cd5.requires_grad_(): \u8be5\u65b9\u6cd5\u53ef\u4ee5\u539f\u5730\u6539\u53d8Tensor\u7684\u5c5e\u6027.requires_grad\u7684\u503c. \u5982\u679c\u6ca1\u6709\u4e3b\u52a8\u8bbe\u5b9a\u9ed8\u8ba4\u4e3aFalse. a = torch . randn ( 2 , 2 ) a = (( a * 3 ) / ( a - 1 )) print ( a . requires_grad ) a . requires_grad_ ( True ) print ( a . requires_grad ) b = ( a * a ) . sum () print ( b . grad_fn ) \u8f93\u51fa\u7ed3\u679c: False True <SumBackward0 object at 0x7f191afd6be0> \u5173\u4e8e\u68af\u5ea6Gradients \u5728Pytorch\u4e2d, \u53cd\u5411\u4f20\u64ad\u662f\u4f9d\u9760.backward()\u5b9e\u73b0\u7684. out . backward () print ( x . grad ) \u8f93\u51fa\u7ed3\u679c: tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) \u5173\u4e8e\u81ea\u52a8\u6c42\u5bfc\u7684\u5c5e\u6027\u8bbe\u7f6e: \u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e.requires_grad=True\u6765\u6267\u884c\u81ea\u52a8\u6c42\u5bfc, \u4e5f\u53ef\u4ee5\u901a\u8fc7\u4ee3\u7801\u5757\u7684\u9650\u5236\u6765\u505c\u6b62\u81ea\u52a8\u6c42\u5bfc. print ( x . requires_grad ) print (( x ** 2 ) . requires_grad ) with torch . no_grad (): print (( x ** 2 ) . requires_grad ) \u8f93\u51fa\u7ed3\u679c: True True False \u53ef\u4ee5\u901a\u8fc7.detach()\u83b7\u5f97\u4e00\u4e2a\u65b0\u7684Tensor, \u62e5\u6709\u76f8\u540c\u7684\u5185\u5bb9\u4f46\u4e0d\u9700\u8981\u81ea\u52a8\u6c42\u5bfc. print ( x . requires_grad ) y = x . detach () print ( y . requires_grad ) print ( x . eq ( y ) . all ()) \u8f93\u51fa\u7ed3\u679c: True False tensor(True) \u5c0f\u8282\u603b\u7ed3 \u5b66\u4e60\u4e86torch.Tensor\u7c7b\u7684\u76f8\u5173\u6982\u5ff5. torch.Tensor\u662f\u6574\u4e2apackage\u4e2d\u7684\u6838\u5fc3\u7c7b, \u5982\u679c\u5c06\u5c5e\u6027.requires_grad\u8bbe\u7f6e\u4e3aTrue, \u5b83\u5c06\u8ffd\u8e2a\u5728\u8fd9\u4e2a\u7c7b\u4e0a\u5b9a\u4e49\u7684\u6240\u6709\u64cd\u4f5c. \u5f53\u4ee3\u7801\u8981\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019, \u76f4\u63a5\u8c03\u7528.backward()\u5c31\u53ef\u4ee5\u81ea\u52a8\u8ba1\u7b97\u6240\u6709\u7684\u68af\u5ea6. \u5728\u8fd9\u4e2aTensor\u4e0a\u7684\u6240\u6709\u68af\u5ea6\u5c06\u88ab\u7d2f\u52a0\u8fdb\u5c5e\u6027.grad\u4e2d. \u6267\u884c.detach()\u547d\u4ee4, \u53ef\u4ee5\u5c06\u8be5Tensor\u4ece\u8ba1\u7b97\u56fe\u4e2d\u64a4\u4e0b, \u5728\u672a\u6765\u7684\u56de\u6eaf\u8ba1\u7b97\u4e2d\u4e0d\u4f1a\u518d\u8ba1\u7b97\u8be5Tensor. \u91c7\u7528\u4ee3\u7801\u5757\u7684\u65b9\u5f0f\u4e5f\u53ef\u4ee5\u7ec8\u6b62\u5bf9\u8ba1\u7b97\u56fe\u7684\u56de\u6eaf: with torch.no_grad(): \u5b66\u4e60\u4e86\u5173\u4e8eTensor\u7684\u82e5\u5e72\u64cd\u4f5c: torch.ones(n, n, requires_grad=True) x.grad_fn a.requires_grad_(True) \u5b66\u4e60\u4e86\u5173\u4e8eGradients\u7684\u5c5e\u6027: x.grad \u53ef\u4ee5\u901a\u8fc7.detach()\u83b7\u5f97\u4e00\u4e2a\u65b0\u7684Tensor, \u62e5\u6709\u76f8\u540c\u7684\u5185\u5bb9\u4f46\u4e0d\u9700\u8981\u81ea\u52a8\u6c42\u5bfc.","title":"1. Pytorch\u57fa\u672c\u8bed\u6cd5"},{"location":"1.html#11-pytorch","text":"","title":"1.1 \u8ba4\u8bc6Pytorch"},{"location":"1.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662fPytorch. \u638c\u63e1Pytorch\u7684\u57fa\u672c\u5143\u7d20\u64cd\u4f5c. \u638c\u63e1Pytorch\u7684\u57fa\u672c\u8fd0\u7b97\u64cd\u4f5c.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"1.html#pytorch","text":"Pytorch\u662f\u4e00\u4e2a\u57fa\u4e8eNumpy\u7684\u79d1\u5b66\u8ba1\u7b97\u5305, \u5411\u5b83\u7684\u4f7f\u7528\u8005\u63d0\u4f9b\u4e86\u4e24\u5927\u529f\u80fd. \u4f5c\u4e3aNumpy\u7684\u66ff\u4ee3\u8005, \u5411\u7528\u6237\u63d0\u4f9b\u4f7f\u7528GPU\u5f3a\u5927\u529f\u80fd\u7684\u80fd\u529b. \u505a\u4e3a\u4e00\u6b3e\u6df1\u5ea6\u5b66\u4e60\u7684\u5e73\u53f0, \u5411\u7528\u6237\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6.","title":"\u4ec0\u4e48\u662fPytorch"},{"location":"1.html#pytorch_1","text":"Tensors\u5f20\u91cf: \u5f20\u91cf\u7684\u6982\u5ff5\u7c7b\u4f3c\u4e8eNumpy\u4e2d\u7684ndarray\u6570\u636e\u7ed3\u6784, \u6700\u5927\u7684\u533a\u522b\u5728\u4e8eTensor\u53ef\u4ee5\u5229\u7528GPU\u7684\u52a0\u901f\u529f\u80fd. \u6211\u4eec\u4f7f\u7528Pytorch\u7684\u65f6\u5019, \u5e38\u89c4\u6b65\u9aa4\u662f\u5148\u5c06torch\u5f15\u7528\u8fdb\u6765, \u5982\u4e0b\u6240\u793a: from __future__ import print_function import torch \u521b\u5efa\u77e9\u9635\u7684\u64cd\u4f5c \u521b\u5efa\u4e00\u4e2a\u6ca1\u6709\u521d\u59cb\u5316\u7684\u77e9\u9635: x = torch . empty ( 5 , 3 ) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([[2.4835e+27, 2.5428e+30, 1.0877e-19], [1.5163e+23, 2.2012e+12, 3.7899e+22], [5.2480e+05, 1.0175e+31, 9.7056e+24], [1.6283e+32, 3.7913e+22, 3.9653e+28], [1.0876e-19, 6.2027e+26, 2.3685e+21]]) \u521b\u5efa\u4e00\u4e2a\u6709\u521d\u59cb\u5316\u7684\u77e9\u9635: x = torch . rand ( 5 , 3 ) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([[0.1368, 0.8070, 0.4567], [0.4369, 0.8278, 0.5552], [0.6848, 0.4473, 0.1031], [0.5308, 0.9194, 0.2761], [0.0484, 0.9941, 0.2227]]) \u5bf9\u6bd4\u6709\u65e0\u521d\u59cb\u5316\u7684\u77e9\u9635: \u5f53\u58f0\u660e\u4e00\u4e2a\u672a\u521d\u59cb\u5316\u7684\u77e9\u9635\u65f6, \u5b83\u672c\u8eab\u4e0d\u5305\u542b\u4efb\u4f55\u786e\u5207\u7684\u503c. \u5f53\u521b\u5efa\u4e00\u4e2a\u672a\u521d\u59cb\u5316\u7684\u77e9\u9635\u65f6, \u5206\u914d\u7ed9\u77e9\u9635\u7684\u5185\u5b58\u4e2d\u6709\u4ec0\u4e48\u6570\u503c\u5c31\u8d4b\u503c\u7ed9\u4e86\u8fd9\u4e2a\u77e9\u9635, \u672c\u8d28\u4e0a\u662f\u6beb\u65e0\u610f\u4e49\u7684\u6570\u636e. \u521b\u5efa\u4e00\u4e2a\u5168\u96f6\u77e9\u9635\u5e76\u53ef\u6307\u5b9a\u6570\u636e\u5143\u7d20\u7684\u7c7b\u578b\u4e3along x = torch . zeros ( 5 , 3 , dtype = torch . long ) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) \u76f4\u63a5\u901a\u8fc7\u6570\u636e\u521b\u5efa\u5f20\u91cf x = torch . tensor ([ 2.5 , 3.5 ]) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([2.5000, 3.3000]) \u901a\u8fc7\u5df2\u6709\u7684\u4e00\u4e2a\u5f20\u91cf\u521b\u5efa\u76f8\u540c\u5c3a\u5bf8\u7684\u65b0\u5f20\u91cf # \u5229\u7528news_methods\u65b9\u6cd5\u5f97\u5230\u4e00\u4e2a\u5f20\u91cf x = x . new_ones ( 5 , 3 , dtype = torch . double ) print ( x ) # \u5229\u7528randn_like\u65b9\u6cd5\u5f97\u5230\u76f8\u540c\u5f20\u91cf\u5c3a\u5bf8\u7684\u4e00\u4e2a\u65b0\u5f20\u91cf, \u5e76\u4e14\u91c7\u7528\u968f\u673a\u521d\u59cb\u5316\u6765\u5bf9\u5176\u8d4b\u503c y = torch . randn_like ( x , dtype = torch . float ) print ( y ) \u8f93\u51fa\u7ed3\u679c: tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[-0.1497, -0.5832, -0.3805], [ 0.9001, 2.0637, 1.3299], [-0.8813, -0.6579, -0.9135], [-0.1374, 0.1000, -0.9343], [-1.1278, -0.9140, -1.5910]]) \u5f97\u5230\u5f20\u91cf\u7684\u5c3a\u5bf8: print ( x . size ()) \u8f93\u51fa\u7ed3\u679c: torch.Size([5, 3]) \u6ce8\u610f: torch.Size\u51fd\u6570\u672c\u8d28\u4e0a\u8fd4\u56de\u7684\u662f\u4e00\u4e2atuple, \u56e0\u6b64\u5b83\u652f\u6301\u4e00\u5207\u5143\u7ec4\u7684\u64cd\u4f5c.","title":"Pytorch\u7684\u57fa\u672c\u5143\u7d20\u64cd\u4f5c"},{"location":"1.html#pytorch_2","text":"\u52a0\u6cd5\u64cd\u4f5c: y = torch . rand ( 5 , 3 ) print ( x + y ) \u8f93\u51fa\u7ed3\u679c: tensor([[ 1.6978, -1.6979, 0.3093], [ 0.4953, 0.3954, 0.0595], [-0.9540, 0.3353, 0.1251], [ 0.6883, 0.9775, 1.1764], [ 2.6784, 0.1209, 1.5542]]) \u7b2c\u4e8c\u79cd\u52a0\u6cd5\u65b9\u5f0f: print ( torch . add ( x , y )) \u8f93\u51fa\u7ed3\u679c: tensor([[ 1.6978, -1.6979, 0.3093], [ 0.4953, 0.3954, 0.0595], [-0.9540, 0.3353, 0.1251], [ 0.6883, 0.9775, 1.1764], [ 2.6784, 0.1209, 1.5542]]) \u7b2c\u4e09\u79cd\u52a0\u6cd5\u65b9\u5f0f: # \u63d0\u524d\u8bbe\u5b9a\u4e00\u4e2a\u7a7a\u7684\u5f20\u91cf result = torch . empty ( 5 , 3 ) # \u5c06\u7a7a\u7684\u5f20\u91cf\u4f5c\u4e3a\u52a0\u6cd5\u7684\u7ed3\u679c\u5b58\u50a8\u5f20\u91cf torch . add ( x , y , out = result ) print ( result ) \u8f93\u51fa\u7ed3\u679c: tensor([[ 1.6978, -1.6979, 0.3093], [ 0.4953, 0.3954, 0.0595], [-0.9540, 0.3353, 0.1251], [ 0.6883, 0.9775, 1.1764], [ 2.6784, 0.1209, 1.5542]]) \u7b2c\u56db\u79cd\u52a0\u6cd5\u65b9\u5f0f: in-place (\u539f\u5730\u7f6e\u6362) y . add_ ( x ) print ( y ) \u8f93\u51fa\u7ed3\u679c: tensor([[ 1.6978, -1.6979, 0.3093], [ 0.4953, 0.3954, 0.0595], [-0.9540, 0.3353, 0.1251], [ 0.6883, 0.9775, 1.1764], [ 2.6784, 0.1209, 1.5542]]) \u6ce8\u610f: \u6240\u6709in-place\u7684\u64cd\u4f5c\u51fd\u6570\u90fd\u6709\u4e00\u4e2a\u4e0b\u5212\u7ebf\u7684\u540e\u7f00. \u6bd4\u5982x.copy_(y), x.add_(y), \u90fd\u4f1a\u76f4\u63a5\u6539\u53d8x\u7684\u503c. \u7528\u7c7b\u4f3c\u4e8eNumpy\u7684\u65b9\u5f0f\u5bf9\u5f20\u91cf\u8fdb\u884c\u64cd\u4f5c: print ( x [:, 1 ]) \u8f93\u51fa\u7ed3\u679c: tensor([-2.0902, -0.4489, -0.1441, 0.8035, -0.8341]) \u6539\u53d8\u5f20\u91cf\u7684\u5f62\u72b6: torch.view() x = torch . randn ( 4 , 4 ) # tensor.view()\u64cd\u4f5c\u9700\u8981\u4fdd\u8bc1\u6570\u636e\u5143\u7d20\u7684\u603b\u6570\u91cf\u4e0d\u53d8 y = x . view ( 16 ) # -1\u4ee3\u8868\u81ea\u52a8\u5339\u914d\u4e2a\u6570 z = x . view ( - 1 , 8 ) print ( x . size (), y . size (), z . size ()) \u8f93\u51fa\u7ed3\u679c: torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) \u5982\u679c\u5f20\u91cf\u4e2d\u53ea\u6709\u4e00\u4e2a\u5143\u7d20, \u53ef\u4ee5\u7528.item()\u5c06\u503c\u53d6\u51fa, \u4f5c\u4e3a\u4e00\u4e2apython number x = torch . randn ( 1 ) print ( x ) print ( x . item ()) \u8f93\u51fa\u7ed3\u679c: tensor([-0.3531]) -0.3530771732330322","title":"Pytorch\u7684\u57fa\u672c\u8fd0\u7b97\u64cd\u4f5c"},{"location":"1.html#torch-tensornumpy-array","text":"Torch Tensor\u548cNumpy array\u5171\u4eab\u5e95\u5c42\u7684\u5185\u5b58\u7a7a\u95f4, \u56e0\u6b64\u6539\u53d8\u5176\u4e2d\u4e00\u4e2a\u7684\u503c, \u53e6\u4e00\u4e2a\u4e5f\u4f1a\u968f\u4e4b\u88ab\u6539\u53d8. a = torch . ones ( 5 ) print ( a ) \u8f93\u51fa\u7ed3\u679c: tensor([1., 1., 1., 1., 1.]) \u5c06Torch Tensor\u8f6c\u6362\u4e3aNumpy array b = a . numpy () print ( b ) \u8f93\u51fa\u7ed3\u679c: [1. 1. 1. 1. 1.] \u5bf9\u5176\u4e2d\u4e00\u4e2a\u8fdb\u884c\u52a0\u6cd5\u64cd\u4f5c, \u53e6\u4e00\u4e2a\u4e5f\u968f\u4e4b\u88ab\u6539\u53d8: a . add_ ( 1 ) print ( a ) print ( b ) \u8f93\u51fa\u7ed3\u679c: tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.] \u5c06Numpy array\u8f6c\u6362\u4e3aTorch Tensor: import numpy as np a = np . ones ( 5 ) b = torch . from_numpy ( a ) np . add ( a , 1 , out = a ) print ( a ) print ( b ) \u8f93\u51fa\u7ed3\u679c: [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64) \u6ce8\u610f: \u6240\u6709\u5728CPU\u4e0a\u7684Tensors, \u9664\u4e86CharTensor, \u90fd\u53ef\u4ee5\u8f6c\u6362\u4e3aNumpy array\u5e76\u53ef\u4ee5\u53cd\u5411\u8f6c\u6362. \u5173\u4e8eCuda Tensor: Tensors\u53ef\u4ee5\u7528.to()\u65b9\u6cd5\u6765\u5c06\u5176\u79fb\u52a8\u5230\u4efb\u610f\u8bbe\u5907\u4e0a. # \u5982\u679c\u670d\u52a1\u5668\u4e0a\u5df2\u7ecf\u5b89\u88c5\u4e86GPU\u548cCUDA if torch . cuda . is_available (): # \u5b9a\u4e49\u4e00\u4e2a\u8bbe\u5907\u5bf9\u8c61, \u8fd9\u91cc\u6307\u5b9a\u6210CUDA, \u5373\u4f7f\u7528GPU device = torch . device ( \"cuda\" ) # \u76f4\u63a5\u5728GPU\u4e0a\u521b\u5efa\u4e00\u4e2aTensor y = torch . ones_like ( x , device = device ) # \u5c06\u5728CPU\u4e0a\u9762\u7684x\u5f20\u91cf\u79fb\u52a8\u5230GPU\u4e0a\u9762 x = x . to ( device ) # x\u548cy\u90fd\u5728GPU\u4e0a\u9762, \u624d\u80fd\u652f\u6301\u52a0\u6cd5\u8fd0\u7b97 z = x + y # \u6b64\u5904\u7684\u5f20\u91cfz\u5728GPU\u4e0a\u9762 print ( z ) # \u4e5f\u53ef\u4ee5\u5c06z\u8f6c\u79fb\u5230CPU\u4e0a\u9762, \u5e76\u540c\u65f6\u6307\u5b9a\u5f20\u91cf\u5143\u7d20\u7684\u6570\u636e\u7c7b\u578b print ( z . to ( \"cpu\" , torch . double )) \u8f93\u51fa\u7ed3\u679c: tensor([0.6469], device='cuda:0') tensor([0.6469], dtype=torch.float64)","title":"\u5173\u4e8eTorch Tensor\u548cNumpy array\u4e4b\u95f4\u7684\u76f8\u4e92\u8f6c\u6362"},{"location":"1.html#_2","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662fPytorch. Pytorch\u662f\u4e00\u4e2a\u57fa\u4e8eNumpy\u7684\u79d1\u5b66\u8ba1\u7b97\u5305, \u4f5c\u4e3aNumpy\u7684\u66ff\u4ee3\u8005, \u5411\u7528\u6237\u63d0\u4f9b\u4f7f\u7528GPU\u5f3a\u5927\u529f\u80fd\u7684\u80fd\u529b. \u505a\u4e3a\u4e00\u6b3e\u6df1\u5ea6\u5b66\u4e60\u7684\u5e73\u53f0, \u5411\u7528\u6237\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6. \u5b66\u4e60\u4e86Pytorch\u7684\u57fa\u672c\u5143\u7d20\u64cd\u4f5c. \u77e9\u9635\u7684\u521d\u59cb\u5316: torch.empty() torch.rand(n, m) torch.zeros(n, m, dtype=torch.long) \u5176\u4ed6\u82e5\u5e72\u64cd\u4f5c: x.new_ones(n, m, dtype=torch.double) torch.randn_like(x, dtype=torch.float) x.size() \u5b66\u4e60\u4e86Pytorch\u7684\u57fa\u672c\u8fd0\u7b97\u64cd\u4f5c. \u52a0\u6cd5\u64cd\u4f5c: x + y torch.add(x, y) torch.add(x, y, out=result) y.add_(x) \u5176\u4ed6\u82e5\u5e72\u64cd\u4f5c: x.view() x.item() \u5b66\u4e60\u4e86Torch Tensor\u548cNumpy Array\u4e4b\u95f4\u7684\u76f8\u4e92\u8f6c\u6362. \u5c06Torch Tensor\u8f6c\u6362\u4e3aNumpy Array: b = a.numpy() \u5c06Numpy Array\u8f6c\u6362\u4e3aTorch Tensor: b = torch.from_numpy(a) \u6ce8\u610f: \u6240\u6709\u624dCPU\u4e0a\u7684Tensor, \u9664\u4e86CharTensor, \u90fd\u53ef\u4ee5\u8f6c\u6362\u4e3aNumpy Array\u5e76\u53ef\u4ee5\u53cd\u5411\u8f6c\u6362. \u5b66\u4e60\u4e86\u4efb\u610f\u7684Tensors\u53ef\u4ee5\u7528.to()\u65b9\u6cd5\u6765\u5c06\u5176\u79fb\u52a8\u5230\u4efb\u610f\u8bbe\u5907\u4e0a. x = x.to(device)","title":"\u5c0f\u8282\u603b\u7ed3"},{"location":"1.html#12-pytorchautograd","text":"","title":"1.2 Pytorch\u4e2d\u7684autograd"},{"location":"1.html#_3","text":"\u638c\u63e1\u81ea\u52a8\u6c42\u5bfc\u4e2d\u7684Tensor\u6982\u5ff5\u548c\u64cd\u4f5c. \u638c\u63e1\u81ea\u52a8\u6c42\u5bfc\u4e2d\u7684\u68af\u5ea6Gradients\u6982\u5ff5\u548c\u64cd\u4f5c. \u5728\u6574\u4e2aPytorch\u6846\u67b6\u4e2d, \u6240\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u672c\u8d28\u4e0a\u90fd\u662f\u4e00\u4e2aautograd package(\u81ea\u52a8\u6c42\u5bfc\u5de5\u5177\u5305) autograd package\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5bf9Tensors\u4e0a\u6240\u6709\u7684\u64cd\u4f5c\u8fdb\u884c\u81ea\u52a8\u5fae\u5206\u7684\u529f\u80fd.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"1.html#torchtensor","text":"torch.Tensor\u662f\u6574\u4e2apackage\u4e2d\u7684\u6838\u5fc3\u7c7b, \u5982\u679c\u5c06\u5c5e\u6027.requires_grad\u8bbe\u7f6e\u4e3aTrue, \u5b83\u5c06\u8ffd\u8e2a\u5728\u8fd9\u4e2a\u7c7b\u4e0a\u5b9a\u4e49\u7684\u6240\u6709\u64cd\u4f5c. \u5f53\u4ee3\u7801\u8981\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019, \u76f4\u63a5\u8c03\u7528.backward()\u5c31\u53ef\u4ee5\u81ea\u52a8\u8ba1\u7b97\u6240\u6709\u7684\u68af\u5ea6. \u5728\u8fd9\u4e2aTensor\u4e0a\u7684\u6240\u6709\u68af\u5ea6\u5c06\u88ab\u7d2f\u52a0\u8fdb\u5c5e\u6027.grad\u4e2d. \u5982\u679c\u60f3\u7ec8\u6b62\u4e00\u4e2aTensor\u5728\u8ba1\u7b97\u56fe\u4e2d\u7684\u8ffd\u8e2a\u56de\u6eaf, \u53ea\u9700\u8981\u6267\u884c.detach()\u5c31\u53ef\u4ee5\u5c06\u8be5Tensor\u4ece\u8ba1\u7b97\u56fe\u4e2d\u64a4\u4e0b, \u5728\u672a\u6765\u7684\u56de\u6eaf\u8ba1\u7b97\u4e2d\u4e5f\u4e0d\u4f1a\u518d\u8ba1\u7b97\u8be5Tensor. \u9664\u4e86.detach(), \u5982\u679c\u60f3\u7ec8\u6b62\u5bf9\u8ba1\u7b97\u56fe\u7684\u56de\u6eaf, \u4e5f\u5c31\u662f\u4e0d\u518d\u8fdb\u884c\u65b9\u5411\u4f20\u64ad\u6c42\u5bfc\u6570\u7684\u8fc7\u7a0b, \u4e5f\u53ef\u4ee5\u91c7\u7528\u4ee3\u7801\u5757\u7684\u65b9\u5f0fwith torch.no_grad():, \u8fd9\u79cd\u65b9\u5f0f\u975e\u5e38\u9002\u7528\u4e8e\u5bf9\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u7684\u65f6\u5019, \u56e0\u4e3a\u9884\u6d4b\u9636\u6bb5\u4e0d\u518d\u9700\u8981\u5bf9\u68af\u5ea6\u8fdb\u884c\u8ba1\u7b97. \u5173\u4e8etorch.Function: Function\u7c7b\u662f\u548cTensor\u7c7b\u540c\u7b49\u91cd\u8981\u7684\u4e00\u4e2a\u6838\u5fc3\u7c7b, \u5b83\u548cTensor\u5171\u540c\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u7c7b, \u6bcf\u4e00\u4e2aTensor\u62e5\u6709\u4e00\u4e2a.grad_fn\u5c5e\u6027, \u4ee3\u8868\u5f15\u7528\u4e86\u54ea\u4e2a\u5177\u4f53\u7684Function\u521b\u5efa\u4e86\u8be5Tensor. \u5982\u679c\u67d0\u4e2a\u5f20\u91cfTensor\u662f\u7528\u6237\u81ea\u5b9a\u4e49\u7684, \u5219\u5176\u5bf9\u5e94\u7684grad_fn is None.","title":"\u5173\u4e8etorch.Tensor"},{"location":"1.html#tensor","text":"x1 = torch . ones ( 3 , 3 ) print ( x1 ) x = torch . ones ( 2 , 2 , requires_grad = True ) print ( x ) \u8f93\u51fa\u7ed3\u679c: tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) tensor([[1., 1.], [1., 1.]], requires_grad=True) \u5728\u5177\u6709requires_grad=True\u7684Tensor\u4e0a\u6267\u884c\u4e00\u4e2a\u52a0\u6cd5\u64cd\u4f5c y = x + 2 print ( y ) \u8f93\u51fa\u7ed3\u679c: tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward0>) \u6253\u5370Tensor\u7684grad_fn\u5c5e\u6027: print ( x . grad_fn ) print ( y . grad_fn ) \u8f93\u51fa\u7ed3\u679c: None <AddBackward0 object at 0x10db11208> \u5728Tensor\u4e0a\u6267\u884c\u66f4\u590d\u6742\u7684\u64cd\u4f5c: z = y * y * 3 out = z . mean () print ( z , out ) \u8f93\u51fa\u7ed3\u679c: tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>) \u5173\u4e8e\u65b9\u6cd5.requires_grad_(): \u8be5\u65b9\u6cd5\u53ef\u4ee5\u539f\u5730\u6539\u53d8Tensor\u7684\u5c5e\u6027.requires_grad\u7684\u503c. \u5982\u679c\u6ca1\u6709\u4e3b\u52a8\u8bbe\u5b9a\u9ed8\u8ba4\u4e3aFalse. a = torch . randn ( 2 , 2 ) a = (( a * 3 ) / ( a - 1 )) print ( a . requires_grad ) a . requires_grad_ ( True ) print ( a . requires_grad ) b = ( a * a ) . sum () print ( b . grad_fn ) \u8f93\u51fa\u7ed3\u679c: False True <SumBackward0 object at 0x7f191afd6be0>","title":"\u5173\u4e8eTensor\u7684\u64cd\u4f5c"},{"location":"1.html#gradients","text":"\u5728Pytorch\u4e2d, \u53cd\u5411\u4f20\u64ad\u662f\u4f9d\u9760.backward()\u5b9e\u73b0\u7684. out . backward () print ( x . grad ) \u8f93\u51fa\u7ed3\u679c: tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) \u5173\u4e8e\u81ea\u52a8\u6c42\u5bfc\u7684\u5c5e\u6027\u8bbe\u7f6e: \u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e.requires_grad=True\u6765\u6267\u884c\u81ea\u52a8\u6c42\u5bfc, \u4e5f\u53ef\u4ee5\u901a\u8fc7\u4ee3\u7801\u5757\u7684\u9650\u5236\u6765\u505c\u6b62\u81ea\u52a8\u6c42\u5bfc. print ( x . requires_grad ) print (( x ** 2 ) . requires_grad ) with torch . no_grad (): print (( x ** 2 ) . requires_grad ) \u8f93\u51fa\u7ed3\u679c: True True False \u53ef\u4ee5\u901a\u8fc7.detach()\u83b7\u5f97\u4e00\u4e2a\u65b0\u7684Tensor, \u62e5\u6709\u76f8\u540c\u7684\u5185\u5bb9\u4f46\u4e0d\u9700\u8981\u81ea\u52a8\u6c42\u5bfc. print ( x . requires_grad ) y = x . detach () print ( y . requires_grad ) print ( x . eq ( y ) . all ()) \u8f93\u51fa\u7ed3\u679c: True False tensor(True)","title":"\u5173\u4e8e\u68af\u5ea6Gradients"},{"location":"1.html#_4","text":"\u5b66\u4e60\u4e86torch.Tensor\u7c7b\u7684\u76f8\u5173\u6982\u5ff5. torch.Tensor\u662f\u6574\u4e2apackage\u4e2d\u7684\u6838\u5fc3\u7c7b, \u5982\u679c\u5c06\u5c5e\u6027.requires_grad\u8bbe\u7f6e\u4e3aTrue, \u5b83\u5c06\u8ffd\u8e2a\u5728\u8fd9\u4e2a\u7c7b\u4e0a\u5b9a\u4e49\u7684\u6240\u6709\u64cd\u4f5c. \u5f53\u4ee3\u7801\u8981\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019, \u76f4\u63a5\u8c03\u7528.backward()\u5c31\u53ef\u4ee5\u81ea\u52a8\u8ba1\u7b97\u6240\u6709\u7684\u68af\u5ea6. \u5728\u8fd9\u4e2aTensor\u4e0a\u7684\u6240\u6709\u68af\u5ea6\u5c06\u88ab\u7d2f\u52a0\u8fdb\u5c5e\u6027.grad\u4e2d. \u6267\u884c.detach()\u547d\u4ee4, \u53ef\u4ee5\u5c06\u8be5Tensor\u4ece\u8ba1\u7b97\u56fe\u4e2d\u64a4\u4e0b, \u5728\u672a\u6765\u7684\u56de\u6eaf\u8ba1\u7b97\u4e2d\u4e0d\u4f1a\u518d\u8ba1\u7b97\u8be5Tensor. \u91c7\u7528\u4ee3\u7801\u5757\u7684\u65b9\u5f0f\u4e5f\u53ef\u4ee5\u7ec8\u6b62\u5bf9\u8ba1\u7b97\u56fe\u7684\u56de\u6eaf: with torch.no_grad(): \u5b66\u4e60\u4e86\u5173\u4e8eTensor\u7684\u82e5\u5e72\u64cd\u4f5c: torch.ones(n, n, requires_grad=True) x.grad_fn a.requires_grad_(True) \u5b66\u4e60\u4e86\u5173\u4e8eGradients\u7684\u5c5e\u6027: x.grad \u53ef\u4ee5\u901a\u8fc7.detach()\u83b7\u5f97\u4e00\u4e2a\u65b0\u7684Tensor, \u62e5\u6709\u76f8\u540c\u7684\u5185\u5bb9\u4f46\u4e0d\u9700\u8981\u81ea\u52a8\u6c42\u5bfc.","title":"\u5c0f\u8282\u603b\u7ed3"},{"location":"2.html","text":"2.1 \u4f7f\u7528Pytorch\u6784\u5efa\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc \u5b66\u4e60\u76ee\u6807 \u638c\u63e1\u7528Pytorch\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u672c\u6d41\u7a0b. \u638c\u63e1\u7528Pytorch\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u73b0\u8fc7\u7a0b. \u5173\u4e8etorch.nn: \u4f7f\u7528Pytorch\u6765\u6784\u5efa\u795e\u7ecf\u7f51\u7edc, \u4e3b\u8981\u7684\u5de5\u5177\u90fd\u5728torch.nn\u5305\u4e2d. nn\u4f9d\u8d56\u4e8eautograd\u6765\u5b9a\u4e49\u6a21\u578b, \u5e76\u5bf9\u5176\u81ea\u52a8\u6c42\u5bfc. \u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u6d41\u7a0b: \u5b9a\u4e49\u4e00\u4e2a\u62e5\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc \u904d\u5386\u8bad\u7ec3\u6570\u636e\u96c6 \u5904\u7406\u8f93\u5165\u6570\u636e\u4f7f\u5176\u6d41\u7ecf\u795e\u7ecf\u7f51\u7edc \u8ba1\u7b97\u635f\u5931\u503c \u5c06\u7f51\u7edc\u53c2\u6570\u7684\u68af\u5ea6\u8fdb\u884c\u53cd\u5411\u4f20\u64ad \u4ee5\u4e00\u5b9a\u7684\u89c4\u5219\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd \u6211\u4eec\u9996\u5148\u5b9a\u4e49\u4e00\u4e2aPytorch\u5b9e\u73b0\u7684\u795e\u7ecf\u7f51\u7edc: # \u5bfc\u5165\u82e5\u5e72\u5de5\u5177\u5305 import torch import torch.nn as nn import torch.nn.functional as F # \u5b9a\u4e49\u4e00\u4e2a\u7b80\u5355\u7684\u7f51\u7edc\u7c7b class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # \u5b9a\u4e49\u7b2c\u4e00\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u8f93\u5165\u901a\u9053\u7ef4\u5ea6=1, \u8f93\u51fa\u901a\u9053\u7ef4\u5ea6=6, \u5377\u79ef\u6838\u5927\u5c0f3*3 self . conv1 = nn . Conv2d ( 1 , 6 , 3 ) # \u5b9a\u4e49\u7b2c\u4e8c\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u8f93\u5165\u901a\u9053\u7ef4\u5ea6=6, \u8f93\u51fa\u901a\u9053\u7ef4\u5ea6=16, \u5377\u79ef\u6838\u5927\u5c0f3*3 self . conv2 = nn . Conv2d ( 6 , 16 , 3 ) # \u5b9a\u4e49\u4e09\u5c42\u5168\u8fde\u63a5\u7f51\u7edc self . fc1 = nn . Linear ( 16 * 6 * 6 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): # \u5728(2, 2)\u7684\u6c60\u5316\u7a97\u53e3\u4e0b\u6267\u884c\u6700\u5927\u6c60\u5316\u64cd\u4f5c x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) x = F . max_pool2d ( F . relu ( self . conv2 ( x )), 2 ) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): # \u8ba1\u7b97size, \u9664\u4e86\u7b2c0\u4e2a\u7ef4\u5ea6\u4e0a\u7684batch_size size = x . size ()[ 1 :] num_features = 1 for s in size : num_features *= s return num_features net = Net () print ( net ) \u8f93\u51fa\u7ed3\u679c: Net( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=576, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) \u6ce8\u610f: \u6a21\u578b\u4e2d\u6240\u6709\u7684\u53ef\u8bad\u7ec3\u53c2\u6570, \u53ef\u4ee5\u901a\u8fc7net.parameters()\u6765\u83b7\u5f97. params = list ( net . parameters ()) print ( len ( params )) print ( params [ 0 ] . size ()) \u8f93\u51fa\u7ed3\u679c: 10 torch.Size([6, 1, 3, 3]) \u5047\u8bbe\u56fe\u50cf\u7684\u8f93\u5165\u5c3a\u5bf8\u4e3a32 * 32: input = torch . randn ( 1 , 1 , 32 , 32 ) out = net ( input ) print ( out ) \u8f93\u51fa\u7ed3\u679c: tensor([[ 0.1242, 0.1194, -0.0584, -0.1140, 0.0661, 0.0191, -0.0966, 0.0480, 0.0775, -0.0451]], grad_fn=<AddmmBackward>) \u6709\u4e86\u8f93\u51fa\u5f20\u91cf\u540e, \u5c31\u53ef\u4ee5\u6267\u884c\u68af\u5ea6\u5f52\u96f6\u548c\u53cd\u5411\u4f20\u64ad\u7684\u64cd\u4f5c\u4e86. net . zero_grad () out . backward ( torch . randn ( 1 , 10 )) \u6ce8\u610f: torch.nn\u6784\u5efa\u7684\u795e\u7ecf\u7f51\u7edc\u53ea\u652f\u6301mini-batches\u7684\u8f93\u5165, \u4e0d\u652f\u6301\u5355\u4e00\u6837\u672c\u7684\u8f93\u5165. \u6bd4\u5982: nn.Conv2d \u9700\u8981\u4e00\u4e2a4D Tensor, \u5f62\u72b6\u4e3a(nSamples, nChannels, Height, Width). \u5982\u679c\u4f60\u7684\u8f93\u5165\u53ea\u6709\u5355\u4e00\u6837\u672c\u5f62\u5f0f, \u5219\u9700\u8981\u6267\u884cinput.unsqueeze(0), \u4e3b\u52a8\u5c063D Tensor\u6269\u5145\u62104D Tensor. \u635f\u5931\u51fd\u6570 \u635f\u5931\u51fd\u6570\u7684\u8f93\u5165\u662f\u4e00\u4e2a\u8f93\u5165\u7684pair: (output, target), \u7136\u540e\u8ba1\u7b97\u51fa\u4e00\u4e2a\u6570\u503c\u6765\u8bc4\u4f30output\u548ctarget\u4e4b\u95f4\u7684\u5dee\u8ddd\u5927\u5c0f. \u5728torch.nn\u4e2d\u6709\u82e5\u5e72\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\u53ef\u4f9b\u4f7f\u7528, \u6bd4\u5982nn.MSELoss\u5c31\u662f\u901a\u8fc7\u8ba1\u7b97\u5747\u65b9\u5dee\u635f\u5931\u6765\u8bc4\u4f30\u8f93\u5165\u548c\u76ee\u6807\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd. \u5e94\u7528nn.MSELoss\u8ba1\u7b97\u635f\u5931\u7684\u4e00\u4e2a\u4f8b\u5b50: output = net ( input ) target = torch . randn ( 10 ) # \u6539\u53d8target\u7684\u5f62\u72b6\u4e3a\u4e8c\u7ef4\u5f20\u91cf, \u4e3a\u4e86\u548coutput\u5339\u914d target = target . view ( 1 , - 1 ) criterion = nn . MSELoss () loss = criterion ( output , target ) print ( loss ) \u8f93\u51fa\u7ed3\u679c: tensor(1.1562, grad_fn=<MseLossBackward>) \u5173\u4e8e\u65b9\u5411\u4f20\u64ad\u7684\u94fe\u6761: \u5982\u679c\u6211\u4eec\u8ddf\u8e2aloss\u53cd\u5411\u4f20\u64ad\u7684\u65b9\u5411, \u4f7f\u7528.grad_fn\u5c5e\u6027\u6253\u5370, \u5c06\u53ef\u4ee5\u770b\u5230\u4e00\u5f20\u5b8c\u6574\u7684\u8ba1\u7b97\u56fe\u5982\u4e0b: input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss \u5f53\u8c03\u7528loss.backward()\u65f6, \u6574\u5f20\u8ba1\u7b97\u56fe\u5c06\u5bf9loss\u8fdb\u884c\u81ea\u52a8\u6c42\u5bfc, \u6240\u6709\u5c5e\u6027requires_grad=True\u7684Tensors\u90fd\u5c06\u53c2\u4e0e\u68af\u5ea6\u6c42\u5bfc\u7684\u8fd0\u7b97, \u5e76\u5c06\u68af\u5ea6\u7d2f\u52a0\u5230Tensors\u4e2d\u7684.grad\u5c5e\u6027\u4e2d. print ( loss . grad_fn ) # MSELoss print ( loss . grad_fn . next_functions [ 0 ][ 0 ]) # Linear print ( loss . grad_fn . next_functions [ 0 ][ 0 ] . next_functions [ 0 ][ 0 ]) # ReLU \u8f93\u51fa\u7ed3\u679c: <MseLossBackward object at 0x7fdba3216da0> <AddmmBackward object at 0x7fdba3216f28> <AccumulateGrad object at 0x7fdba3216f28> \u53cd\u5411\u4f20\u64ad(backpropagation) \u5728Pytorch\u4e2d\u6267\u884c\u53cd\u5411\u4f20\u64ad\u975e\u5e38\u7b80\u4fbf, \u5168\u90e8\u7684\u64cd\u4f5c\u5c31\u662floss.backward(). \u5728\u6267\u884c\u53cd\u5411\u4f20\u64ad\u4e4b\u524d, \u8981\u5148\u5c06\u68af\u5ea6\u6e05\u96f6, \u5426\u5219\u68af\u5ea6\u4f1a\u5728\u4e0d\u540c\u7684\u6279\u6b21\u6570\u636e\u4e4b\u95f4\u88ab\u7d2f\u52a0. \u6267\u884c\u4e00\u4e2a\u53cd\u5411\u4f20\u64ad\u7684\u5c0f\u4f8b\u5b50: # Pytorch\u4e2d\u6267\u884c\u68af\u5ea6\u6e05\u96f6\u7684\u4ee3\u7801 net . zero_grad () print ( 'conv1.bias.grad before backward' ) print ( net . conv1 . bias . grad ) # Pytorch\u4e2d\u6267\u884c\u53cd\u5411\u4f20\u64ad\u7684\u4ee3\u7801 loss . backward () print ( 'conv1.bias.grad after backward' ) print ( net . conv1 . bias . grad ) \u8f93\u51fa\u7ed3\u679c: conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([-0.0002, 0.0045, 0.0017, -0.0099, 0.0092, -0.0044]) \u66f4\u65b0\u7f51\u7edc\u53c2\u6570 \u66f4\u65b0\u53c2\u6570\u6700\u7b80\u5355\u7684\u7b97\u6cd5\u5c31\u662fSGD(\u968f\u673a\u68af\u5ea6\u4e0b\u964d). \u5177\u4f53\u7684\u7b97\u6cd5\u516c\u5f0f\u8868\u8fbe\u5f0f\u4e3a: weight = weight - learning_rate * gradient \u9996\u5148\u7528\u4f20\u7edf\u7684Python\u4ee3\u7801\u6765\u5b9e\u73b0SGD\u5982\u4e0b: learning_rate = 0.01 for f in net . parameters (): f . data . sub_ ( f . grad . data * learning_rate ) \u7136\u540e\u4f7f\u7528Pytorch\u5b98\u65b9\u63a8\u8350\u7684\u6807\u51c6\u4ee3\u7801\u5982\u4e0b: # \u9996\u5148\u5bfc\u5165\u4f18\u5316\u5668\u7684\u5305, optim\u4e2d\u5305\u542b\u82e5\u5e72\u5e38\u7528\u7684\u4f18\u5316\u7b97\u6cd5, \u6bd4\u5982SGD, Adam\u7b49 import torch.optim as optim # \u901a\u8fc7optim\u521b\u5efa\u4f18\u5316\u5668\u5bf9\u8c61 optimizer = optim . SGD ( net . parameters (), lr = 0.01 ) # \u5c06\u4f18\u5316\u5668\u6267\u884c\u68af\u5ea6\u6e05\u96f6\u7684\u64cd\u4f5c optimizer . zero_grad () output = net ( input ) loss = criterion ( output , target ) # \u5bf9\u635f\u5931\u503c\u6267\u884c\u53cd\u5411\u4f20\u64ad\u7684\u64cd\u4f5c loss . backward () # \u53c2\u6570\u7684\u66f4\u65b0\u901a\u8fc7\u4e00\u884c\u6807\u51c6\u4ee3\u7801\u6765\u6267\u884c optimizer . step () \u5c0f\u8282\u603b\u7ed3 \u5b66\u4e60\u4e86\u6784\u5efa\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u6d41\u7a0b: \u5b9a\u4e49\u4e00\u4e2a\u62e5\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc \u904d\u5386\u8bad\u7ec3\u6570\u636e\u96c6 \u5904\u7406\u8f93\u5165\u6570\u636e\u4f7f\u5176\u6d41\u7ecf\u795e\u7ecf\u7f51\u7edc \u8ba1\u7b97\u635f\u5931\u503c \u5c06\u7f51\u7edc\u53c2\u6570\u7684\u68af\u5ea6\u8fdb\u884c\u53cd\u5411\u4f20\u64ad \u4ee5\u4e00\u5b9a\u7684\u89c4\u5219\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd \u5b66\u4e60\u4e86\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49: \u91c7\u7528torch.nn.MSELoss()\u8ba1\u7b97\u5747\u65b9\u8bef\u5dee. \u901a\u8fc7loss.backward()\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u65f6, \u6574\u5f20\u8ba1\u7b97\u56fe\u5c06\u5bf9loss\u8fdb\u884c\u81ea\u52a8\u6c42\u5bfc, \u6240\u6709\u5c5e\u6027requires_grad=True\u7684Tensors\u90fd\u5c06\u53c2\u4e0e\u68af\u5ea6\u6c42\u5bfc\u7684\u8fd0\u7b97, \u5e76\u5c06\u68af\u5ea6\u7d2f\u52a0\u5230Tensors\u4e2d\u7684.grad\u5c5e\u6027\u4e2d. \u5b66\u4e60\u4e86\u53cd\u5411\u4f20\u64ad\u7684\u8ba1\u7b97\u65b9\u6cd5: \u5728Pytorch\u4e2d\u6267\u884c\u53cd\u5411\u4f20\u64ad\u975e\u5e38\u7b80\u4fbf, \u5168\u90e8\u7684\u64cd\u4f5c\u5c31\u662floss.backward(). \u5728\u6267\u884c\u53cd\u5411\u4f20\u64ad\u4e4b\u524d, \u8981\u5148\u5c06\u68af\u5ea6\u6e05\u96f6, \u5426\u5219\u68af\u5ea6\u4f1a\u5728\u4e0d\u540c\u7684\u6279\u6b21\u6570\u636e\u4e4b\u95f4\u88ab\u7d2f\u52a0. net.zero_grad() loss.backward() \u5b66\u4e60\u4e86\u53c2\u6570\u7684\u66f4\u65b0\u65b9\u6cd5: \u5b9a\u4e49\u4f18\u5316\u5668\u6765\u6267\u884c\u53c2\u6570\u7684\u4f18\u5316\u4e0e\u66f4\u65b0. optimizer = optim.SGD(net.parameters(), lr=0.01) \u901a\u8fc7\u4f18\u5316\u5668\u6765\u6267\u884c\u5177\u4f53\u7684\u53c2\u6570\u66f4\u65b0. optimizer.step() 2.2 \u4f7f\u7528Pytorch\u6784\u5efa\u4e00\u4e2a\u5206\u7c7b\u5668 \u5b66\u4e60\u76ee\u6807 \u4e86\u89e3\u5206\u7c7b\u5668\u7684\u4efb\u52a1\u548c\u6570\u636e\u6837\u5f0f \u638c\u63e1\u5982\u4f55\u7528Pytorch\u5b9e\u73b0\u4e00\u4e2a\u5206\u7c7b\u5668 \u5206\u7c7b\u5668\u4efb\u52a1\u548c\u6570\u636e\u4ecb\u7ecd \u6784\u9020\u4e00\u4e2a\u5c06\u4e0d\u540c\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668, \u5bf9\u8f93\u5165\u7684\u56fe\u7247\u8fdb\u884c\u5224\u522b\u5e76\u5b8c\u6210\u5206\u7c7b. \u672c\u6848\u4f8b\u91c7\u7528CIFAR10\u6570\u636e\u96c6\u4f5c\u4e3a\u539f\u59cb\u56fe\u7247\u6570\u636e. CIFAR10\u6570\u636e\u96c6\u4ecb\u7ecd: \u6570\u636e\u96c6\u4e2d\u6bcf\u5f20\u56fe\u7247\u7684\u5c3a\u5bf8\u662f3 * 32 * 32, \u4ee3\u8868\u5f69\u82723\u901a\u9053 CIFAR10\u6570\u636e\u96c6\u603b\u5171\u670910\u79cd\u4e0d\u540c\u7684\u5206\u7c7b, \u5206\u522b\u662f\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\". CIFAR10\u6570\u636e\u96c6\u7684\u6837\u4f8b\u5982\u4e0b\u56fe\u6240\u793a: \u8bad\u7ec3\u5206\u7c7b\u5668\u7684\u6b65\u9aa4 1: \u4f7f\u7528torchvision\u4e0b\u8f7dCIFAR10\u6570\u636e\u96c6 2: \u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc 3: \u5b9a\u4e49\u635f\u5931\u51fd\u6570 4: \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b 5: \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u6a21\u578b 1: \u4f7f\u7528torchvision\u4e0b\u8f7dCIFAR10\u6570\u636e\u96c6 \u5bfc\u5165torchvision\u5305\u6765\u8f85\u52a9\u4e0b\u8f7d\u6570\u636e\u96c6 import torch import torchvision import torchvision.transforms as transforms \u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u5bf9\u56fe\u7247\u8fdb\u884c\u8c03\u6574, \u56e0\u4e3atorchvision\u6570\u636e\u96c6\u7684\u8f93\u51fa\u662fPILImage\u683c\u5f0f, \u6570\u636e\u57df\u5728[0, 1]. \u6211\u4eec\u5c06\u5176\u8f6c\u6362\u4e3a\u6807\u51c6\u6570\u636e\u57df[-1, 1]\u7684\u5f20\u91cf\u683c\u5f0f. transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) \u8f93\u51fa\u7ed3\u679c: Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified \u6ce8\u610f: \u5982\u679c\u4f60\u662f\u5728Windows\u7cfb\u7edf\u4e0b\u8fd0\u884c\u4e0a\u8ff0\u4ee3\u7801, \u5e76\u4e14\u51fa\u73b0\u62a5\u9519\u4fe1\u606f \"BrokenPipeError\", \u53ef\u4ee5\u5c1d\u8bd5\u5c06torch.utils.data.DataLoader()\u4e2d\u7684num_workers\u8bbe\u7f6e\u4e3a0. \u5c55\u793a\u82e5\u5e72\u8bad\u7ec3\u96c6\u7684\u56fe\u7247 # \u5bfc\u5165\u753b\u56fe\u5305\u548cnumpy import matplotlib.pyplot as plt import numpy as np # \u6784\u5efa\u5c55\u793a\u56fe\u7247\u7684\u51fd\u6570 def imshow ( img ): img = img / 2 + 0.5 npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) plt . show () # \u4ece\u6570\u636e\u8fed\u4ee3\u5668\u4e2d\u8bfb\u53d6\u4e00\u5f20\u56fe\u7247 dataiter = iter ( trainloader ) images , labels = dataiter . next () # \u5c55\u793a\u56fe\u7247 imshow ( torchvision . utils . make_grid ( images )) # \u6253\u5370\u6807\u7b7elabel print ( ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) \u8f93\u51fa\u56fe\u7247\u7ed3\u679c: \u8f93\u51fa\u6807\u7b7e\u7ed3\u679c: bird truck cat cat 2: \u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u4eff\u71672.1\u8282\u4e2d\u7684\u7c7b\u6765\u6784\u9020\u6b64\u5904\u7684\u7c7b, \u552f\u4e00\u7684\u533a\u522b\u662f\u6b64\u5904\u91c7\u75283\u901a\u90533-channel import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () 3: \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u91c7\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668. import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 ) 4: \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b \u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u5316\u7b97\u6cd5, \u90fd\u9700\u8981\u5f88\u591a\u4e2a\u8f6e\u6b21\u7684\u8fed\u4ee3\u8bad\u7ec3. for epoch in range ( 2 ): # loop over the dataset multiple times running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # data\u4e2d\u5305\u542b\u8f93\u5165\u56fe\u50cf\u5f20\u91cfinputs, \u6807\u7b7e\u5f20\u91cflabels inputs , labels = data # \u9996\u5148\u5c06\u4f18\u5316\u5668\u68af\u5ea6\u5f52\u96f6 optimizer . zero_grad () # \u8f93\u5165\u56fe\u50cf\u5f20\u91cf\u8fdb\u7f51\u7edc, \u5f97\u5230\u8f93\u51fa\u5f20\u91cfoutputs outputs = net ( inputs ) # \u5229\u7528\u7f51\u7edc\u7684\u8f93\u51faoutputs\u548c\u6807\u7b7elabels\u8ba1\u7b97\u635f\u5931\u503c loss = criterion ( outputs , labels ) # \u53cd\u5411\u4f20\u64ad+\u53c2\u6570\u66f4\u65b0, \u662f\u6807\u51c6\u4ee3\u7801\u7684\u6807\u51c6\u6d41\u7a0b loss . backward () optimizer . step () # \u6253\u5370\u8f6e\u6b21\u548c\u635f\u5931\u503c running_loss += loss . item () if ( i + 1 ) % 2000 == 0 : print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Finished Training' ) \u8f93\u51fa\u7ed3\u679c: [1, 2000] loss: 2.227 [1, 4000] loss: 1.884 [1, 6000] loss: 1.672 [1, 8000] loss: 1.582 [1, 10000] loss: 1.526 [1, 12000] loss: 1.474 [2, 2000] loss: 1.407 [2, 4000] loss: 1.384 [2, 6000] loss: 1.362 [2, 8000] loss: 1.341 [2, 10000] loss: 1.331 [2, 12000] loss: 1.291 Finished Training \u4fdd\u5b58\u6a21\u578b: # \u9996\u5148\u8bbe\u5b9a\u6a21\u578b\u7684\u4fdd\u5b58\u8def\u5f84 PATH = './cifar_net.pth' # \u4fdd\u5b58\u6a21\u578b\u7684\u72b6\u6001\u5b57\u5178 torch . save ( net . state_dict (), PATH ) 5: \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u6a21\u578b \u7b2c\u4e00\u6b65, \u5c55\u793a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u82e5\u5e72\u56fe\u7247 dataiter = iter ( testloader ) images , labels = dataiter . next () # \u6253\u5370\u539f\u59cb\u56fe\u7247 imshow ( torchvision . utils . make_grid ( images )) # \u6253\u5370\u771f\u5b9e\u7684\u6807\u7b7e print ( 'GroundTruth: ' , ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) \u8f93\u51fa\u56fe\u7247\u7ed3\u679c: \u8f93\u51fa\u6807\u7b7e\u7ed3\u679c: GroundTruth: cat ship ship plane \u7b2c\u4e8c\u6b65, \u52a0\u8f7d\u6a21\u578b\u5e76\u5bf9\u6d4b\u8bd5\u56fe\u7247\u8fdb\u884c\u9884\u6d4b # \u9996\u5148\u5b9e\u4f8b\u5316\u6a21\u578b\u7684\u7c7b\u5bf9\u8c61 net = Net () # \u52a0\u8f7d\u8bad\u7ec3\u9636\u6bb5\u4fdd\u5b58\u597d\u7684\u6a21\u578b\u7684\u72b6\u6001\u5b57\u5178 net . load_state_dict ( torch . load ( PATH )) # \u5229\u7528\u6a21\u578b\u5bf9\u56fe\u7247\u8fdb\u884c\u9884\u6d4b outputs = net ( images ) # \u5171\u670910\u4e2a\u7c7b\u522b, \u91c7\u7528\u6a21\u578b\u8ba1\u7b97\u51fa\u7684\u6982\u7387\u6700\u5927\u7684\u4f5c\u4e3a\u9884\u6d4b\u7684\u7c7b\u522b _ , predicted = torch . max ( outputs , 1 ) # \u6253\u5370\u9884\u6d4b\u6807\u7b7e\u7684\u7ed3\u679c print ( 'Predicted: ' , ' ' . join ( ' %5s ' % classes [ predicted [ j ]] for j in range ( 4 ))) \u8f93\u51fa\u7ed3\u679c: Predicted: cat ship ship plane \u63a5\u4e0b\u6765\u770b\u4e00\u4e0b\u5728\u5168\u90e8\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0 correct = 0 total = 0 with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) \u8f93\u51fa\u7ed3\u679c: Accuracy of the network on the 10000 test images: 53 % \u5206\u6790\u7ed3\u679c: \u5bf9\u4e8e\u62e5\u670910\u4e2a\u7c7b\u522b\u7684\u6570\u636e\u96c6, \u968f\u673a\u731c\u6d4b\u7684\u51c6\u786e\u7387\u662f10%, \u6a21\u578b\u8fbe\u5230\u4e8653%, \u8bf4\u660e\u6a21\u578b\u5b66\u5230\u4e86\u771f\u5b9e\u7684\u4e1c\u897f. \u4e3a\u4e86\u66f4\u52a0\u7ec6\u81f4\u7684\u770b\u4e00\u4e0b\u6a21\u578b\u5728\u54ea\u4e9b\u7c7b\u522b\u4e0a\u8868\u73b0\u66f4\u597d, \u5728\u54ea\u4e9b\u7c7b\u522b\u4e0a\u8868\u73b0\u66f4\u5dee, \u6211\u4eec\u5206\u7c7b\u522b\u7684\u8fdb\u884c\u51c6\u786e\u7387\u8ba1\u7b97. class_correct = list ( 0. for i in range ( 10 )) class_total = list ( 0. for i in range ( 10 )) with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 4 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( 10 ): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) \u8f93\u51fa\u7ed3\u679c: Accuracy of plane : 62 % Accuracy of car : 62 % Accuracy of bird : 45 % Accuracy of cat : 36 % Accuracy of deer : 52 % Accuracy of dog : 25 % Accuracy of frog : 69 % Accuracy of horse : 60 % Accuracy of ship : 70 % Accuracy of truck : 48 % \u5728GPU\u4e0a\u8bad\u7ec3\u6a21\u578b \u4e3a\u4e86\u771f\u6b63\u5229\u7528Pytorch\u4e2dTensor\u7684\u4f18\u79c0\u5c5e\u6027, \u52a0\u901f\u6a21\u578b\u7684\u8bad\u7ec3, \u6211\u4eec\u53ef\u4ee5\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u8f6c\u79fb\u5230GPU\u4e0a\u8fdb\u884c. \u9996\u5148\u8981\u5b9a\u4e49\u8bbe\u5907, \u5982\u679cCUDA\u662f\u53ef\u7528\u7684\u5219\u88ab\u5b9a\u4e49\u6210GPU, \u5426\u5219\u88ab\u5b9a\u4e49\u6210CPU. device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( device ) \u8f93\u51fa\u7ed3\u679c: cuda:0 \u5f53\u8bad\u7ec3\u6a21\u578b\u7684\u65f6\u5019, \u53ea\u9700\u8981\u5c06\u6a21\u578b\u8f6c\u79fb\u5230GPU\u4e0a, \u540c\u65f6\u5c06\u8f93\u5165\u7684\u56fe\u7247\u548c\u6807\u7b7e\u9875\u8f6c\u79fb\u5230GPU\u4e0a\u5373\u53ef. # \u5c06\u6a21\u578b\u8f6c\u79fb\u5230GPU\u4e0a net . to ( device ) # \u5c06\u8f93\u5165\u7684\u56fe\u7247\u5f20\u91cf\u548c\u6807\u7b7e\u5f20\u91cf\u8f6c\u79fb\u5230GPU\u4e0a inputs , labels = data [ 0 ] . to ( device ), data [ 1 ] . to ( device ) \u5c0f\u8282\u603b\u7ed3 \u5b66\u4e60\u4e86\u5206\u7c7b\u5668\u7684\u4efb\u52a1\u548c\u6570\u636e\u6837\u5f0f. \u5c06\u4e0d\u540c\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668, \u5bf9\u8f93\u5165\u7684\u56fe\u7247\u8fdb\u884c\u5224\u522b\u5e76\u5b8c\u6210\u5206\u7c7b. \u91c7\u7528CIFAR10\u6570\u636e\u96c6\u4f5c\u4e3a\u539f\u59cb\u56fe\u7247\u6570\u636e, CIFAR10\u6570\u636e\u96c6\u62e5\u670910\u4e2a\u7c7b\u522b\u76843 * 32 * 32\u5f69\u8272\u56fe\u7247. \u5b66\u4e60\u4e86\u8bad\u7ec3\u5206\u7c7b\u5668\u7684\u6b65\u9aa4: \u4f7f\u7528torchvision\u4e0b\u8f7dCIFAR10\u6570\u636e\u96c6. \u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc. \u5b9a\u4e49\u635f\u5931\u51fd\u6570. \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b. \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u6a21\u578b. \u5b66\u4e60\u4e86\u5728GPU\u4e0a\u8bad\u7ec3\u6a21\u578b. \u9996\u5148\u9700\u8981\u5b9a\u4e49\u8bbe\u5907, CPU\u548cGPU\u4e8c\u9009\u4e00: device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \u7136\u540e\u5c06\u6a21\u578b\u8f6c\u79fb\u5230GPU\u4e0a\u53bb: net.to(device) \u6700\u540e\u5728\u8fed\u4ee3\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d, \u6bcf\u4e00\u6b65\u90fd\u5c06\u56fe\u7247\u548c\u6807\u7b7e\u5f20\u91cf\u8f6c\u79fb\u5230GPU\u4e0a\u53bb: inputs, labels = data[0].to(device), data[1].to(device)","title":"2. Pytorch\u521d\u6b65\u5e94\u7528"},{"location":"2.html#21-pytorch","text":"","title":"2.1 \u4f7f\u7528Pytorch\u6784\u5efa\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc"},{"location":"2.html#_1","text":"\u638c\u63e1\u7528Pytorch\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u672c\u6d41\u7a0b. \u638c\u63e1\u7528Pytorch\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u73b0\u8fc7\u7a0b. \u5173\u4e8etorch.nn: \u4f7f\u7528Pytorch\u6765\u6784\u5efa\u795e\u7ecf\u7f51\u7edc, \u4e3b\u8981\u7684\u5de5\u5177\u90fd\u5728torch.nn\u5305\u4e2d. nn\u4f9d\u8d56\u4e8eautograd\u6765\u5b9a\u4e49\u6a21\u578b, \u5e76\u5bf9\u5176\u81ea\u52a8\u6c42\u5bfc. \u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u6d41\u7a0b: \u5b9a\u4e49\u4e00\u4e2a\u62e5\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc \u904d\u5386\u8bad\u7ec3\u6570\u636e\u96c6 \u5904\u7406\u8f93\u5165\u6570\u636e\u4f7f\u5176\u6d41\u7ecf\u795e\u7ecf\u7f51\u7edc \u8ba1\u7b97\u635f\u5931\u503c \u5c06\u7f51\u7edc\u53c2\u6570\u7684\u68af\u5ea6\u8fdb\u884c\u53cd\u5411\u4f20\u64ad \u4ee5\u4e00\u5b9a\u7684\u89c4\u5219\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd \u6211\u4eec\u9996\u5148\u5b9a\u4e49\u4e00\u4e2aPytorch\u5b9e\u73b0\u7684\u795e\u7ecf\u7f51\u7edc: # \u5bfc\u5165\u82e5\u5e72\u5de5\u5177\u5305 import torch import torch.nn as nn import torch.nn.functional as F # \u5b9a\u4e49\u4e00\u4e2a\u7b80\u5355\u7684\u7f51\u7edc\u7c7b class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # \u5b9a\u4e49\u7b2c\u4e00\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u8f93\u5165\u901a\u9053\u7ef4\u5ea6=1, \u8f93\u51fa\u901a\u9053\u7ef4\u5ea6=6, \u5377\u79ef\u6838\u5927\u5c0f3*3 self . conv1 = nn . Conv2d ( 1 , 6 , 3 ) # \u5b9a\u4e49\u7b2c\u4e8c\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u8f93\u5165\u901a\u9053\u7ef4\u5ea6=6, \u8f93\u51fa\u901a\u9053\u7ef4\u5ea6=16, \u5377\u79ef\u6838\u5927\u5c0f3*3 self . conv2 = nn . Conv2d ( 6 , 16 , 3 ) # \u5b9a\u4e49\u4e09\u5c42\u5168\u8fde\u63a5\u7f51\u7edc self . fc1 = nn . Linear ( 16 * 6 * 6 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): # \u5728(2, 2)\u7684\u6c60\u5316\u7a97\u53e3\u4e0b\u6267\u884c\u6700\u5927\u6c60\u5316\u64cd\u4f5c x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) x = F . max_pool2d ( F . relu ( self . conv2 ( x )), 2 ) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): # \u8ba1\u7b97size, \u9664\u4e86\u7b2c0\u4e2a\u7ef4\u5ea6\u4e0a\u7684batch_size size = x . size ()[ 1 :] num_features = 1 for s in size : num_features *= s return num_features net = Net () print ( net ) \u8f93\u51fa\u7ed3\u679c: Net( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=576, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) \u6ce8\u610f: \u6a21\u578b\u4e2d\u6240\u6709\u7684\u53ef\u8bad\u7ec3\u53c2\u6570, \u53ef\u4ee5\u901a\u8fc7net.parameters()\u6765\u83b7\u5f97. params = list ( net . parameters ()) print ( len ( params )) print ( params [ 0 ] . size ()) \u8f93\u51fa\u7ed3\u679c: 10 torch.Size([6, 1, 3, 3]) \u5047\u8bbe\u56fe\u50cf\u7684\u8f93\u5165\u5c3a\u5bf8\u4e3a32 * 32: input = torch . randn ( 1 , 1 , 32 , 32 ) out = net ( input ) print ( out ) \u8f93\u51fa\u7ed3\u679c: tensor([[ 0.1242, 0.1194, -0.0584, -0.1140, 0.0661, 0.0191, -0.0966, 0.0480, 0.0775, -0.0451]], grad_fn=<AddmmBackward>) \u6709\u4e86\u8f93\u51fa\u5f20\u91cf\u540e, \u5c31\u53ef\u4ee5\u6267\u884c\u68af\u5ea6\u5f52\u96f6\u548c\u53cd\u5411\u4f20\u64ad\u7684\u64cd\u4f5c\u4e86. net . zero_grad () out . backward ( torch . randn ( 1 , 10 )) \u6ce8\u610f: torch.nn\u6784\u5efa\u7684\u795e\u7ecf\u7f51\u7edc\u53ea\u652f\u6301mini-batches\u7684\u8f93\u5165, \u4e0d\u652f\u6301\u5355\u4e00\u6837\u672c\u7684\u8f93\u5165. \u6bd4\u5982: nn.Conv2d \u9700\u8981\u4e00\u4e2a4D Tensor, \u5f62\u72b6\u4e3a(nSamples, nChannels, Height, Width). \u5982\u679c\u4f60\u7684\u8f93\u5165\u53ea\u6709\u5355\u4e00\u6837\u672c\u5f62\u5f0f, \u5219\u9700\u8981\u6267\u884cinput.unsqueeze(0), \u4e3b\u52a8\u5c063D Tensor\u6269\u5145\u62104D Tensor.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"2.html#_2","text":"\u635f\u5931\u51fd\u6570\u7684\u8f93\u5165\u662f\u4e00\u4e2a\u8f93\u5165\u7684pair: (output, target), \u7136\u540e\u8ba1\u7b97\u51fa\u4e00\u4e2a\u6570\u503c\u6765\u8bc4\u4f30output\u548ctarget\u4e4b\u95f4\u7684\u5dee\u8ddd\u5927\u5c0f. \u5728torch.nn\u4e2d\u6709\u82e5\u5e72\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\u53ef\u4f9b\u4f7f\u7528, \u6bd4\u5982nn.MSELoss\u5c31\u662f\u901a\u8fc7\u8ba1\u7b97\u5747\u65b9\u5dee\u635f\u5931\u6765\u8bc4\u4f30\u8f93\u5165\u548c\u76ee\u6807\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd. \u5e94\u7528nn.MSELoss\u8ba1\u7b97\u635f\u5931\u7684\u4e00\u4e2a\u4f8b\u5b50: output = net ( input ) target = torch . randn ( 10 ) # \u6539\u53d8target\u7684\u5f62\u72b6\u4e3a\u4e8c\u7ef4\u5f20\u91cf, \u4e3a\u4e86\u548coutput\u5339\u914d target = target . view ( 1 , - 1 ) criterion = nn . MSELoss () loss = criterion ( output , target ) print ( loss ) \u8f93\u51fa\u7ed3\u679c: tensor(1.1562, grad_fn=<MseLossBackward>) \u5173\u4e8e\u65b9\u5411\u4f20\u64ad\u7684\u94fe\u6761: \u5982\u679c\u6211\u4eec\u8ddf\u8e2aloss\u53cd\u5411\u4f20\u64ad\u7684\u65b9\u5411, \u4f7f\u7528.grad_fn\u5c5e\u6027\u6253\u5370, \u5c06\u53ef\u4ee5\u770b\u5230\u4e00\u5f20\u5b8c\u6574\u7684\u8ba1\u7b97\u56fe\u5982\u4e0b: input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss \u5f53\u8c03\u7528loss.backward()\u65f6, \u6574\u5f20\u8ba1\u7b97\u56fe\u5c06\u5bf9loss\u8fdb\u884c\u81ea\u52a8\u6c42\u5bfc, \u6240\u6709\u5c5e\u6027requires_grad=True\u7684Tensors\u90fd\u5c06\u53c2\u4e0e\u68af\u5ea6\u6c42\u5bfc\u7684\u8fd0\u7b97, \u5e76\u5c06\u68af\u5ea6\u7d2f\u52a0\u5230Tensors\u4e2d\u7684.grad\u5c5e\u6027\u4e2d. print ( loss . grad_fn ) # MSELoss print ( loss . grad_fn . next_functions [ 0 ][ 0 ]) # Linear print ( loss . grad_fn . next_functions [ 0 ][ 0 ] . next_functions [ 0 ][ 0 ]) # ReLU \u8f93\u51fa\u7ed3\u679c: <MseLossBackward object at 0x7fdba3216da0> <AddmmBackward object at 0x7fdba3216f28> <AccumulateGrad object at 0x7fdba3216f28>","title":"\u635f\u5931\u51fd\u6570"},{"location":"2.html#backpropagation","text":"\u5728Pytorch\u4e2d\u6267\u884c\u53cd\u5411\u4f20\u64ad\u975e\u5e38\u7b80\u4fbf, \u5168\u90e8\u7684\u64cd\u4f5c\u5c31\u662floss.backward(). \u5728\u6267\u884c\u53cd\u5411\u4f20\u64ad\u4e4b\u524d, \u8981\u5148\u5c06\u68af\u5ea6\u6e05\u96f6, \u5426\u5219\u68af\u5ea6\u4f1a\u5728\u4e0d\u540c\u7684\u6279\u6b21\u6570\u636e\u4e4b\u95f4\u88ab\u7d2f\u52a0. \u6267\u884c\u4e00\u4e2a\u53cd\u5411\u4f20\u64ad\u7684\u5c0f\u4f8b\u5b50: # Pytorch\u4e2d\u6267\u884c\u68af\u5ea6\u6e05\u96f6\u7684\u4ee3\u7801 net . zero_grad () print ( 'conv1.bias.grad before backward' ) print ( net . conv1 . bias . grad ) # Pytorch\u4e2d\u6267\u884c\u53cd\u5411\u4f20\u64ad\u7684\u4ee3\u7801 loss . backward () print ( 'conv1.bias.grad after backward' ) print ( net . conv1 . bias . grad ) \u8f93\u51fa\u7ed3\u679c: conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([-0.0002, 0.0045, 0.0017, -0.0099, 0.0092, -0.0044])","title":"\u53cd\u5411\u4f20\u64ad(backpropagation)"},{"location":"2.html#_3","text":"\u66f4\u65b0\u53c2\u6570\u6700\u7b80\u5355\u7684\u7b97\u6cd5\u5c31\u662fSGD(\u968f\u673a\u68af\u5ea6\u4e0b\u964d). \u5177\u4f53\u7684\u7b97\u6cd5\u516c\u5f0f\u8868\u8fbe\u5f0f\u4e3a: weight = weight - learning_rate * gradient \u9996\u5148\u7528\u4f20\u7edf\u7684Python\u4ee3\u7801\u6765\u5b9e\u73b0SGD\u5982\u4e0b: learning_rate = 0.01 for f in net . parameters (): f . data . sub_ ( f . grad . data * learning_rate ) \u7136\u540e\u4f7f\u7528Pytorch\u5b98\u65b9\u63a8\u8350\u7684\u6807\u51c6\u4ee3\u7801\u5982\u4e0b: # \u9996\u5148\u5bfc\u5165\u4f18\u5316\u5668\u7684\u5305, optim\u4e2d\u5305\u542b\u82e5\u5e72\u5e38\u7528\u7684\u4f18\u5316\u7b97\u6cd5, \u6bd4\u5982SGD, Adam\u7b49 import torch.optim as optim # \u901a\u8fc7optim\u521b\u5efa\u4f18\u5316\u5668\u5bf9\u8c61 optimizer = optim . SGD ( net . parameters (), lr = 0.01 ) # \u5c06\u4f18\u5316\u5668\u6267\u884c\u68af\u5ea6\u6e05\u96f6\u7684\u64cd\u4f5c optimizer . zero_grad () output = net ( input ) loss = criterion ( output , target ) # \u5bf9\u635f\u5931\u503c\u6267\u884c\u53cd\u5411\u4f20\u64ad\u7684\u64cd\u4f5c loss . backward () # \u53c2\u6570\u7684\u66f4\u65b0\u901a\u8fc7\u4e00\u884c\u6807\u51c6\u4ee3\u7801\u6765\u6267\u884c optimizer . step ()","title":"\u66f4\u65b0\u7f51\u7edc\u53c2\u6570"},{"location":"2.html#_4","text":"\u5b66\u4e60\u4e86\u6784\u5efa\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u6d41\u7a0b: \u5b9a\u4e49\u4e00\u4e2a\u62e5\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc \u904d\u5386\u8bad\u7ec3\u6570\u636e\u96c6 \u5904\u7406\u8f93\u5165\u6570\u636e\u4f7f\u5176\u6d41\u7ecf\u795e\u7ecf\u7f51\u7edc \u8ba1\u7b97\u635f\u5931\u503c \u5c06\u7f51\u7edc\u53c2\u6570\u7684\u68af\u5ea6\u8fdb\u884c\u53cd\u5411\u4f20\u64ad \u4ee5\u4e00\u5b9a\u7684\u89c4\u5219\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd \u5b66\u4e60\u4e86\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49: \u91c7\u7528torch.nn.MSELoss()\u8ba1\u7b97\u5747\u65b9\u8bef\u5dee. \u901a\u8fc7loss.backward()\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u65f6, \u6574\u5f20\u8ba1\u7b97\u56fe\u5c06\u5bf9loss\u8fdb\u884c\u81ea\u52a8\u6c42\u5bfc, \u6240\u6709\u5c5e\u6027requires_grad=True\u7684Tensors\u90fd\u5c06\u53c2\u4e0e\u68af\u5ea6\u6c42\u5bfc\u7684\u8fd0\u7b97, \u5e76\u5c06\u68af\u5ea6\u7d2f\u52a0\u5230Tensors\u4e2d\u7684.grad\u5c5e\u6027\u4e2d. \u5b66\u4e60\u4e86\u53cd\u5411\u4f20\u64ad\u7684\u8ba1\u7b97\u65b9\u6cd5: \u5728Pytorch\u4e2d\u6267\u884c\u53cd\u5411\u4f20\u64ad\u975e\u5e38\u7b80\u4fbf, \u5168\u90e8\u7684\u64cd\u4f5c\u5c31\u662floss.backward(). \u5728\u6267\u884c\u53cd\u5411\u4f20\u64ad\u4e4b\u524d, \u8981\u5148\u5c06\u68af\u5ea6\u6e05\u96f6, \u5426\u5219\u68af\u5ea6\u4f1a\u5728\u4e0d\u540c\u7684\u6279\u6b21\u6570\u636e\u4e4b\u95f4\u88ab\u7d2f\u52a0. net.zero_grad() loss.backward() \u5b66\u4e60\u4e86\u53c2\u6570\u7684\u66f4\u65b0\u65b9\u6cd5: \u5b9a\u4e49\u4f18\u5316\u5668\u6765\u6267\u884c\u53c2\u6570\u7684\u4f18\u5316\u4e0e\u66f4\u65b0. optimizer = optim.SGD(net.parameters(), lr=0.01) \u901a\u8fc7\u4f18\u5316\u5668\u6765\u6267\u884c\u5177\u4f53\u7684\u53c2\u6570\u66f4\u65b0. optimizer.step()","title":"\u5c0f\u8282\u603b\u7ed3"},{"location":"2.html#22-pytorch","text":"","title":"2.2 \u4f7f\u7528Pytorch\u6784\u5efa\u4e00\u4e2a\u5206\u7c7b\u5668"},{"location":"2.html#_5","text":"\u4e86\u89e3\u5206\u7c7b\u5668\u7684\u4efb\u52a1\u548c\u6570\u636e\u6837\u5f0f \u638c\u63e1\u5982\u4f55\u7528Pytorch\u5b9e\u73b0\u4e00\u4e2a\u5206\u7c7b\u5668","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"2.html#_6","text":"\u6784\u9020\u4e00\u4e2a\u5c06\u4e0d\u540c\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668, \u5bf9\u8f93\u5165\u7684\u56fe\u7247\u8fdb\u884c\u5224\u522b\u5e76\u5b8c\u6210\u5206\u7c7b. \u672c\u6848\u4f8b\u91c7\u7528CIFAR10\u6570\u636e\u96c6\u4f5c\u4e3a\u539f\u59cb\u56fe\u7247\u6570\u636e. CIFAR10\u6570\u636e\u96c6\u4ecb\u7ecd: \u6570\u636e\u96c6\u4e2d\u6bcf\u5f20\u56fe\u7247\u7684\u5c3a\u5bf8\u662f3 * 32 * 32, \u4ee3\u8868\u5f69\u82723\u901a\u9053 CIFAR10\u6570\u636e\u96c6\u603b\u5171\u670910\u79cd\u4e0d\u540c\u7684\u5206\u7c7b, \u5206\u522b\u662f\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\". CIFAR10\u6570\u636e\u96c6\u7684\u6837\u4f8b\u5982\u4e0b\u56fe\u6240\u793a:","title":"\u5206\u7c7b\u5668\u4efb\u52a1\u548c\u6570\u636e\u4ecb\u7ecd"},{"location":"2.html#_7","text":"1: \u4f7f\u7528torchvision\u4e0b\u8f7dCIFAR10\u6570\u636e\u96c6 2: \u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc 3: \u5b9a\u4e49\u635f\u5931\u51fd\u6570 4: \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b 5: \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u6a21\u578b 1: \u4f7f\u7528torchvision\u4e0b\u8f7dCIFAR10\u6570\u636e\u96c6 \u5bfc\u5165torchvision\u5305\u6765\u8f85\u52a9\u4e0b\u8f7d\u6570\u636e\u96c6 import torch import torchvision import torchvision.transforms as transforms \u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u5bf9\u56fe\u7247\u8fdb\u884c\u8c03\u6574, \u56e0\u4e3atorchvision\u6570\u636e\u96c6\u7684\u8f93\u51fa\u662fPILImage\u683c\u5f0f, \u6570\u636e\u57df\u5728[0, 1]. \u6211\u4eec\u5c06\u5176\u8f6c\u6362\u4e3a\u6807\u51c6\u6570\u636e\u57df[-1, 1]\u7684\u5f20\u91cf\u683c\u5f0f. transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) \u8f93\u51fa\u7ed3\u679c: Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified \u6ce8\u610f: \u5982\u679c\u4f60\u662f\u5728Windows\u7cfb\u7edf\u4e0b\u8fd0\u884c\u4e0a\u8ff0\u4ee3\u7801, \u5e76\u4e14\u51fa\u73b0\u62a5\u9519\u4fe1\u606f \"BrokenPipeError\", \u53ef\u4ee5\u5c1d\u8bd5\u5c06torch.utils.data.DataLoader()\u4e2d\u7684num_workers\u8bbe\u7f6e\u4e3a0. \u5c55\u793a\u82e5\u5e72\u8bad\u7ec3\u96c6\u7684\u56fe\u7247 # \u5bfc\u5165\u753b\u56fe\u5305\u548cnumpy import matplotlib.pyplot as plt import numpy as np # \u6784\u5efa\u5c55\u793a\u56fe\u7247\u7684\u51fd\u6570 def imshow ( img ): img = img / 2 + 0.5 npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) plt . show () # \u4ece\u6570\u636e\u8fed\u4ee3\u5668\u4e2d\u8bfb\u53d6\u4e00\u5f20\u56fe\u7247 dataiter = iter ( trainloader ) images , labels = dataiter . next () # \u5c55\u793a\u56fe\u7247 imshow ( torchvision . utils . make_grid ( images )) # \u6253\u5370\u6807\u7b7elabel print ( ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) \u8f93\u51fa\u56fe\u7247\u7ed3\u679c: \u8f93\u51fa\u6807\u7b7e\u7ed3\u679c: bird truck cat cat 2: \u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc \u4eff\u71672.1\u8282\u4e2d\u7684\u7c7b\u6765\u6784\u9020\u6b64\u5904\u7684\u7c7b, \u552f\u4e00\u7684\u533a\u522b\u662f\u6b64\u5904\u91c7\u75283\u901a\u90533-channel import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () 3: \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u91c7\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668. import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 ) 4: \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b \u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u5316\u7b97\u6cd5, \u90fd\u9700\u8981\u5f88\u591a\u4e2a\u8f6e\u6b21\u7684\u8fed\u4ee3\u8bad\u7ec3. for epoch in range ( 2 ): # loop over the dataset multiple times running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # data\u4e2d\u5305\u542b\u8f93\u5165\u56fe\u50cf\u5f20\u91cfinputs, \u6807\u7b7e\u5f20\u91cflabels inputs , labels = data # \u9996\u5148\u5c06\u4f18\u5316\u5668\u68af\u5ea6\u5f52\u96f6 optimizer . zero_grad () # \u8f93\u5165\u56fe\u50cf\u5f20\u91cf\u8fdb\u7f51\u7edc, \u5f97\u5230\u8f93\u51fa\u5f20\u91cfoutputs outputs = net ( inputs ) # \u5229\u7528\u7f51\u7edc\u7684\u8f93\u51faoutputs\u548c\u6807\u7b7elabels\u8ba1\u7b97\u635f\u5931\u503c loss = criterion ( outputs , labels ) # \u53cd\u5411\u4f20\u64ad+\u53c2\u6570\u66f4\u65b0, \u662f\u6807\u51c6\u4ee3\u7801\u7684\u6807\u51c6\u6d41\u7a0b loss . backward () optimizer . step () # \u6253\u5370\u8f6e\u6b21\u548c\u635f\u5931\u503c running_loss += loss . item () if ( i + 1 ) % 2000 == 0 : print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Finished Training' ) \u8f93\u51fa\u7ed3\u679c: [1, 2000] loss: 2.227 [1, 4000] loss: 1.884 [1, 6000] loss: 1.672 [1, 8000] loss: 1.582 [1, 10000] loss: 1.526 [1, 12000] loss: 1.474 [2, 2000] loss: 1.407 [2, 4000] loss: 1.384 [2, 6000] loss: 1.362 [2, 8000] loss: 1.341 [2, 10000] loss: 1.331 [2, 12000] loss: 1.291 Finished Training \u4fdd\u5b58\u6a21\u578b: # \u9996\u5148\u8bbe\u5b9a\u6a21\u578b\u7684\u4fdd\u5b58\u8def\u5f84 PATH = './cifar_net.pth' # \u4fdd\u5b58\u6a21\u578b\u7684\u72b6\u6001\u5b57\u5178 torch . save ( net . state_dict (), PATH ) 5: \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u6a21\u578b \u7b2c\u4e00\u6b65, \u5c55\u793a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u82e5\u5e72\u56fe\u7247 dataiter = iter ( testloader ) images , labels = dataiter . next () # \u6253\u5370\u539f\u59cb\u56fe\u7247 imshow ( torchvision . utils . make_grid ( images )) # \u6253\u5370\u771f\u5b9e\u7684\u6807\u7b7e print ( 'GroundTruth: ' , ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) \u8f93\u51fa\u56fe\u7247\u7ed3\u679c: \u8f93\u51fa\u6807\u7b7e\u7ed3\u679c: GroundTruth: cat ship ship plane \u7b2c\u4e8c\u6b65, \u52a0\u8f7d\u6a21\u578b\u5e76\u5bf9\u6d4b\u8bd5\u56fe\u7247\u8fdb\u884c\u9884\u6d4b # \u9996\u5148\u5b9e\u4f8b\u5316\u6a21\u578b\u7684\u7c7b\u5bf9\u8c61 net = Net () # \u52a0\u8f7d\u8bad\u7ec3\u9636\u6bb5\u4fdd\u5b58\u597d\u7684\u6a21\u578b\u7684\u72b6\u6001\u5b57\u5178 net . load_state_dict ( torch . load ( PATH )) # \u5229\u7528\u6a21\u578b\u5bf9\u56fe\u7247\u8fdb\u884c\u9884\u6d4b outputs = net ( images ) # \u5171\u670910\u4e2a\u7c7b\u522b, \u91c7\u7528\u6a21\u578b\u8ba1\u7b97\u51fa\u7684\u6982\u7387\u6700\u5927\u7684\u4f5c\u4e3a\u9884\u6d4b\u7684\u7c7b\u522b _ , predicted = torch . max ( outputs , 1 ) # \u6253\u5370\u9884\u6d4b\u6807\u7b7e\u7684\u7ed3\u679c print ( 'Predicted: ' , ' ' . join ( ' %5s ' % classes [ predicted [ j ]] for j in range ( 4 ))) \u8f93\u51fa\u7ed3\u679c: Predicted: cat ship ship plane \u63a5\u4e0b\u6765\u770b\u4e00\u4e0b\u5728\u5168\u90e8\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0 correct = 0 total = 0 with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) \u8f93\u51fa\u7ed3\u679c: Accuracy of the network on the 10000 test images: 53 % \u5206\u6790\u7ed3\u679c: \u5bf9\u4e8e\u62e5\u670910\u4e2a\u7c7b\u522b\u7684\u6570\u636e\u96c6, \u968f\u673a\u731c\u6d4b\u7684\u51c6\u786e\u7387\u662f10%, \u6a21\u578b\u8fbe\u5230\u4e8653%, \u8bf4\u660e\u6a21\u578b\u5b66\u5230\u4e86\u771f\u5b9e\u7684\u4e1c\u897f. \u4e3a\u4e86\u66f4\u52a0\u7ec6\u81f4\u7684\u770b\u4e00\u4e0b\u6a21\u578b\u5728\u54ea\u4e9b\u7c7b\u522b\u4e0a\u8868\u73b0\u66f4\u597d, \u5728\u54ea\u4e9b\u7c7b\u522b\u4e0a\u8868\u73b0\u66f4\u5dee, \u6211\u4eec\u5206\u7c7b\u522b\u7684\u8fdb\u884c\u51c6\u786e\u7387\u8ba1\u7b97. class_correct = list ( 0. for i in range ( 10 )) class_total = list ( 0. for i in range ( 10 )) with torch . no_grad (): for data in testloader : images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 4 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( 10 ): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) \u8f93\u51fa\u7ed3\u679c: Accuracy of plane : 62 % Accuracy of car : 62 % Accuracy of bird : 45 % Accuracy of cat : 36 % Accuracy of deer : 52 % Accuracy of dog : 25 % Accuracy of frog : 69 % Accuracy of horse : 60 % Accuracy of ship : 70 % Accuracy of truck : 48 %","title":"\u8bad\u7ec3\u5206\u7c7b\u5668\u7684\u6b65\u9aa4"},{"location":"2.html#gpu","text":"\u4e3a\u4e86\u771f\u6b63\u5229\u7528Pytorch\u4e2dTensor\u7684\u4f18\u79c0\u5c5e\u6027, \u52a0\u901f\u6a21\u578b\u7684\u8bad\u7ec3, \u6211\u4eec\u53ef\u4ee5\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u8f6c\u79fb\u5230GPU\u4e0a\u8fdb\u884c. \u9996\u5148\u8981\u5b9a\u4e49\u8bbe\u5907, \u5982\u679cCUDA\u662f\u53ef\u7528\u7684\u5219\u88ab\u5b9a\u4e49\u6210GPU, \u5426\u5219\u88ab\u5b9a\u4e49\u6210CPU. device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) print ( device ) \u8f93\u51fa\u7ed3\u679c: cuda:0 \u5f53\u8bad\u7ec3\u6a21\u578b\u7684\u65f6\u5019, \u53ea\u9700\u8981\u5c06\u6a21\u578b\u8f6c\u79fb\u5230GPU\u4e0a, \u540c\u65f6\u5c06\u8f93\u5165\u7684\u56fe\u7247\u548c\u6807\u7b7e\u9875\u8f6c\u79fb\u5230GPU\u4e0a\u5373\u53ef. # \u5c06\u6a21\u578b\u8f6c\u79fb\u5230GPU\u4e0a net . to ( device ) # \u5c06\u8f93\u5165\u7684\u56fe\u7247\u5f20\u91cf\u548c\u6807\u7b7e\u5f20\u91cf\u8f6c\u79fb\u5230GPU\u4e0a inputs , labels = data [ 0 ] . to ( device ), data [ 1 ] . to ( device )","title":"\u5728GPU\u4e0a\u8bad\u7ec3\u6a21\u578b"},{"location":"2.html#_8","text":"\u5b66\u4e60\u4e86\u5206\u7c7b\u5668\u7684\u4efb\u52a1\u548c\u6570\u636e\u6837\u5f0f. \u5c06\u4e0d\u540c\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668, \u5bf9\u8f93\u5165\u7684\u56fe\u7247\u8fdb\u884c\u5224\u522b\u5e76\u5b8c\u6210\u5206\u7c7b. \u91c7\u7528CIFAR10\u6570\u636e\u96c6\u4f5c\u4e3a\u539f\u59cb\u56fe\u7247\u6570\u636e, CIFAR10\u6570\u636e\u96c6\u62e5\u670910\u4e2a\u7c7b\u522b\u76843 * 32 * 32\u5f69\u8272\u56fe\u7247. \u5b66\u4e60\u4e86\u8bad\u7ec3\u5206\u7c7b\u5668\u7684\u6b65\u9aa4: \u4f7f\u7528torchvision\u4e0b\u8f7dCIFAR10\u6570\u636e\u96c6. \u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc. \u5b9a\u4e49\u635f\u5931\u51fd\u6570. \u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b. \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u6a21\u578b. \u5b66\u4e60\u4e86\u5728GPU\u4e0a\u8bad\u7ec3\u6a21\u578b. \u9996\u5148\u9700\u8981\u5b9a\u4e49\u8bbe\u5907, CPU\u548cGPU\u4e8c\u9009\u4e00: device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \u7136\u540e\u5c06\u6a21\u578b\u8f6c\u79fb\u5230GPU\u4e0a\u53bb: net.to(device) \u6700\u540e\u5728\u8fed\u4ee3\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d, \u6bcf\u4e00\u6b65\u90fd\u5c06\u56fe\u7247\u548c\u6807\u7b7e\u5f20\u91cf\u8f6c\u79fb\u5230GPU\u4e0a\u53bb: inputs, labels = data[0].to(device), data[1].to(device)","title":"\u5c0f\u8282\u603b\u7ed3"}]}